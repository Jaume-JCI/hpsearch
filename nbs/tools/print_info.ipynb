{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp tools.print_info\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print info\n",
    "\n",
    "> Prints information about the experiment indicated as argument: \n",
    "- path to experiment data\n",
    "- parameters\n",
    "- name of evaluation scores in experiment csv data\n",
    "- name of metrics monitored during training\n",
    "- scores obtained across all the runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('src')\n",
    "\n",
    "from hpsearch.config.hpconfig import get_path_experiments, get_path_results, get_experiment_manager\n",
    "import hpsearch.utils.experiment_utils as ut\n",
    "from hpsearch.tools.metric_visualization import include_best_and_last_experiment\n",
    "import hpsearch.config.hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "from hpsearch.examples.dummy_experiment_manager import generate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_info (experiments=[-1], base=None, folder=None, display_all=False, include_best=False, \n",
    "                op=None, metric=None, round_digits=2, compare=True, compact=0, run_number=0, \n",
    "                manager_path=dflt.manager_path):\n",
    "        \n",
    "    em = get_experiment_manager (manager_path=manager_path)\n",
    "    if path_experiments is not None or folder is not None:\n",
    "        em.set_path_experiments (path_experiments=path_experiments, folder=folder)\n",
    "    if metric is not None:\n",
    "        em.key_score = metric\n",
    "    if op is not None:\n",
    "        em.op = op\n",
    "        \n",
    "    path_experiments = em.path_experiments\n",
    "        \n",
    "    df = pd.read_pickle (path_experiments/'experiments_data.pk')\n",
    "\n",
    "    metric_column = f'{run_number}_{em.key_score}'\n",
    "        \n",
    "    experiments = include_best_and_last_experiment ([em.key_score], experiments=experiments, \n",
    "                                                    run_number=run_number, op=self.op)\n",
    "\n",
    "\n",
    "    df_scores = None\n",
    "    print ('\\n*****************************')\n",
    "    for experiment in experiments:\n",
    "        parameters = ut.get_parameters_columns(df.loc[[experiment]], True)\n",
    "        print (f'\\nparameters for {experiment}:')\n",
    "        df2 = df.copy()\n",
    "        df2[metric] = df.loc[experiment, metric_column]\n",
    "        display (df2.loc[experiment, parameters + [metric]])\n",
    "        print ('scores for all experiments:')\n",
    "        df_scores = ut.get_experiment_scores(df.loc[[experiment]], suffix_results='_%s' %metric, remove_suffix=True)\n",
    "        display(df_scores.round(round_digits))\n",
    "\n",
    "        path_results = em.get_path_results (experiment, run_number)\n",
    "        print (f'path to results: {path_results}')\n",
    "        scores_names = ut.get_scores_names (df, experiment=experiment, run_number=run_number)\n",
    "        print (f'scores names: {scores_names}')\n",
    "        monitored_metrics = ut.get_monitored_training_metrics (experiment, run_number, path_results=path_results)\n",
    "        print (f'monitored metrics: {monitored_metrics}')\n",
    "        print ('\\n*****************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_print_info\n",
    "def test_print_info ():\n",
    "    em = generate_data ('print_info')\n",
    "    \n",
    "    print_info (manager_path=em.manager_path)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_print_info\n",
      "\n",
      "*****************************\n",
      "\n",
      "parameters for 8:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "epochs                       30\n",
       "noise                       0.1\n",
       "offset                      0.6\n",
       "rate                       0.03\n",
       "validation_accuracy    0.945403\n",
       "Name: 8, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores for all experiments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1    2     3    4\n",
       "8  0.95  0.91  0.9  0.89  0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to results: test_print_info//experiments/00008/0\n",
      "scores names: ['test_accuracy', 'validation_accuracy']\n",
      "monitored metrics: ['validation_accuracy', 'test_accuracy', 'accuracy']\n",
      "\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_print_info, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser(description='print table') \n",
    "    # Datasets\n",
    "    parser.add_argument('--folder', type=str, default=None, help='name of experiments folder')\n",
    "    parser.add_argument('--path_experiments', type=str, default=None, help='full experiments path')\n",
    "    parser.add_argument('-m', '--metric', type=str, default=None, help='metric score')\n",
    "    parser.add_argument('-e', type=int, nargs='+', default=[-1, -2], help='experiment numbers')\n",
    "    parser.add_argument('-a', type=bool, default=False)\n",
    "    parser.add_argument ('-b', '--best', action= \"store_true\", help='include experiment with best performance (on given run id!!)')\n",
    "    parser.add_argument ('-r', '--run', type=int, default=0, help='run id') \n",
    "    parser.add_argument('-n', '--no_comp', action= \"store_true\", help='do not perform comparison')\n",
    "    parser.add_argument('--compact', type=int, default=0, help='compact parameters to this number of characters') \n",
    "    parser.add_argument('--op', default=None, type=str)\n",
    "    parser.add_argument('--round', default=2, type=int, help='round scores to this number of digits')\n",
    "    parser.add_argument('-p', '--path', default=dflt.manager_path, type=str)\n",
    "    pars = parser.parse_args(args)\n",
    "    \n",
    "    return pars\n",
    "\n",
    "def parse_arguments_and_run (args):\n",
    "    \n",
    "    pars = parse_args(args)\n",
    "\n",
    "    print_info (experiments=pars.e, path_experiments = pars.path_experiments, folder=pars.folder, \n",
    "                display_all=pars.a, include_best=pars.best, op=pars.op, metric=pars.metric, \n",
    "                round_digits=pars.round, compare=not pars.no_comp, compact=pars.compact, \n",
    "                run_number=pars.run, manager_path=pars.path)\n",
    "\n",
    "def main():\n",
    "    parse_arguments_and_run (sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_print_info\n",
    "def test_parse_args ():\n",
    "    em = generate_data ('parse_args')\n",
    "    \n",
    "    args = ['-e', '4', '3',\n",
    "       '--compact', '3',\n",
    "       '--round', '3',\n",
    "       '-p', em.manager_path]\n",
    "    parse_arguments_and_run (args)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_parse_args\n",
      "\n",
      "*****************************\n",
      "\n",
      "parameters for 4:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "epochs                       15\n",
       "noise                       0.1\n",
       "offset                      0.3\n",
       "rate                       0.03\n",
       "validation_accuracy    0.815981\n",
       "Name: 4, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores for all experiments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.816</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1    2    3      4\n",
       "4  0.816  0.656  0.8  1.0  0.819"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to results: test_parse_args//experiments/00004/0\n",
      "scores names: ['test_accuracy', 'validation_accuracy']\n",
      "monitored metrics: ['validation_accuracy', 'test_accuracy', 'accuracy']\n",
      "\n",
      "*****************************\n",
      "\n",
      "parameters for 3:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "epochs                      15\n",
       "noise                      0.1\n",
       "offset                     0.1\n",
       "rate                      0.03\n",
       "validation_accuracy    0.54907\n",
       "Name: 3, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores for all experiments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4\n",
       "3  0.549  0.604  0.471  0.492  0.491"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to results: test_parse_args//experiments/00003/0\n",
      "scores names: ['test_accuracy', 'validation_accuracy']\n",
      "monitored metrics: ['validation_accuracy', 'test_accuracy', 'accuracy']\n",
      "\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_parse_args, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpsearch)",
   "language": "python",
   "name": "hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
