{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp tools.query\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "#tst = TestRunner (targets=['dummy'])\n",
    "tst = TestRunner (targets=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query\n",
    "\n",
    "> Shows results for queried experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from collections import namedtuple\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# hpsearch api\n",
    "import hpsearch.utils.experiment_utils as ut\n",
    "import hpsearch.config.hp_defaults as dflt\n",
    "from hpsearch.config.hpconfig import get_experiment_manager, get_em_args, add_em_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "import hpsearch.config.hp_defaults as dflt\n",
    "from hpsearch.examples.dummy_experiment_manager import generate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def query (pv={}, pf={}, pall=[], pexact=False, folder=None, \n",
    "           metric=None, experiments=None, runs=None, op=None, stats=['mean'], \n",
    "           results=0, other_parameters=False, **kwargs):\n",
    "    \n",
    "    result_query = ut.query(folder_experiments=folder, score_name=metric, experiments=experiments,\n",
    "                            run_number=runs, parameters_fixed=pf, parameters_variable=pv, \n",
    "                            parameters_all=pall, exact_match=pexact, ascending=op=='min', \n",
    "                            stats=stats, min_results=results, \n",
    "                            query_other_parameters=other_parameters, **kwargs)\n",
    "    \n",
    "    return result_query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## do_query_and_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def do_query_and_show (pall=[], best=None, compact=0, exact=False, experiments=None, pf={}, last=None, \n",
    "                       other_parameters=False, input_range=None, results=0, \n",
    "                       round=2, runs=None, show=False, stats=['mean'], pv={},\n",
    "                       sort=None, display_all_columns=False, col_width=None, **kwargs):\n",
    "    em_args = get_em_args (kwargs)\n",
    "    em = get_experiment_manager (**em_args)\n",
    "    kwargs = {k: kwargs[k] for k in kwargs if k not in em_args}\n",
    "    folder, metric, op = em.folder, em.key_score, em.op\n",
    "        \n",
    "    df = query (pv=pv, pf=pf, pall=pall, pexact=exact, folder=em.folder, \n",
    "               metric=em.key_score, experiments=experiments, runs=runs, op=em.op, stats=stats, \n",
    "               results=results, other_parameters=other_parameters, **kwargs)\n",
    "    df = ut.replace_with_default_values (df)\n",
    "    if sort is not None:\n",
    "        stats_cols = df[dflt.stats_col].columns.get_level_values(1)\n",
    "        pars_cols = df[dflt.parameters_col].columns.get_level_values(0)\n",
    "        if sort in stats_cols:\n",
    "            score_name_sort = df[dflt.stats_col].columns.get_level_values(0).unique()\n",
    "            if len(score_name_sort)>1: print (f'sorting using first score_name from {score_name_sort}')\n",
    "            score_name_sort = score_name_sort[0]\n",
    "            sort_col = (dflt.stats_col, score_name_sort, sort)\n",
    "        elif sort in pars_col:\n",
    "            sort_col = (dflt.parameters_col, sort, '')\n",
    "        else:\n",
    "            raise ValueError (f'sort must be in {stats_cols.tolist()+pars_cols.tolist()}')\n",
    "        df = df.sort_values(by=sort_col, ascending=(em.op=='min'))\n",
    "    if experiments is None:\n",
    "        experiments = []\n",
    "    if last is not None:\n",
    "        experiments += range(df.index.max()-last+1, df.index.max()+1)\n",
    "    if best is not None:\n",
    "        experiments += list(df.index[:best])\n",
    "    if input_range is not None:\n",
    "        assert len(input_range) == 2\n",
    "        experiments += range(input_range[0], input_range[1])\n",
    "    if len(experiments) > 0: \n",
    "        df = df.loc[[x for x in df.index if x in experiments]]\n",
    "    \n",
    "    if col_width is not None:\n",
    "        pd.set_option('max_colwidth', col_width)\n",
    "    \n",
    "    if (round is not None) and (round != 0):\n",
    "        stats_col = pd.MultiIndex.from_product ([[dflt.stats_col], \n",
    "                                                 df[dflt.stats_col].columns.get_level_values(0).unique(),\n",
    "                                                 stats])\n",
    "        df[stats_col] = df[stats_col].round(round)\n",
    "    if display_all_columns:\n",
    "        display (df)\n",
    "    \n",
    "    print (f'experiments: {list(df.index)}')\n",
    "    print (f'min experiment #: {df.index.min()}, max experiment #: {df.index.max()}')\n",
    "\n",
    "    print ('result of query:')\n",
    "    _, df2 = ut.get_parameters_unique(df)\n",
    "    #df2.index.name = 'experiment #'\n",
    "    if compact > 0:\n",
    "        prev_cols = df2.columns.copy()\n",
    "        df2, dict_rename = ut.compact_parameters (df2, compact)\n",
    "        for k, kor in zip(df2.columns, prev_cols):\n",
    "            print (f'{k} => {kor}')\n",
    "    display (df2)\n",
    "            \n",
    "    if show:\n",
    "        import hpsearch.visualization.plot_visdom as pv\n",
    "        pv.plot_multiple_histories(df.index, folder=em.folder,metrics=em.key_score, parameters=None)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simple query\n",
    "\n",
    "Run query without any condition, retrieving all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_query\n",
    "def test_do_query_and_show ():\n",
    "    path_results = 'do_query_and_show'\n",
    "    em = generate_data (path_results)\n",
    "    \n",
    "    df=do_query_and_show (manager_path=em.manager_path)\n",
    "    assert sorted(os.listdir (f'test_{path_results}/default/managers'))==['fields', 'info', 'logs.txt', 'whole']\n",
    "    assert sorted(os.listdir (f'test_{path_results}/default/managers/whole'))==['DummyExperimentManager-default.pk', 'last.pk']\n",
    "    par = lambda parameter: (dflt.parameters_col, parameter, '')\n",
    "    assert (df[par('epochs')] == [15,30,5,15,30,15,5,30,5]).all()\n",
    "    assert (df[par('offset')] == [.6,.6,.6,.3,.3,.1,.3,.1,.1]).all()\n",
    "    assert (df[(dflt.stats_col, 'validation_accuracy', 'mean')] == [0.97, 0.89, 0.81, 0.8 , 0.65, 0.55, 0.46, 0.44, 0.19]).all()\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_do_query_and_show, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Change the metric that we want to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_query\n",
    "def test_do_query_and_show_change_metric ():\n",
    "    em = generate_data ('do_query_and_show_change_metric')\n",
    "    \n",
    "    # if we want to use a single metric, we can simply indicate its name:\n",
    "    do_query_and_show (metric='test_accuracy', manager_path=em.manager_path)\n",
    "    \n",
    "    # we can indicate more than one metric, using a list:\n",
    "    do_query_and_show (metric=['test_accuracy', 'validation_accuracy'], manager_path=em.manager_path)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_do_query_and_show_change_metric, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run query with conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_query\n",
    "def test_do_query_and_show_with_conditions ():\n",
    "    em = generate_data ('do_query_and_show_with_conditions')\n",
    "    \n",
    "    do_query_and_show (metric='validation_accuracy', op='max', pf={'epochs':15}, \n",
    "                       manager_path=em.manager_path)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_do_query_and_show_with_conditions, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run query that sorts by maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_query\n",
    "def test_do_query_and_show_sort_maximum ():\n",
    "    em = generate_data ('do_query_and_show_sort_maximum')\n",
    "    \n",
    "    do_query_and_show (metric='validation_accuracy', op='max', sort='max', \n",
    "                       stats=['mean', 'min', 'max'], manager_path=em.manager_path);\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_do_query_and_show_sort_maximum, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_args(args):\n",
    "    default_always = ''\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Run query')\n",
    "    # Datasets\n",
    "    parser.add_argument('--stats', type=str, nargs='+', default=['mean'],  help=\"statistics for multiple runs\")\n",
    "    parser.add_argument('--experiments', type=int, nargs='+', default=None,  help=\"experiment numbers\")\n",
    "    parser.add_argument('-v', type=str, default='{}', help='variable parameters')\n",
    "    parser.add_argument('-f', type=str, default='{}', help='fixed parameters')\n",
    "    parser.add_argument('-a', type=str, default='[]', help='all parameters')\n",
    "    parser.add_argument('-e', '--exact', action= \"store_true\", help='exact match') \n",
    "    parser.add_argument('--last', type=int, default=None, help='include these last experiments') \n",
    "    parser.add_argument('--best', type=int, default=None, help='include these best experiments')\n",
    "    parser.add_argument('--range', type=int, nargs='+', default=None, help='include this range of experiments')\n",
    "    parser.add_argument('-c', '--compact', type=int, default=0, help='compact parameters to this number of characters') \n",
    "    parser.add_argument('--results', type=int, default=0, help='min number of results to consider') \n",
    "    parser.add_argument('-s', '--show', action= \"store_true\")\n",
    "    parser.add_argument('--other', action= \"store_true\")\n",
    "    parser.add_argument('--always', type=str, default = default_always)\n",
    "    parser.add_argument('--round', default=2, type=int, help='round scores to this number of digits')\n",
    "    parser.add_argument('--runs', default=None, type=int, nargs='+', help='query restricted to run number provided')\n",
    "    parser.add_argument('--sort', default=None, type=str)\n",
    "    parser.add_argument('--width', default=None, type=int, help='max column width')\n",
    "    parser.add_argument('--metric', type=str, nargs='+', default=None, help='include these metrics')\n",
    "    add_em_args (parser, but=['metric'])\n",
    "    pars = parser.parse_args(args)\n",
    "\n",
    "    pars.v = eval(pars.v)\n",
    "    pars.f = eval(pars.f)\n",
    "    pars.a = eval(pars.a)\n",
    "    pars.always = eval('dict(%s)' %pars.always)\n",
    "    pars.f.update(pars.always)\n",
    "\n",
    "    print (f'dictionary of query terms={pars.f}')\n",
    "    \n",
    "    return pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse_arguments_and_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_arguments_and_query (args):\n",
    "    \n",
    "    pars = parse_args(args)\n",
    "\n",
    "    do_query_and_show (pall=pars.a, best=pars.best, compact=pars.compact, exact=pars.exact, \n",
    "                       experiments=pars.experiments, pf=pars.f, last=pars.last, \n",
    "                       other_parameters=pars.other, input_range=pars.range, results=pars.results, \n",
    "                       round=pars.round, runs=pars.runs, show=pars.show, stats=pars.stats,\n",
    "                       pv=pars.v, sort=pars.sort, col_width=pars.width, **get_em_args (pars))\n",
    "\n",
    "def main():\n",
    "    parse_arguments_and_query (sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the metric that we want to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_query\n",
    "def test_parse_arguments_and_query_change_metric ():\n",
    "    em = generate_data ('parse_arguments_and_query_change_metric')\n",
    "       \n",
    "    # indicate more than one metric\n",
    "    command = f'--metric test_accuracy validation_accuracy --manager_path {em.manager_path}'\n",
    "    parse_arguments_and_query (command.split())\n",
    "    \n",
    "    # indicate a single metric\n",
    "    command = f'--metric test_accuracy --manager_path {em.manager_path}'\n",
    "    parse_arguments_and_query (command.split())\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_parse_arguments_and_query_change_metric, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run query with conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.tools.test_query\n",
    "def test_parse_arguments_and_query_with_conditions ():\n",
    "    em = generate_data ('parse_arguments_and_query_with_conditions')\n",
    "\n",
    "    command = f'--metric validation_accuracy --op max -f dict(epochs=15) --manager_path {em.manager_path}'\n",
    "    parse_arguments_and_query (command.split())\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_parse_arguments_and_query_with_conditions, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpsearch)",
   "language": "python",
   "name": "hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
