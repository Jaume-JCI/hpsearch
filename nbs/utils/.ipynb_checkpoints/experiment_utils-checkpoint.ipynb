{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp utils.experiment_utils\n",
    "from nbdev.showdoc import show_doc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "\n",
    "> Helper functions for querying and retrieving results from past experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Routines for comparing results in experiments\n",
    "##############################################################\n",
    "\n",
    "def remove_defaults (parameters):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    defaults = get_default_parameters(parameters)\n",
    "    for key in defaults.keys():\n",
    "        if key in parameters.keys() and (parameters[key] == defaults[key]):\n",
    "            del parameters[key]\n",
    "    return parameters\n",
    "\n",
    "def query (path_experiments = None, \n",
    "              folder_experiments = None,\n",
    "              intersection = False, \n",
    "              experiments = None, \n",
    "              suffix_results='', \n",
    "              min_results=0, \n",
    "              classes = None, \n",
    "              parameters_fixed = {},\n",
    "              parameters_variable = {},\n",
    "              parameters_all = [],\n",
    "              exact_match = True,\n",
    "              output='all',\n",
    "              ascending=False,\n",
    "              suffix_test_set = None,\n",
    "              stats = ['mean','median','rank','min','max','std'],\n",
    "              query_other_parameters=False):\n",
    "  \n",
    "    if path_experiments is None:\n",
    "        from hpsearch.config.hpconfig import get_path_experiments\n",
    "        path_experiments = get_path_experiments(path_experiments=path_experiments, folder = folder_experiments)\n",
    "    \n",
    "    if query_other_parameters:\n",
    "        path_csv = '%s/other_parameters.csv' %path_experiments\n",
    "    else:\n",
    "        path_csv = '%s/experiments_data.csv' %path_experiments\n",
    "    experiment_data = pd.read_csv(path_csv, index_col=0)\n",
    "    \n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_variable))\n",
    "    experiment_numbers = []\n",
    "    for (i, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_fixed)\n",
    "        parameters_none = {k:v for k,v in parameters.items() if v is None}\n",
    "        parameters_not_none = {k:v for k,v in parameters.items() if v is not None}\n",
    "        parameters = remove_defaults (parameters_not_none)\n",
    "        parameters.update(parameters_none)\n",
    "    \n",
    "        experiment_numbers_i, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, ignore_keys=parameters_all, exact_match = exact_match)\n",
    "        experiment_numbers += experiment_numbers_i\n",
    "    \n",
    "    experiment_data = experiment_data.iloc[experiment_numbers]\n",
    "    \n",
    "    if experiments is not None:\n",
    "        experiment_data = experiment_data.loc[experiments]\n",
    "        \n",
    "    if query_other_parameters:\n",
    "        return experiment_data\n",
    "  \n",
    "    d=summarize_results(path_experiments = path_experiments, \n",
    "                      folder_experiments = folder_experiments,\n",
    "                      intersection = intersection, \n",
    "                      experiments = experiments, \n",
    "                      suffix_results=suffix_results, \n",
    "                      min_results=min_results, \n",
    "                      class_ids = classes, \n",
    "                      parameters = None,\n",
    "                      output='all',\n",
    "                      data = experiment_data,\n",
    "                      ascending=ascending,\n",
    "                      suffix_test_set = suffix_test_set,\n",
    "                      stats = stats)\n",
    "                      \n",
    "    return d['mean'], d\n",
    "\n",
    "def summarize_results(path_experiments = None, \n",
    "                      folder_experiments = None,\n",
    "                      intersection = False, \n",
    "                      experiments = None, \n",
    "                      suffix_results='', \n",
    "                      min_results=0, \n",
    "                      class_ids = None, \n",
    "                      parameters = None,\n",
    "                      output='all',\n",
    "                      data = None,\n",
    "                      ascending=False,\n",
    "                      suffix_test_set = None,\n",
    "                      stats = ['mean','median','rank','min','max','std']):\n",
    "    \"\"\"Obtains summary scores for the desired list of experiments. Uses the experiment_data csv for that purpose\n",
    "    \n",
    "    Example use: \n",
    "        - restricting class_ids:\n",
    "            summarize_results(class_ids= [1058,1059],suffix_results='_m3');\n",
    "    \n",
    "        - with a predetermined list of class_ids:\n",
    "            summarize_results(class_ids='qualified',suffix_results='_m3',min_results=96);\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if data is None:\n",
    "        if path_experiments is None:\n",
    "            from hpsearch.config.hpconfig import get_path_experiments\n",
    "            path_experiments = get_path_experiments(path_experiments=path_experiments, folder = folder_experiments)\n",
    "        path_csv = '%s/experiments_data.csv' %path_experiments\n",
    "        path_pickle = path_csv.replace('csv', 'pk')\n",
    "        if os.path.exists (path_pickle):\n",
    "            experiment_data = pd.read_pickle (path_pickle)\n",
    "        else:\n",
    "            experiment_data = pd.read_csv(path_csv, index_col=0)\n",
    "        clear_access (path_csv)\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        if experiments is not None:\n",
    "            experiment_data = experiment_data.loc[experiments,:]\n",
    "        if parameters is not None:\n",
    "            experiment_rows, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, create_if_not_exists=False, exact_match=False)\n",
    "            experiment_data = experiment_data.loc[experiment_rows]\n",
    "    else:\n",
    "        experiment_data = data.copy()\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        \n",
    "    # Determine the columnns that provide evaluation scores. \n",
    "    result_columns = get_scores_columns (experiment_data, suffix_results=suffix_results, class_ids=class_ids)\n",
    "    \n",
    "    experiment_data.loc[:,'num_results'] = np.sum(~experiment_data.loc[:,result_columns].isnull(),axis=1)\n",
    "    if min_results > 0:\n",
    "        number_before = experiment_data.shape[0]\n",
    "        experiment_data = experiment_data[experiment_data.num_results>=min_results]\n",
    "        print ('%d out of %d experiments have %d class_ids completed' %(experiment_data.shape[0], number_before, min_results))\n",
    "    \n",
    "    # Take only those class_ids where all experiments provide some score\n",
    "    if intersection:\n",
    "        number_before = len(result_columns)\n",
    "        all_have_results = ~experiment_data.loc[:,result_columns].isnull().any(axis=0)\n",
    "        result_columns = (np.array(result_columns)[all_have_results]).tolist()\n",
    "        print ('%d out of %d class_ids for whom all the selected experiments have completed' %(len(result_columns), number_before))\n",
    "        \n",
    "    print ('total data examined: %d experiments with at least %d class_ids each' %(experiment_data.shape[0],experiment_data['num_results'].min()))\n",
    "        \n",
    "    scores = -experiment_data.loc[:,result_columns].values\n",
    "    rank = np.argsort(scores,axis=0)\n",
    "    rank = np.argsort(rank,axis=0).astype(np.float32)\n",
    "    rank[experiment_data.loc[:,result_columns].isnull()]=np.nan\n",
    "    \n",
    "    parameters = get_parameters_columns(experiment_data, True)\n",
    "    experiment_data.loc[:,'mean'] = experiment_data.loc[:,result_columns].mean(axis=1)\n",
    "    experiment_data.loc[:,'min'] = experiment_data.loc[:,result_columns].min(axis=1)\n",
    "    experiment_data.loc[:,'max'] = experiment_data.loc[:,result_columns].max(axis=1)\n",
    "    experiment_data.loc[:,'std'] = experiment_data.loc[:,result_columns].std(axis=1)\n",
    "    experiment_data.loc[:,'median'] = experiment_data.loc[:,result_columns].median(axis=1)\n",
    "    experiment_data.loc[:,'rank'] = np.nanmean(rank,axis=1)\n",
    "    experiment_data.loc[:,'good'] = (experiment_data.loc[:,result_columns]>=0.1666666).sum(axis=1)\n",
    "    \n",
    "    scores_to_return = dict(mean=['mean'], median=['median'], rank=['rank'], good=['good'])\n",
    "    if suffix_test_set is not None:\n",
    "        def add_score_to_return (suffix_test_set_i):\n",
    "            result_columns_test_set = get_scores_columns (experiment_data, suffix_results=suffix_test_set_i, class_ids=class_ids)\n",
    "            experiment_data.loc[:,'mean%s' %suffix_test_set_i] = experiment_data.loc[:,result_columns_test_set].mean(axis=1)\n",
    "            experiment_data.loc[:,'median%s' %suffix_test_set_i] = experiment_data.loc[:,result_columns_test_set].median(axis=1)\n",
    "            scores_test_set = -experiment_data.loc[:,result_columns_test_set].values\n",
    "            rank_test_set = np.argsort(scores_test_set,axis=0)\n",
    "            rank_test_set = np.argsort(rank_test_set,axis=0).astype(np.float32)\n",
    "            rank_test_set[experiment_data.loc[:,result_columns].isnull()]=np.nan\n",
    "            experiment_data.loc[:,'rank%s' %suffix_test_set_i] = np.nanmean(rank_test_set,axis=1)\n",
    "            experiment_data.loc[:,'good%s' %suffix_test_set_i] = (experiment_data.loc[:,result_columns_test_set]>=0.1666666).sum(axis=1)\n",
    "            for k in scores_to_return.keys():\n",
    "                scores_to_return[k] += ['%s%s' %(k, suffix_test_set_i)]\n",
    "        if type(suffix_test_set) == str:\n",
    "            suffix_test_set = [suffix_test_set]\n",
    "        for suffix_test_set_i in suffix_test_set:\n",
    "            add_score_to_return(suffix_test_set_i)\n",
    "        \n",
    "    if output == 'all':\n",
    "        summary = dict (mean = experiment_data.loc[:,parameters+scores_to_return['mean']].sort_values(by='mean',ascending=ascending),\n",
    "                        median = experiment_data.loc[:,parameters+scores_to_return['median']].sort_values(by='median',ascending=ascending),\n",
    "                        rank = experiment_data.loc[:,parameters+scores_to_return['rank']].sort_values(by='rank'),\n",
    "                        good = experiment_data.loc[:,parameters+scores_to_return['good']].sort_values(by='good',ascending=False),\n",
    "                        stats = experiment_data.loc[:,parameters+stats].sort_values(by='mean',ascending=ascending),\n",
    "                        unordered = experiment_data.loc[:,parameters],\n",
    "                        allcols = experiment_data,\n",
    "                        original = experiment_data_original\n",
    "                        )\n",
    "    elif output == 'stats':\n",
    "        summary = experiment_data.loc[:,parameters+['mean','median','rank']]\n",
    "    elif output == 'unordered':\n",
    "        summary = experiment_data.loc[:,parameters]\n",
    "    elif output == 'allcols':\n",
    "        summary = experiment_data\n",
    "    elif output == 'original':\n",
    "        summary = experiment_data_original\n",
    "    else:\n",
    "        summary = experiment_data.loc[:,parameters+[output]].sort_values(by=output, ascending=output=='rank')\n",
    "        \n",
    "\n",
    "    return summary\n",
    "    \n",
    "def summary (df, experiments = None, score=None, compact=True):\n",
    "    if experiments is not None:\n",
    "        df = df.loc[experiments]\n",
    "    if compact:\n",
    "        _, df = get_parameters_unique(df)\n",
    "    parameters_columns = get_parameters_columns(df, True)\n",
    "    scores_columns = ut.get_scores_columns (df, suffix_results=score)\n",
    "    df = df.loc[experiments,parameters_columns + scores_columns]\n",
    "    df = df.rename (columns={'0_%s' %score: score})\n",
    "    return df\n",
    "    \n",
    "def get_parameters_columns (experiment_data, only_not_null=False):\n",
    "    parameters =  [par for par in experiment_data.columns if not par[0].isdigit() and (par.find('time_')<0) and (par.find('date')<0)]\n",
    "    if only_not_null:\n",
    "        parameters = np.array(parameters)[~experiment_data.loc[:,parameters].isnull().all(axis=0)].tolist()\n",
    "    return parameters\n",
    "\n",
    "def get_experiment_parameters (experiment_data, only_not_null=False):\n",
    "    return experiment_data[get_parameters_columns (experiment_data, only_not_null=only_not_null)]\n",
    "    \n",
    "def get_scores_columns (experiment_data=None, suffix_results='', class_ids = None):\n",
    "    ''' Determine the columnns that provide evaluation scores. We assume that they start with the class number, and that the other columns do not start with a digit'''\n",
    "    if class_ids is not None:\n",
    "        scores_columns = ['%d%s' %(col,suffix_results) for col in class_ids]\n",
    "    else:\n",
    "        if experiment_data is None:\n",
    "            raise ValueError ('Either experiment_data or class_ids should be different than None')\n",
    "        scores_columns = [col for col in experiment_data.columns if col[0].isdigit()]\n",
    "        # For some experiments, we have multiple scores per class (e.g., due to different evaluation criteria). The argument suffix_results can be used to select the appropriate score.\n",
    "        if len(suffix_results) > 0:\n",
    "            scores_columns = [col for col in scores_columns if (len(col.split(suffix_results))==2) and (len(col.split(suffix_results)[1])==0) and (col.split(suffix_results)[0].isdigit()) ]\n",
    "        else:\n",
    "            # We assume that default scores are in columns whose names only have the class number \n",
    "            scores_columns = [col for col in scores_columns if (len(col.split('_'))==1)]\n",
    "    return scores_columns\n",
    "    \n",
    "def get_experiment_scores (experiment_data = None, suffix_results = '', class_ids = None, remove_suffix=False):\n",
    "    df = experiment_data[get_scores_columns (experiment_data, suffix_results=suffix_results, class_ids=class_ids)]\n",
    "    if remove_suffix:\n",
    "        df.columns=[c.split('_')[0] for c in df.columns]\n",
    "    return df\n",
    "    \n",
    "def find_rows_with_parameters_dict (experiment_data, parameters_dict, create_if_not_exists=True, exact_match=True, ignore_keys=[], precision = 1e-10):\n",
    "    \"\"\" Finds rows that match parameters. If the dataframe doesn't have any parameter with that name, a new column is created and changed_dataframe is set to True.\"\"\"\n",
    "    changed_dataframe = False\n",
    "    matching_all_condition = pd.Series([True]*experiment_data.shape[0])\n",
    "    existing_keys = [par for par in parameters_dict.keys() if par not in ignore_keys]\n",
    "    for parameter in existing_keys:\n",
    "        if parameter not in experiment_data.columns:\n",
    "            if create_if_not_exists:\n",
    "                experiment_data[parameter] = None\n",
    "                changed_dataframe = True\n",
    "            else:\n",
    "                raise ValueError ('parameter %s not found in experiment_data' %parameter)\n",
    "        if parameters_dict[parameter] is None:\n",
    "            matching_condition = experiment_data[parameter].isnull()\n",
    "        elif experiment_data[parameter].isnull().all():\n",
    "            matching_condition = ~experiment_data[parameter].isnull()\n",
    "        elif (type(parameters_dict[parameter]) == float) or (type(parameters_dict[parameter]) == np.float32) or (type(parameters_dict[parameter]) == np.float64):\n",
    "            if parameters_dict[parameter] == np.floor(parameters_dict[parameter]):\n",
    "                matching_condition = experiment_data[parameter]==parameters_dict[parameter]\n",
    "            else:\n",
    "                matching_condition = experiment_data[parameter]==parameters_dict[parameter]\n",
    "                for idx, v in enumerate(experiment_data[parameter]):\n",
    "                    if (type(v) == float or type(v) == np.float32 or type(v) == np.float64) and (np.abs(v-parameters_dict[parameter]) < precision):\n",
    "                        matching_condition[idx]=True\n",
    "                    else:\n",
    "                        matching_condition[idx]=False\n",
    "        else:\n",
    "            matching_condition = experiment_data[parameter]==parameters_dict[parameter]\n",
    "\n",
    "        matching_all_condition = matching_all_condition & matching_condition.values\n",
    "            \n",
    "    # We assume that all the columns correspond to parameters, except for those that start with a digit (corresponding to the class evaluated) and those that start with time (giving an estimation of the computational cost)\n",
    "    if exact_match:\n",
    "        rest_parameters = get_parameters_columns (experiment_data)\n",
    "        rest_parameters = [par for par in rest_parameters if par not in parameters_dict.keys()]\n",
    "        rest_parameters = [par for par in rest_parameters if par not in ignore_keys]\n",
    "        for parameter in rest_parameters:\n",
    "            matching_condition = experiment_data[parameter].isnull()\n",
    "            matching_all_condition = matching_all_condition & matching_condition.values\n",
    "    \n",
    "    matching_rows = matching_all_condition.index[matching_all_condition].tolist()\n",
    "    \n",
    "    return matching_rows, changed_dataframe, matching_all_condition\n",
    "\n",
    "def get_classes_with_results (experiment_data = None, suffix_results = '', class_ids = None):\n",
    "    '''Gets the list of class_ids for whom there are results in experiment_data.\n",
    "    \n",
    "    Example usage with summarize_results:\n",
    "        import hpsearch.utils.vc_experiment_utils as ut\n",
    "        d=ut.summarize_results(class_ids='qualified',suffix_results='_auc_roc', min_results=10);\n",
    "        ch=ut.get_classes_with_results(d['original'].loc[d['mean'].index],suffix_results='_auc_roc');\n",
    "    '''\n",
    "    result_columns = get_scores_columns (experiment_data, suffix_results=suffix_results, class_ids=class_ids)\n",
    "    completed_results = ~experiment_data.loc[:,result_columns].isnull()\n",
    "    completed_results = completed_results.all(axis=0)\n",
    "    completed_results = completed_results.iloc[np.where(completed_results)]\n",
    "    completed_results = completed_results.index\n",
    "\n",
    "    return [int(x[:-len(suffix_results)]) for x in completed_results]\n",
    "\n",
    "def get_parameters_unique(df):\n",
    "    parameters = []\n",
    "    for k in df.columns:\n",
    "        if len(df[k].unique()) > 1:\n",
    "            parameters += [k]\n",
    "    return parameters, df[parameters]\n",
    "\n",
    "def compact_parameters (df, number_characters=1):\n",
    "    par_or = df.columns\n",
    "    par_new = [''.join(y[0].upper()+y[1:number_characters] for y in x.split('_')) for x in par_or]\n",
    "    dict_rename = {k:v for k,v in zip(par_or, par_new)}\n",
    "    df = df.rename (columns = dict_rename)\n",
    "    \n",
    "    return df, dict_rename\n",
    "\n",
    "def replace_with_default_values (df, parameters={}):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    defaults = get_default_parameters(parameters=parameters)\n",
    "    for k in df.columns:\n",
    "        df.loc[df[k].isna(), k] = defaults.get(k)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted default_parameters.ipynb.\n",
      "Converted get_paths.ipynb.\n",
      "Converted hpconfig.ipynb.\n",
      "Converted manager_factory.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted example_experiment.ipynb.\n",
      "Converted example_experiment_manager.ipynb.\n",
      "Converted experiment_manager.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted experiment_utils.ipynb.\n",
      "Converted organize_experiments.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script(recursive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ra)",
   "language": "python",
   "name": "ra"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
