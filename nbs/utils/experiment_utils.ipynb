{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp utils.experiment_utils\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "#tst = TestRunner (targets=['dummy'])\n",
    "tst = TestRunner (targets=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Utils\n",
    "\n",
    "> Helper functions for querying and retrieving results from past experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "\n",
    "from hpsearch.config import hp_defaults as dflt\n",
    "from hpsearch.config.hpconfig import get_experiment_manager\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from dsblocks.utils.nbdev_utils import md\n",
    "from hpsearch.examples.dummy_experiment_manager import (DummyExperimentManager, \n",
    "                                                        run_multiple_experiments)\n",
    "from hpsearch.examples.complex_dummy_experiment_manager import generate_data, init_em\n",
    "from hpsearch.config import hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.utils.test_experiment_utils\n",
    "def generate_data_exp_utils (name_folder):\n",
    "    path_experiments = f'test_{name_folder}/debug'\n",
    "    manager_path = f'{path_experiments}/managers'\n",
    "    em = DummyExperimentManager (path_experiments=path_experiments, manager_path=manager_path,\n",
    "                                 verbose=0)\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    run_multiple_experiments(em=em, nruns=5, noise=0.1, verbose=False,\n",
    "                             parameters_multiple_values=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 10, 100]))\n",
    "    run_multiple_experiments(em=em, nruns=5, noise=0.1, verbose=False, rate=0.0001,\n",
    "                             parameters_multiple_values=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 10, 100]))\n",
    "    return em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_df (path, name='experiments_data'):\n",
    "    path_csv = f'{path}/{name}.csv'\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    path_columns_pickle = path_csv.replace('.csv', '_columns.pk')\n",
    "    experiment_data = None\n",
    "    try:\n",
    "        experiment_data = pd.read_pickle (path_pickle)\n",
    "    except:\n",
    "        try:\n",
    "            experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "            if os.path.exists (path_columns_pickle):\n",
    "                c = joblib.load (path_columns_pickle)\n",
    "                experiment_data.columns = pd.MultiIndex.from_tuples (c)\n",
    "        except:\n",
    "            experiment_data = None\n",
    "    return experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_df (df, path, name='experiments_data'):\n",
    "    path_csv = f'{path}/{name}.csv'\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    path_columns_pickle = path_csv.replace('.csv', '_columns.pk')\n",
    "    df.to_pickle (path_pickle)\n",
    "    df.to_csv (path_csv)\n",
    "    c = df.columns.tolist ()\n",
    "    joblib.dump (c, path_columns_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_binary_df_if_not_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_binary_df_if_not_exists (df, path, name='experiments_data'):\n",
    "    path_pickle = f'{path}/{name}.pk'\n",
    "    df.to_pickle (path_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_data (experiments=None):\n",
    "    \"\"\"\n",
    "    Returns data stored from previous experiments in the form DataFrame. \n",
    "    \n",
    "    If path_experiments is not given, it uses the default one. \n",
    "    \"\"\"\n",
    "    from hpsearch.config.hpconfig import get_experiment_data\n",
    "    return get_experiment_data (experiments=experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_experiment_data ():\n",
    "    path_experiments = 'get_experiment_data'\n",
    "    em = generate_data (path_experiments)\n",
    "    \n",
    "    df = get_experiment_data ()\n",
    "    reference = em.get_experiment_data ()\n",
    "    pd.testing.assert_frame_equal (df, reference)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_experiment_data, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get experiment parameters and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_parameters_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_columns (experiment_data, only_not_null=False):\n",
    "    parameters = experiment_data[dflt.parameters_col].columns\n",
    "    parameters = [(dflt.parameters_col, *x) for x in parameters]\n",
    "    if only_not_null:\n",
    "        parameters = np.array(parameters)[~experiment_data.loc[:,parameters].isnull().all(axis=0)].tolist()\n",
    "        parameters = [(*x,) for x in parameters]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_experiment_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_parameters (experiment_data, only_not_null=False):\n",
    "    return experiment_data[get_parameters_columns (experiment_data, only_not_null=only_not_null)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_scores_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_columns (experiment_data=None, score_name=None, run_number=None):\n",
    "    \"\"\"\n",
    "    Determine the columnns that provide evaluation scores.\n",
    "    \"\"\"\n",
    "    if score_name is None and experiment_data is None:\n",
    "        raise ValueError ('Either experiment_data or run_number should be different than None')\n",
    "    if score_name is not None and not isinstance(score_name, list):\n",
    "        score_name = [score_name]\n",
    "    if run_number is not None and not isinstance(run_number, list):\n",
    "        if isinstance(run_number, range): run_number=list(run_number)\n",
    "        else: run_number = [run_number]\n",
    "    if score_name is not None:\n",
    "        scores_columns = []\n",
    "        for score in score_name:\n",
    "            new_columns = experiment_data[dflt.scores_col, score].columns\n",
    "            if run_number is not None:\n",
    "                new_columns = list(set(new_columns).intersection (run_number))\n",
    "            new_columns = [(dflt.scores_col, score, c) for c in new_columns]\n",
    "            scores_columns.extend (new_columns)\n",
    "    else:\n",
    "        scores_columns = experiment_data[dflt.scores_col].columns\n",
    "        scores_columns = [(dflt.scores_col, *x) for x in scores_columns]\n",
    "    return scores_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### get_experiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_scores (experiment_data = None, score_name=None, run_number=None, remove_score_name=False):\n",
    "    df = experiment_data[get_scores_columns (experiment_data, score_name=score_name, run_number=run_number)]\n",
    "    if remove_score_name: df.columns = df.columns.get_level_values(2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_parameters_and_scores ():\n",
    "    path_experiments = 'test_get_parameters_and_scores'\n",
    "    em = generate_data (path_experiments)\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_parameters_columns\n",
    "    # ************************************************************\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'offset', 'rate']]\n",
    "    assert get_parameters_columns (df) == expected_result\n",
    "\n",
    "    mi_offset = (dflt.parameters_col, 'offset', '')\n",
    "    offset = df[mi_offset].values.copy()\n",
    "    md ('- We can take only those which have at least some value that is not None.')\n",
    "    df.loc[:, mi_offset] = None\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'rate']]\n",
    "    assert get_parameters_columns (df, only_not_null=True) == expected_result\n",
    "\n",
    "    md ('- If only some elements are None for a given parameter, we still include it.')\n",
    "    df.loc[:, mi_offset] = offset\n",
    "    df.loc[2, mi_offset] = None\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'offset', 'rate']]\n",
    "    assert get_parameters_columns (df, only_not_null=True)==expected_result\n",
    "    df.loc[:, mi_offset] = offset\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_experiment_parameters\n",
    "    # ************************************************************\n",
    "    md ('- Same as get_parameters_columns, but returning dataframe of parameter values.')\n",
    "    result = get_experiment_parameters (df)\n",
    "    assert result.shape == (9, 4)\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'offset', 'rate']]\n",
    "    assert result.columns.tolist() == expected_result\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_scores_columns\n",
    "    # ************************************************************\n",
    "    md ('- Retrieve all columns that have scores, for all runs')\n",
    "    expected_result = [(dflt.scores_col, x, y) for x in ['test_accuracy', 'validation_accuracy']\n",
    "                       for y in range(5)]\n",
    "    assert get_scores_columns (df) == expected_result\n",
    "\n",
    "    md ('- Retrieve all columns for given score name, for all runs')\n",
    "    expected_result = [('scores', 'test_accuracy', 0), ('scores', 'test_accuracy', 1), \n",
    "                       ('scores', 'test_accuracy', 2), ('scores', 'test_accuracy', 3), \n",
    "                       ('scores', 'test_accuracy', 4)]\n",
    "    assert get_scores_columns (df, score_name='test_accuracy') == expected_result\n",
    "\n",
    "    md ('- Retrieve all columns for given score name, for given runs')\n",
    "    expected_result = [(dflt.scores_col, x, y) for x in ['test_accuracy'] \n",
    "                       for y in [2, 4]]\n",
    "    assert get_scores_columns (df, score_name='test_accuracy', run_number=[2, 4]) == expected_result\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_experiment_scores\n",
    "    # ************************************************************\n",
    "    md ('- Same, but returning dataframe with selected scores values:')\n",
    "    result = get_experiment_scores (df)\n",
    "    display (result)\n",
    "    assert result.shape==(9,10)\n",
    "\n",
    "    result = get_experiment_scores (df, score_name='test_accuracy')\n",
    "    display (result)\n",
    "    assert result.shape==(9,5)\n",
    "\n",
    "    result = get_experiment_scores (df, score_name='test_accuracy', run_number=[2,4])\n",
    "    display (result)\n",
    "    assert result.shape==(9,2)\n",
    "\n",
    "    md ('- We can remove the metric name and only keep the run number in each column:')\n",
    "    result = get_experiment_scores (df, score_name='test_accuracy', run_number=[2,4], remove_score_name=True)\n",
    "    display (result)\n",
    "    assert result.shape==(9,2)\n",
    "    \n",
    "    # ************************************************************\n",
    "    # get_scores_columns, first usage example: we do not indicate the name of the score\n",
    "    # ************************************************************\n",
    "    expected_result = [(dflt.scores_col, x, y) for x in ['test_accuracy', 'validation_accuracy'] \n",
    "                       for y in range(5)]\n",
    "    assert get_scores_columns (df)==expected_result\n",
    "    \n",
    "    # ************************************************************\n",
    "    # get_scores_columns, second usage: we indicate the name of the score\n",
    "    # ************************************************************\n",
    "    result = get_scores_columns (df, run_number=range(5), score_name='validation_accuracy')\n",
    "    expected_result = [(dflt.scores_col, 'validation_accuracy', y) for y in range(5)]\n",
    "    assert result == expected_result\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_parameters_and_scores, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_scores_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isnull (x): return x is None or np.isnan(x)\n",
    "\n",
    "def get_scores_names (experiment_data=None, run_number=None, experiment=None, only_valid=True):\n",
    "    \"\"\" \n",
    "    Determine the names of the scores included in experiment data. \n",
    "    \n",
    "    If run_number is provided, we provide the scores stored for that run number. If, in addition to this, \n",
    "    experiment is provided, and only_valid=True, we provide only the scores that are not NaN for the given \n",
    "    experiment number.\n",
    "    \"\"\"\n",
    "    \n",
    "    if run_number is None:\n",
    "        scores_names = experiment_data[dflt.scores_col].columns.get_level_values(0).unique()\n",
    "    else:\n",
    "        if not isinstance(run_number, list):\n",
    "            if isinstance(run_number, range): run_number=list(run_number)\n",
    "            else: run_number = [run_number]\n",
    "        scores_names = [(dflt.scores_col, *c) for c in experiment_data[dflt.scores_col].columns \n",
    "                        if c[1] in run_number]\n",
    "        if (experiment is not None) and only_valid:\n",
    "            scores_names = [c for c in scores_names if not isnull(experiment_data.loc[experiment, c])]\n",
    "        scores_names = pd.MultiIndex.from_tuples(scores_names).get_level_values(1).unique()\n",
    "    scores_names = list(np.sort(scores_names))\n",
    "    return scores_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_scores_names ():\n",
    "    em = generate_data_exp_utils ('get_scores_names')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    scores_names = get_scores_names (df)\n",
    "    print (scores_names)\n",
    "    assert scores_names == ['test_accuracy', 'validation_accuracy']\n",
    "    \n",
    "    scores_names=get_scores_names (df, run_number=3, experiment=7)\n",
    "    print(scores_names)\n",
    "    assert list(np.sort(scores_names))==['test_accuracy', 'validation_accuracy']\n",
    "\n",
    "    # test when only some scores are valid\n",
    "    df2 = df.copy()\n",
    "    df2.loc[7, (dflt.scores_col, 'test_accuracy', 3)]=np.nan\n",
    "    scores_names=get_scores_names (df2, run_number=3, experiment=7)\n",
    "    print (scores_names)\n",
    "    assert scores_names==['validation_accuracy']\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_scores_names, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_monitored_training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_monitored_training_metrics (experiment, run_number=0, history_file_name='model_history.pk', \n",
    "                                    path_results=None):\n",
    "    if path_results is None:\n",
    "        from hpsearch.config.hpconfig import get_path_results\n",
    "        path_results = get_path_results(experiment, run_number)\n",
    "    path_history = f'{path_results}/{history_file_name}'\n",
    "    if os.path.exists(path_history):\n",
    "        history=joblib.load(path_history)\n",
    "        return list(history.keys())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_monitored_training_metrics ():\n",
    "    em = generate_data_exp_utils ('get_monitored_training_metrics')\n",
    "    \n",
    "    monitored_metrics = get_monitored_training_metrics (0)\n",
    "    print (monitored_metrics)\n",
    "    assert monitored_metrics==['validation_accuracy', 'test_accuracy', 'accuracy']\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_monitored_training_metrics, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_runs_with_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_runs_with_results (experiment_data = None, score_name=None, run_number=None):\n",
    "    \"\"\"\n",
    "    Gets the list of run_number for whom there are results in experiment_data.\n",
    "    \"\"\"\n",
    "    assert experiment_data is not None, 'experiment_data must be introduced'\n",
    "    result_columns = get_scores_columns (experiment_data, score_name=score_name, run_number=run_number)\n",
    "    completed_results = ~experiment_data.loc[:,result_columns].isnull()\n",
    "    completed_results = completed_results.all(axis=0)\n",
    "    completed_results = completed_results.iloc[np.where(completed_results)]\n",
    "    completed_results = completed_results.index\n",
    "    return completed_results.get_level_values(2).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_runs_with_results ():\n",
    "    em = generate_data ('get_runs_with_results')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    # we need to introduce experiment_data df, and score_name\n",
    "    result = get_runs_with_results (df, score_name='validation_accuracy')\n",
    "    display (result)\n",
    "    assert result==[0,1,2,3,4]\n",
    "    \n",
    "    # we can also restrict to certain run_number\n",
    "    result = get_runs_with_results (df, score_name='validation_accuracy', run_number=[0,2])\n",
    "    display (result)\n",
    "    assert result==[0,2]\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_runs_with_results, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## get_parameters_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_unique(df):\n",
    "    assert df.columns.nlevels == 3\n",
    "    df_all = df\n",
    "    df = df[dflt.parameters_col]\n",
    "    if df.shape[0] > 1:\n",
    "        parameters = []\n",
    "        for k in df.columns:\n",
    "            if len(df[k].unique()) > 1:\n",
    "                parameters += [k]\n",
    "    else:\n",
    "        parameters = df.columns.tolist()\n",
    "    df_parameters = df[parameters]\n",
    "    columns = pd.MultiIndex.from_tuples([(dflt.parameters_col, *c) for c in parameters])\n",
    "    df_parameters.columns = columns\n",
    "    all_cols = df_all.columns.get_level_values(0).unique()\n",
    "    no_par_cols = all_cols [all_cols != dflt.parameters_col]\n",
    "    df_no_par = df_all[no_par_cols]\n",
    "    df_all = pd.concat([df_parameters, df_no_par], axis=1)\n",
    "    return columns, df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_parameters_unique ():\n",
    "    em = generate_data_exp_utils ('get_parameters_unique')\n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    # keeps only those parameters with more than one value,\n",
    "    # removing 'noise' in this case, since it has the same value in all rows\n",
    "    result = get_parameters_unique (df)\n",
    "    assert result[1].shape==(18,28)\n",
    "    assert result[0].tolist() == [(dflt.parameters_col, 'epochs', ''), (dflt.parameters_col, 'offset', ''), \n",
    "                         (dflt.parameters_col, 'rate', '')]\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_parameters_unique, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true",
    "tags": []
   },
   "source": [
    "## compact_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compact_parameters (df, number_characters=1):\n",
    "    par_or = df.columns.get_level_values(1)\n",
    "    par_new = [''.join(y[0].upper()+y[1:number_characters] for y in x.split('_')) for x in par_or]\n",
    "    dict_rename = {k:v for k,v in zip(par_or, par_new)}\n",
    "    if df.columns.nlevels==3:\n",
    "        df.columns = pd.MultiIndex.from_arrays ([df.columns.get_level_values(0), par_new, \n",
    "                                             df.columns.get_level_values(2)])\n",
    "    else:\n",
    "        df.columns = pd.MultiIndex.from_arrays ([df.columns.get_level_values(0), par_new])\n",
    "    \n",
    "    return df, dict_rename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_compact_parameters ():\n",
    "    em = generate_data_exp_utils ('compact_parameters')\n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    result = compact_parameters (df, number_characters=2)\n",
    "    display (result[0].head())\n",
    "    assert result[0].columns.tolist() == [('parameters',   'Ep', ''),\n",
    "            ('parameters',   'No', ''),\n",
    "            ('parameters',   'Of', ''),\n",
    "            ('parameters',   'Ra', ''),\n",
    "            (  'run_info',   'Da',  0),\n",
    "            (  'run_info',   'Da',  1),\n",
    "            (  'run_info',   'Da',  2),\n",
    "            (  'run_info',   'Da',  3),\n",
    "            (  'run_info',   'Da',  4),\n",
    "            (  'run_info',   'Fi',  0),\n",
    "            (  'run_info',   'Fi',  1),\n",
    "            (  'run_info',   'Fi',  2),\n",
    "            (  'run_info',   'Fi',  3),\n",
    "            (  'run_info',   'Fi',  4),\n",
    "            (  'run_info',   'Ti',  0),\n",
    "            (  'run_info',   'Ti',  1),\n",
    "            (  'run_info',   'Ti',  2),\n",
    "            (  'run_info',   'Ti',  3),\n",
    "            (  'run_info',   'Ti',  4),\n",
    "            (    'scores', 'TeAc',  0),\n",
    "            (    'scores', 'TeAc',  1),\n",
    "            (    'scores', 'TeAc',  2),\n",
    "            (    'scores', 'TeAc',  3),\n",
    "            (    'scores', 'TeAc',  4),\n",
    "            (    'scores', 'VaAc',  0),\n",
    "            (    'scores', 'VaAc',  1),\n",
    "            (    'scores', 'VaAc',  2),\n",
    "            (    'scores', 'VaAc',  3),\n",
    "            (    'scores', 'VaAc',  4)]\n",
    "\n",
    "    assert result[1]=={'epochs': 'Ep', 'noise': 'No', 'offset': 'Of', 'rate': 'Ra', 'date': 'Da', \n",
    "                       'finished': 'Fi', 'time': 'Ti', 'test_accuracy': 'TeAc', 'validation_accuracy': 'VaAc'}\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_compact_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## replace_with_default_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_with_default_values (df, parameters={}):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    parameters_names = get_parameters_columns (df)\n",
    "    \n",
    "    for k in df.columns:\n",
    "        experiments_idx=np.argwhere(df[k].isna().ravel()).ravel()\n",
    "        experiments=df.index[experiments_idx]\n",
    "        for experiment in experiments:\n",
    "            parameters = df.loc[experiment, parameters_names].copy()\n",
    "            parameters[parameters.isna().values] = None\n",
    "            parameters = parameters.to_dict()\n",
    "            parameters = {c[1]:parameters[c] for c in parameters if parameters[c] is not None}\n",
    "            defaults = get_default_parameters(parameters)\n",
    "            df.loc[experiment, k] = defaults.get(k[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_replace_with_default_values ():\n",
    "    em = generate_data_exp_utils ('replace_with_default_values')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    df=replace_with_default_values(df)\n",
    "    mi_epoch = (dflt.parameters_col, 'epochs', '')\n",
    "    assert (df[mi_epoch].values == ([5.]*3 + [10.]*3 + [100.]*3)*2).all()\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_replace_with_default_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_defaults (parameters):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    defaults = get_default_parameters(parameters)\n",
    "    for key in defaults.keys():\n",
    "        if key in parameters.keys() and (parameters[key] == defaults[key]):\n",
    "            del parameters[key]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_remove_defaults ():\n",
    "    em = init_em ('remove_defaults')\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.1, 'rate': 0.05})\n",
    "    assert parameters=={'offset':0.1, 'rate': 0.05}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.1, 'rate': 0.01, 'epochs': 10})\n",
    "    assert parameters=={'offset':0.1}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.5, 'rate': 0.000001, 'epochs': 10})\n",
    "    assert parameters=={'rate': 0.000001, 'epochs': 10}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.5, 'rate': 0.000001, 'epochs': 100})\n",
    "    assert parameters=={'rate': 0.000001}\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_remove_defaults, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## find_rows_with_parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_rows_with_parameters_dict (experiment_data, parameters_dict, create_if_not_exists=True, \n",
    "                                    exact_match=True, ignore_keys=[], precision = 1e-10):\n",
    "    \"\"\"\n",
    "    Finds rows that match parameters. \n",
    "    \n",
    "    If the dataframe doesn't have any parameter with that name, a new column \n",
    "    is created and changed_dataframe is set to True.\n",
    "    \"\"\"\n",
    "    changed_dataframe = False\n",
    "    matching_all_condition = pd.Series([True]*experiment_data.shape[0])\n",
    "    existing_keys = [par for par in parameters_dict.keys() if par not in ignore_keys]\n",
    "    for parameter in existing_keys:\n",
    "        mi_parameter = (dflt.parameters_col, parameter, '')\n",
    "        if mi_parameter not in experiment_data.columns:\n",
    "            if create_if_not_exists:\n",
    "                experiment_data[mi_parameter] = None\n",
    "                changed_dataframe = True\n",
    "            else:\n",
    "                raise ValueError ('parameter %s not found in experiment_data' %parameter)\n",
    "        \n",
    "        if experiment_data[mi_parameter].dtype == np.dtype('O'):\n",
    "            idx_true = experiment_data[mi_parameter] == 'True'\n",
    "            idx_false = experiment_data[mi_parameter] == 'False'\n",
    "            experiment_data.loc[idx_true, mi_parameter]=True\n",
    "            experiment_data.loc[idx_false, mi_parameter]=False\n",
    "            try:\n",
    "                experiment_data[mi_parameter] = pd.to_numeric(experiment_data[mi_parameter])\n",
    "                experiment_data.loc[idx_true, mi_parameter]=True\n",
    "                experiment_data.loc[idx_false, mi_parameter]=False\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if parameters_dict[parameter] is None:\n",
    "            matching_condition = experiment_data[mi_parameter].isnull()\n",
    "        elif experiment_data[mi_parameter].isnull().all():\n",
    "            matching_condition = ~experiment_data[mi_parameter].isnull()\n",
    "        elif ((type(parameters_dict[parameter]) == float) or \n",
    "              (type(parameters_dict[parameter]) == np.float32) or \n",
    "              (type(parameters_dict[parameter]) == np.float64)):\n",
    "            if parameters_dict[parameter] == np.floor(parameters_dict[parameter]):\n",
    "                matching_condition = experiment_data[mi_parameter]==parameters_dict[parameter]\n",
    "            else:\n",
    "                matching_condition = experiment_data[mi_parameter]==parameters_dict[parameter]\n",
    "                for idx, v in enumerate(experiment_data[mi_parameter]):\n",
    "                    if (type(v) == float or type(v) == np.float32 or type(v) == np.float64) and (np.abs(v-parameters_dict[parameter]) < precision):\n",
    "                        matching_condition.iloc[idx]=True\n",
    "                    else:\n",
    "                        matching_condition.iloc[idx]=False\n",
    "        else:\n",
    "            matching_condition = experiment_data[mi_parameter]==parameters_dict[parameter]\n",
    "\n",
    "        matching_all_condition = matching_all_condition & matching_condition.values\n",
    "            \n",
    "    if exact_match:\n",
    "        rest_parameters = experiment_data[dflt.parameters_col].columns.get_level_values(0)\n",
    "        rest_parameters = [par for par in rest_parameters if par not in parameters_dict.keys()]\n",
    "        rest_parameters = [par for par in rest_parameters if par not in ignore_keys]\n",
    "        for parameter in rest_parameters:\n",
    "            mi_parameter = (dflt.parameters_col, parameter, '')\n",
    "            matching_condition = experiment_data[mi_parameter].isnull()\n",
    "            matching_all_condition = matching_all_condition & matching_condition.values\n",
    "    \n",
    "    matching_rows = matching_all_condition.index[matching_all_condition].tolist()\n",
    "    \n",
    "    return matching_rows, changed_dataframe, matching_all_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_find_rows_with_parameters_dict ():\n",
    "    em = generate_data_exp_utils ('find_rows_with_parameters_dict')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001))\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[]\n",
    "    assert not changed_dataframe\n",
    "    \n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows == [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    \n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001, epochs=5, offset=0.6), exact_match=False,\n",
    "                                        ignore_keys=['epochs'])\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[11, 14, 17]\n",
    "\n",
    "    mi_rate = (dflt.parameters_col, 'rate', '')\n",
    "    df.loc[16, mi_rate]=0.00011\n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[9, 10, 11, 12, 13, 14, 15, 17]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001), exact_match=False, precision = 0.0001)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (new_par=4), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert changed_dataframe\n",
    "    assert df.shape == (18, 30)\n",
    "    assert matching_rows==[]\n",
    "    assert (dflt.parameters_col, 'new_par', '') in df.columns\n",
    "    assert matching_rows==[]\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_find_rows_with_parameters_dict, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_find_rows_with_parameters_dict_corner ():\n",
    "    df = pd.DataFrame ({'a': ['1.0', '0.0', 'False', '1', '1', 'True'],\n",
    "                        'b': [1,     2,     'yes',   'no',  1,   True],\n",
    "                        'c': ['a',     'b',     'yes',   'no',  'c',  'd']})\n",
    "    df.columns = pd.MultiIndex.from_tuples ([('parameters','a',''),\n",
    "                                             ('parameters','b',''),\n",
    "                                             ('parameters','c','')])\n",
    "    result = find_rows_with_parameters_dict (df, dict (a=False, c='yes'), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[2]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (a=False, c='no'), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (a=True, b=True), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[0, 4, 5]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (a=0.0), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_find_rows_with_parameters_dict_corner\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_find_rows_with_parameters_dict_corner, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarize_results(intersection=False, \n",
    "                      experiments=None, \n",
    "                      score_name=None, \n",
    "                      min_results=0, \n",
    "                      run_number=None, \n",
    "                      parameters=None,\n",
    "                      include_parameters=True,\n",
    "                      include_num_results=True,\n",
    "                      other_columns=None,\n",
    "                      data=None,\n",
    "                      ascending=False,\n",
    "                      sort_key='mean',\n",
    "                      #stats = ['mean','median','rank','min','max','std'],\n",
    "                      stats = ['mean','median','min','max','std']):\n",
    "    \"\"\"\n",
    "    Obtains summary scores for the desired list of experiments. \n",
    "    \n",
    "    Uses the experiment_data csv for that purpose.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if data is None:\n",
    "        experiment_data = get_experiment_data ()\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        if experiments is not None:\n",
    "            experiment_data = experiment_data.loc[experiments,:]\n",
    "        if parameters is not None:\n",
    "            experiment_rows, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, \n",
    "                                                                    create_if_not_exists=False, \n",
    "                                                                    exact_match=False)\n",
    "            experiment_data = experiment_data.loc[experiment_rows]\n",
    "    else:\n",
    "        experiment_data = data.copy()\n",
    "        \n",
    "    # Determine the columnns that provide evaluation scores. \n",
    "    result_columns = get_scores_columns (experiment_data, score_name=score_name, run_number=run_number)\n",
    "    \n",
    "    # Determine num_results and select those with minimum number of runs\n",
    "    #num_results = (~experiment_data.loc[:,result_columns].isnull()).sum(axis=1, level=1)\n",
    "    num_results = (~experiment_data.loc[:,result_columns].isnull()).groupby(axis=1, level=1).sum()\n",
    "    num_results.columns = pd.MultiIndex.from_product ([[dflt.stats_col], num_results.columns.tolist(), \n",
    "                                                       ['num_results']])\n",
    "    experiment_data = pd.concat([experiment_data, num_results], axis=1)\n",
    "    num_results_columns = experiment_data.columns[\n",
    "        experiment_data.columns.get_level_values(2) == 'num_results'\n",
    "    ]\n",
    "    min_num_results = experiment_data[num_results_columns].min(axis=1)\n",
    "    experiment_data = experiment_data.drop (columns=num_results_columns)\n",
    "    num_results_column = (dflt.num_results_col, 'num_results', '')\n",
    "    experiment_data[num_results_column] = min_num_results\n",
    "    if min_results > 0:\n",
    "        number_before = experiment_data.shape[0]\n",
    "        experiment_data = experiment_data[min_num_results>=min_results]\n",
    "        print (f'{experiment_data.shape[0]} out of {number_before} experiments have {min_results} runs '\n",
    "               'completed')\n",
    "    \n",
    "    # Take only those run_number where all experiments provide some score\n",
    "    if intersection:\n",
    "        number_before = len(result_columns)\n",
    "        all_have_results = ~experiment_data.loc[:,result_columns].isnull().any(axis=0)\n",
    "        result_columns = (np.array(result_columns)[all_have_results]).tolist()\n",
    "        print (f'{len(result_columns)} out of {number_before} runs for whom all the '\n",
    "                'selected experiments have completed')\n",
    "        \n",
    "    print (f'total data examined: {experiment_data.shape[0]} experiments '\n",
    "           f'with at least {min_num_results.min()} runs done for each one')\n",
    "        \n",
    "    # TODO: make it work across different metrics\n",
    "    #scores = experiment_data.loc[:, result_columns]\n",
    "    #scores[scores.isna()]=np.nan\n",
    "    #scores = -scores.values\n",
    "    #rank = np.argsort(scores,axis=0)\n",
    "    #rank = np.argsort(rank,axis=0).astype(np.float32)\n",
    "    #rank[experiment_data.loc[:,result_columns].isnull()]=np.nan\n",
    "    \n",
    "    if other_columns != 'all':\n",
    "        if include_parameters:\n",
    "            columns_to_include = get_parameters_columns(experiment_data, True)\n",
    "        else:\n",
    "            columns_to_include = []\n",
    "        if include_num_results:\n",
    "            columns_to_include.append (num_results_column)\n",
    "        if other_columns is not None: \n",
    "            columns_to_include.extend(other_columns)\n",
    "    else:\n",
    "        columns_to_include = experiment_data.columns.tolist()\n",
    "    scores_to_return={}\n",
    "    stat_df_all = []\n",
    "    stats_columns=[]\n",
    "    for stat in stats:\n",
    "        stat_df = experiment_data.loc[:,result_columns].groupby (level=1, axis=1).agg(stat)\n",
    "        #stat_df = experiment_data.loc[:,result_columns].agg(stat, axis=1, level=1)\n",
    "        stat_df.columns = pd.MultiIndex.from_product (\n",
    "            [[dflt.stats_col], stat_df.columns.tolist(), [stat]])\n",
    "        scores_to_return[stat] = stat_df.columns.tolist()\n",
    "        stats_columns.extend (stat_df.columns.tolist())\n",
    "        stat_df_all.append (stat_df)\n",
    "    experiment_data = pd.concat ([experiment_data]+stat_df_all, axis=1)\n",
    "    if score_name is None: \n",
    "        score_name = experiment_data[dflt.scores_col].columns.get_level_values(0).unique()\n",
    "        score_name = score_name[0]\n",
    "    elif isinstance (score_name, list):\n",
    "        score_name = score_name[0]\n",
    "    summary = experiment_data.loc[:,columns_to_include+stats_columns]\n",
    "    sort_column = None\n",
    "    if sort_key is not None: \n",
    "        if sort_key in stats: \n",
    "            sort_column = (dflt.stats_col, score_name, sort_key)\n",
    "        elif sort_key in summary[dflt.parameters_col].columns.get_level_values(0):\n",
    "            sort_column = (dflt.parameters_col, sort_key, '')\n",
    "        elif (dflt.scores_col in summary.columns.get_level_values(0) and \n",
    "              sort_key in summary[dflt.scores_col].columns.get_level_values(0)):\n",
    "            run_number = summary[dflt.scores_col].columns.get_level_values(1)[0]\n",
    "            sort_column = (dflt.scores_col, sort_key, run_number)\n",
    "    if sort_column is not None:\n",
    "        summary = summary.sort_values(by=sort_column,ascending=ascending)\n",
    "    summary = summary[summary.columns.sort_values()]\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_summarize_results ():\n",
    "    em = init_em ('summarize_results')\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.1, rate=0.01), nruns=3)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.2, rate=0.001), nruns=5)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.3, rate=0.02), nruns=2)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    summary = summarize_results ()\n",
    "    display (summary)\n",
    "    mi_num_results = (dflt.num_results_col, 'num_results', '')\n",
    "    assert summary[mi_num_results].sum() == 10\n",
    "    assert summary.shape==(3, 13)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can restrict the metric to be the indicated one:')\n",
    "    summary = summarize_results (score_name='validation_accuracy')\n",
    "    display (summary)\n",
    "    assert summary[mi_num_results].sum() == 10\n",
    "    assert summary.shape==(3, 8)\n",
    "    assert set(['mean','median','min','max','std'])==set(summary[dflt.stats_col, 'validation_accuracy'].columns)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can indicate more than one metric:')\n",
    "    summary = summarize_results (score_name=['validation_accuracy', 'test_accuracy'])\n",
    "    display (summary)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can also restrict the stats to be provided:')\n",
    "    summary = summarize_results (score_name='validation_accuracy', stats=['mean', 'min', 'max'])\n",
    "    assert summary.shape == (3, 6)\n",
    "    assert set(['mean','min','max'])==set(summary[dflt.stats_col, 'validation_accuracy'].columns)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can filter those results that have less than X runs: ')\n",
    "    summary = summarize_results (score_name='validation_accuracy', min_results=5)\n",
    "    display (summary)\n",
    "    assert summary[mi_num_results].sum() == 5\n",
    "    assert summary.shape==(1, 8)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can filter by experiment number and/or number of results, and retrieve the original dataframe,'\n",
    "        'plus new columns with stats: ')\n",
    "    summary = summarize_results (score_name='validation_accuracy', experiments=[0,2])\n",
    "    display (summary)\n",
    "    assert summary.shape==(2, 8)\n",
    "    assert all(summary.index==[2, 0])\n",
    "    assert (sorted(summary.columns.get_level_values(1).unique().tolist())==\n",
    "            sorted(['offset', 'rate', 'num_results', 'validation_accuracy']))\n",
    "    assert summary['stats','validation_accuracy'].columns.tolist()==['max', 'mean', 'median', 'min', 'std']\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_summarize_results, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def query (path_experiments=None, \n",
    "              folder_experiments=None,\n",
    "              experiments=None,  \n",
    "              parameters_fixed={},\n",
    "              parameters_variable={},\n",
    "              parameters_all=[],\n",
    "              exact_match=True,\n",
    "              query_other_parameters=False,\n",
    "              em=None,\n",
    "              **kwargs):\n",
    "  \n",
    "    if em is None: em = get_experiment_manager ()\n",
    "    if path_experiments is None: path_experiments = em.path_experiments\n",
    "    \n",
    "    if query_other_parameters:\n",
    "        experiment_data = pd.read_csv(f'{path_experiments}/other_parameters.csv', index_col=0)\n",
    "    else:\n",
    "        experiment_data = em.get_experiment_data ()\n",
    "    if experiment_data is None:\n",
    "        return None\n",
    "    \n",
    "    non_valid_pars = set(\n",
    "        [(dflt.parameters_col, c, '') for c in parameters_fixed.keys()]\n",
    "    ).difference(set(experiment_data.columns))\n",
    "    non_valid_pars = non_valid_pars.union(\n",
    "        set([(dflt.parameters_col, c, '') for c in parameters_variable.keys()]\n",
    "    ).difference(set(experiment_data.columns)))\n",
    "    \n",
    "    if len(non_valid_pars) > 0:\n",
    "        print (f'\\n**The following query parameters are not valid: {list(non_valid_pars)}**')\n",
    "        print (f'\\nValid parameters:\\n{sorted(get_parameters_columns(experiment_data))}\\n')\n",
    "    \n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_variable))\n",
    "    experiment_numbers = []\n",
    "    for (i, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_fixed)\n",
    "        parameters_none = {k:v for k,v in parameters.items() if v is None}\n",
    "        parameters_not_none = {k:v for k,v in parameters.items() if v is not None}\n",
    "\n",
    "        parameters = remove_defaults (parameters_not_none)\n",
    "        parameters.update(parameters_none)\n",
    "    \n",
    "        experiment_numbers_i, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, \n",
    "                                                                     ignore_keys=parameters_all, \n",
    "                                                                     exact_match=exact_match)\n",
    "        experiment_numbers += experiment_numbers_i\n",
    "    \n",
    "    experiment_data = experiment_data.iloc[experiment_numbers]\n",
    "    \n",
    "    if experiments is not None:\n",
    "        experiment_data = experiment_data.loc[experiments]\n",
    "        \n",
    "    if query_other_parameters:\n",
    "        return experiment_data\n",
    "  \n",
    "    summary = summarize_results (data=experiment_data, **kwargs)\n",
    "                      \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_query ():\n",
    "    em = generate_data_exp_utils ('query')\n",
    "        \n",
    "    summary = query (parameters_fixed=dict (rate=0.0001))\n",
    "    assert summary.empty\n",
    "    \n",
    "    md ('the dataframe only has mean. Results are sorted by mean score')\n",
    "    summary = query (parameters_fixed=dict (rate=0.0001), exact_match=False)\n",
    "    par = lambda parameter: (dflt.parameters_col, parameter, '')\n",
    "    stat = lambda statv: (dflt.stats_col, 'test_accuracy', statv)\n",
    "    assert (summary.shape[0]==9 and (summary[par('rate')]==0.0001).all() and \n",
    "            len(summary[par('offset')].unique())==3 and \n",
    "            summary[stat('mean')].iloc[0]>summary[stat('mean')].iloc[1] \n",
    "            and summary[stat('mean')].iloc[1] > summary[stat('mean')].iloc[2])\n",
    "    \n",
    "    display (summary)\n",
    "    md ('The second output d contains a field \"stats\" which is a dataframe. Results are sorted by mean score')\n",
    "    assert (summary['stats','validation_accuracy'].columns.tolist()==[\n",
    "        'max', 'mean', 'median', 'min', 'std'])\n",
    "    assert summary.shape==(9, 15)\n",
    "    \n",
    "    md ('We can request parameter be in specific list of values')   \n",
    "    summary = query (parameters_fixed=dict(rate=0.0001), exact_match=False, \n",
    "                  parameters_variable=dict(epochs=[5,10], offset=[0.1, 0.3]))\n",
    "    assert sorted(summary[par('epochs')].unique()) == [5,10]\n",
    "    assert sorted(summary[par('offset')].unique()) == [0.1, 0.3]\n",
    "    assert summary.shape==(4, 15)\n",
    "    display (summary)\n",
    "    \n",
    "    md ('If we want a value that is the default, we need to indicate None')\n",
    "    summary = query (parameters_fixed=dict(rate=0.0001), exact_match=False, \n",
    "              parameters_variable=dict(epochs=[10, None], offset=[0.1, 0.3]))\n",
    "    assert summary.shape==(4, 15)\n",
    "    assert summary[par('epochs')].isna().sum() == 2\n",
    "    assert (summary[par('epochs')] == 10).sum() == 2\n",
    "    display (summary)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_query, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summary (df, experiments = None, score=None, compact=True):\n",
    "    if experiments is not None:\n",
    "        df = df.loc[experiments]\n",
    "    if compact:\n",
    "        _, df = get_parameters_unique(df)\n",
    "    parameters_columns = get_parameters_columns(df, True)\n",
    "    df_pars = df[parameters_columns]\n",
    "    df_pars.columns = df_pars.columns.get_level_values(level=1)\n",
    "    df_scores = get_experiment_scores (df, score_name=score, remove_score_name=True)\n",
    "    df = pd.concat([df_pars, df_scores], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_summary ():\n",
    "    em = init_em ('summary')\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.1, rate=0.01), nruns=3)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.2, rate=0.001), nruns=5)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.3, rate=0.02), nruns=2)\n",
    "    df = em.get_experiment_data()\n",
    "    result = summary (df, score='validation_accuracy')\n",
    "    display (result)\n",
    "    assert result.columns.tolist() == ['offset', 'rate', 0, 1, 2, 3, 4]\n",
    "    assert result.shape == (3, 7)\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_summary, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python (test_hpsearch)",
   "language": "python",
   "name": "test_hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
