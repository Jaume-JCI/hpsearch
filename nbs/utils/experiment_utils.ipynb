{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp utils.experiment_utils\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])\n",
    "#tst = TestRunner (targets=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Utils\n",
    "\n",
    "> Helper functions for querying and retrieving results from past experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "\n",
    "from hpsearch.config import hp_defaults as dflt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from dsblocks.utils.nbdev_utils import md\n",
    "from hpsearch.examples.dummy_experiment_manager import (DummyExperimentManager, \n",
    "                                                        run_multiple_experiments)\n",
    "from hpsearch.examples.complex_dummy_experiment_manager import generate_data, init_em\n",
    "from hpsearch.config import hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.utils.test_experiment_utils\n",
    "def generate_data_exp_utils (name_folder):\n",
    "    path_experiments = f'test_{name_folder}/debug'\n",
    "    manager_path = f'{path_experiments}/managers'\n",
    "    em = DummyExperimentManager (path_experiments=path_experiments, manager_path=manager_path,\n",
    "                                 verbose=0)\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    run_multiple_experiments(em=em, nruns=5, noise=0.1, verbose=False,\n",
    "                             values_to_explore=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 10, 100]))\n",
    "    run_multiple_experiments(em=em, nruns=5, noise=0.1, verbose=False, rate=0.0001,\n",
    "                             values_to_explore=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 10, 100]))\n",
    "    return em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_data (path_experiments=None, folder_experiments=None, experiments=None):\n",
    "    \"\"\"\n",
    "    Returns data stored from previous experiments in the form DataFrame. \n",
    "    \n",
    "    If path_experiments is not given, it uses the default one. \n",
    "    \"\"\"\n",
    "    from hpsearch.config.hpconfig import get_experiment_data\n",
    "    return get_experiment_data (experiments=experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_experiment_data ():\n",
    "    path_experiments = 'get_experiment_data'\n",
    "    em = generate_data (path_experiments)\n",
    "    \n",
    "    df = get_experiment_data ()\n",
    "    reference = em.get_experiment_data ()\n",
    "    pd.testing.assert_frame_equal (df, reference)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_get_experiment_data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dflt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14986/3002970389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_get_experiment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dummy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jaume/workspace/remote/ds-blocks/dsblocks/utils/nbdev_utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_func, data_func, do, include, debug, exclude, tag, show, store)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'running {name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mtest_func\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14986/45032524.py\u001b[0m in \u001b[0;36mtest_get_experiment_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_get_experiment_data\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpath_experiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'get_experiment_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_experiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_experiment_data\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/examples/complex_dummy_experiment_manager.py\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(name_folder, nruns, noise, verbose_model, verbose, parameters_multiple_values, parameters_single_value, other_parameters, em_args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                                         verbose=verbose, **kwargs)\n\u001b[1;32m    124\u001b[0m     \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_previous_experiments\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     run_multiple_experiments (em=em, nruns=nruns, noise=noise, verbose=verbose,\n\u001b[0m\u001b[1;32m    126\u001b[0m                               \u001b[0mvalues_to_explore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_multiple_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                               \u001b[0mparameters_single_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_single_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/examples/complex_dummy_experiment_manager.py\u001b[0m in \u001b[0;36mrun_multiple_experiments\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_multiple_experiments\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mdummy_em\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_multiple_experiments\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mComplexDummyExperimentManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_previous_experiments\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/examples/dummy_experiment_manager.py\u001b[0m in \u001b[0;36mrun_multiple_experiments\u001b[0;34m(nruns, noise, verbose, rate, values_to_explore, other_parameters, EM, em, em_args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mother_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# parameters that control other aspects that are not part of our experiment definition (a new experiment is not created if we assign different values for these parametsers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mother_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother_parameters_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     em.grid_search (log_message='fixed rate, multiple epochs values',\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mparameters_single_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_single_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mparameters_multiple_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters_multiple_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(self, parameters_multiple_values, parameters_single_value, other_parameters, info, run_numbers, random_search, load_previous, log_message, nruns, keep, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m                 self.create_experiment_and_run (parameters=parameters, other_parameters=other_parameters,\n\u001b[0m\u001b[1;32m    639\u001b[0m                                                 info=info, run_number=run_number, **kwargs)\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m in \u001b[0;36mcreate_experiment_and_run\u001b[0;34m(self, parameters, other_parameters, info, em_args, run_number, log_message, stack_level, precision, experiment_number, repeat_experiment, remove_not_finished, only_remove_not_finished, check_finished, recompute_metrics, force_recompute_metrics, check_finished_if_interrupted, prev_epoch, use_previous_best, from_exp, skip_interrupted, use_last_result, run_if_not_interrumpted, use_last_result_from_dict, previous_model_file_name, model_extension, model_name, epoch_offset, name_best_model, name_last_epoch, min_iterations, use_process)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mpath_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{path_experiments}/experiments_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mpath_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         experiment_number, experiment_data = load_or_create_experiment_values (\n\u001b[0m\u001b[1;32m    322\u001b[0m             path_csv, parameters, precision=precision)\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m in \u001b[0;36mload_or_create_experiment_values\u001b[0;34m(path_csv, parameters, precision, logger)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mexperiment_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m         \u001b[0mexperiment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_defaults_from_experiment_data\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexperiment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0;31m# Finds rows that match parameters. If the dataframe doesn't have any parameter with that name,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/utils/organize_experiments.py\u001b[0m in \u001b[0;36mremove_defaults_from_experiment_data\u001b[0;34m(experiment_data)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mexperiment_data_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mparameters_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters_columns\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexperiment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mparameters_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_data_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparameters_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mchanged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaume/workspace/remote/hpsearch/hpsearch/utils/experiment_utils.py\u001b[0m in \u001b[0;36mget_parameters_columns\u001b[0;34m(experiment_data, only_not_null)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_parameters_columns\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexperiment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_not_null\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdflt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0monly_not_null\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mparameters_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdflt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dflt' is not defined"
     ]
    }
   ],
   "source": [
    "tst.run (test_get_experiment_data, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get experiment parameters and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_parameters_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_columns (experiment_data, only_not_null=False):\n",
    "    parameters = experiment_data[dflt.parameters_col].columns\n",
    "    parameters = [(dflt.parameters_col, *x) for x in parameters]\n",
    "    if only_not_null:\n",
    "        parameters = np.array(parameters)[~experiment_data.loc[:,parameters].isnull().all(axis=0)].tolist()\n",
    "        parameters = [(*x,) for x in parameters]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_experiment_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_parameters (experiment_data, only_not_null=False):\n",
    "    return experiment_data[get_parameters_columns (experiment_data, only_not_null=only_not_null)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_scores_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_columns (experiment_data=None, score_name=None, run_number = None):\n",
    "    \"\"\"\n",
    "    Determine the columnns that provide evaluation scores.\n",
    "    \"\"\"\n",
    "    if score_name is None and experiment_data is None:\n",
    "        raise ValueError ('Either experiment_data or run_number should be different than None')\n",
    "    if score_name is not None and not isinstance(score_name, list):\n",
    "        score_name = [score_name]\n",
    "    if run_number is not None and not isinstance(run_number, list):\n",
    "        if isinstance(run_number, range): run_number=list(run_number)\n",
    "        else: run_number = [run_number]\n",
    "    if score_name is not None:\n",
    "        scores_columns = []\n",
    "        for score in score_name:\n",
    "            new_columns = experiment_data[dflt.scores_col, score].columns\n",
    "            if run_number is not None:\n",
    "                new_columns = list(set(new_columns).intersection (run_number))\n",
    "            new_columns = [(dflt.scores_col, score, c) for c in new_columns]\n",
    "            scores_columns.extend (new_columns)\n",
    "    else:\n",
    "        scores_columns = experiment_data[dflt.scores_col].columns\n",
    "        scores_columns = [(dflt.scores_col, *x) for x in scores_columns]\n",
    "    return scores_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### get_experiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_scores (experiment_data = None, score_name=None, run_number=None, remove_score_name=False):\n",
    "    df = experiment_data[get_scores_columns (experiment_data, score_name=score_name, run_number=run_number)]\n",
    "    if remove_score_name: df.columns = df.columns.get_level_values(2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_parameters_and_scores ():\n",
    "    path_experiments = 'test_get_parameters_and_scores'\n",
    "    em = generate_data (path_experiments)\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_parameters_columns\n",
    "    # ************************************************************\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'offset', 'rate']]\n",
    "    assert get_parameters_columns (df) == expected_result\n",
    "\n",
    "    mi_offset = (dflt.parameters_col, 'offset', '')\n",
    "    offset = df[mi_offset].values.copy()\n",
    "    md ('- We can take only those which have at least some value that is not None.')\n",
    "    df.loc[:, mi_offset] = None\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'rate']]\n",
    "    assert get_parameters_columns (df, only_not_null=True) == expected_result\n",
    "\n",
    "    md ('- If only some elements are None for a given parameter, we still include it.')\n",
    "    df.loc[:, mi_offset] = offset\n",
    "    df.loc[2, mi_offset] = None\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'offset', 'rate']]\n",
    "    assert get_parameters_columns (df, only_not_null=True)==expected_result\n",
    "    df.loc[:, mi_offset] = offset\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_experiment_parameters\n",
    "    # ************************************************************\n",
    "    md ('- Same as get_parameters_columns, but returning dataframe of parameter values.')\n",
    "    result = get_experiment_parameters (df)\n",
    "    assert result.shape == (9, 4)\n",
    "    expected_result = [(dflt.parameters_col, x, '') for x in ['epochs', 'noise', 'offset', 'rate']]\n",
    "    assert result.columns.tolist() == expected_result\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_scores_columns\n",
    "    # ************************************************************\n",
    "    md ('- Retrieve all columns that have scores, for all runs')\n",
    "    expected_result = [(dflt.scores_col, x, y) for x in ['test_accuracy', 'validation_accuracy']\n",
    "                       for y in range(5)]\n",
    "    assert get_scores_columns (df) == expected_result\n",
    "\n",
    "    md ('- Retrieve all columns for given score name, for all runs')\n",
    "    expected_result = [('scores', 'test_accuracy', 0), ('scores', 'test_accuracy', 1), \n",
    "                       ('scores', 'test_accuracy', 2), ('scores', 'test_accuracy', 3), \n",
    "                       ('scores', 'test_accuracy', 4)]\n",
    "    assert get_scores_columns (df, score_name='test_accuracy') == expected_result\n",
    "\n",
    "    md ('- Retrieve all columns for given score name, for given runs')\n",
    "    expected_result = [(dflt.scores_col, x, y) for x in ['test_accuracy'] \n",
    "                       for y in [2, 4]]\n",
    "    assert get_scores_columns (df, score_name='test_accuracy', run_number=[2, 4]) == expected_result\n",
    "\n",
    "    # ************************************************************\n",
    "    # get_experiment_scores\n",
    "    # ************************************************************\n",
    "    md ('- Same, but returning dataframe with selected scores values:')\n",
    "    result = get_experiment_scores (df)\n",
    "    display (result)\n",
    "    assert result.shape==(9,10)\n",
    "\n",
    "    result = get_experiment_scores (df, score_name='test_accuracy')\n",
    "    display (result)\n",
    "    assert result.shape==(9,5)\n",
    "\n",
    "    result = get_experiment_scores (df, score_name='test_accuracy', run_number=[2,4])\n",
    "    display (result)\n",
    "    assert result.shape==(9,2)\n",
    "\n",
    "    md ('- We can remove the metric name and only keep the run number in each column:')\n",
    "    result = get_experiment_scores (df, score_name='test_accuracy', run_number=[2,4], remove_score_name=True)\n",
    "    display (result)\n",
    "    assert result.shape==(9,2)\n",
    "    \n",
    "    # ************************************************************\n",
    "    # get_scores_columns, first usage example: we do not indicate the name of the score\n",
    "    # ************************************************************\n",
    "    expected_result = [(dflt.scores_col, x, y) for x in ['test_accuracy', 'validation_accuracy'] \n",
    "                       for y in range(5)]\n",
    "    assert get_scores_columns (df)==expected_result\n",
    "    \n",
    "    # ************************************************************\n",
    "    # get_scores_columns, second usage: we indicate the name of the score\n",
    "    # ************************************************************\n",
    "    result = get_scores_columns (df, run_number=range(5), score_name='validation_accuracy')\n",
    "    expected_result = [(dflt.scores_col, 'validation_accuracy', y) for y in range(5)]\n",
    "    assert result == expected_result\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_parameters_and_scores, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_scores_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_names (experiment_data=None, run_number=None, experiment=None, only_valid=True):\n",
    "    \"\"\" \n",
    "    Determine the names of the scores included in experiment data. \n",
    "    \n",
    "    If run_number is provided, we provide the scores stored for that run number. If, in addition to this, \n",
    "    experiment is provided, and only_valid=True, we provide only the scores that are not NaN for the given \n",
    "    experiment number.\n",
    "    \"\"\"\n",
    "    \n",
    "    if run_number is None:\n",
    "        scores_names = experiment_data[dflt.scores_col].columns.get_level_values(0).unique()\n",
    "    else:\n",
    "        if not isinstance(run_number, list):\n",
    "            if isinstance(run_number, range): run_number=list(run_number)\n",
    "            else: run_number = [run_number]\n",
    "        scores_names = [(dflt.scores_col, *c) for c in experiment_data[dflt.scores_col].columns \n",
    "                        if c[1] in run_number]\n",
    "        if (experiment is not None) and only_valid:\n",
    "            scores_names = [c for c in scores_names if not np.isnan(experiment_data.loc[experiment, c])]\n",
    "        scores_names = pd.MultiIndex.from_tuples(scores_names).get_level_values(1).unique()\n",
    "    scores_names = list(np.sort(scores_names))\n",
    "    return scores_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_scores_names ():\n",
    "    em = generate_data_exp_utils ('get_scores_names')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    scores_names = get_scores_names (df)\n",
    "    print (scores_names)\n",
    "    assert scores_names == ['test_accuracy', 'validation_accuracy']\n",
    "    \n",
    "    scores_names=get_scores_names (df, run_number=3, experiment=7)\n",
    "    print(scores_names)\n",
    "    assert list(np.sort(scores_names))==['test_accuracy', 'validation_accuracy']\n",
    "\n",
    "    # test when only some scores are valid\n",
    "    df2 = df.copy()\n",
    "    df2.loc[7, (dflt.scores_col, 'test_accuracy', 3)]=np.nan\n",
    "    scores_names=get_scores_names (df2, run_number=3, experiment=7)\n",
    "    print (scores_names)\n",
    "    assert scores_names==['validation_accuracy']\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_scores_names, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_monitored_training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_monitored_training_metrics (experiment, run_number=0, history_file_name='model_history.pk', \n",
    "                                    path_results=None):\n",
    "    if path_results is None:\n",
    "        from hpsearch.config.hpconfig import get_path_results\n",
    "        path_results = get_path_results(experiment, run_number)\n",
    "    path_history = f'{path_results}/{history_file_name}'\n",
    "    if os.path.exists(path_history):\n",
    "        history=pickle.load(open(path_history,'rb'))\n",
    "        return list(history.keys())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_monitored_training_metrics ():\n",
    "    em = generate_data_exp_utils ('get_monitored_training_metrics')\n",
    "    \n",
    "    monitored_metrics = get_monitored_training_metrics (0)\n",
    "    print (monitored_metrics)\n",
    "    assert monitored_metrics==['validation_accuracy', 'test_accuracy', 'accuracy']\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_monitored_training_metrics, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_runs_with_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_runs_with_results (experiment_data = None, score_name=None, run_number=None):\n",
    "    \"\"\"\n",
    "    Gets the list of run_number for whom there are results in experiment_data.\n",
    "    \"\"\"\n",
    "    assert experiment_data is not None, 'experiment_data must be introduced'\n",
    "    result_columns = get_scores_columns (experiment_data, score_name=score_name, run_number=run_number)\n",
    "    completed_results = ~experiment_data.loc[:,result_columns].isnull()\n",
    "    completed_results = completed_results.all(axis=0)\n",
    "    completed_results = completed_results.iloc[np.where(completed_results)]\n",
    "    completed_results = completed_results.index\n",
    "    return completed_results.get_level_values(2).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_runs_with_results ():\n",
    "    em = generate_data ('get_runs_with_results')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    # we need to introduce experiment_data df, and score_name\n",
    "    result = get_runs_with_results (df, score_name='validation_accuracy')\n",
    "    display (result)\n",
    "    assert result==[0,1,2,3,4]\n",
    "    \n",
    "    # we can also restrict to certain run_number\n",
    "    result = get_runs_with_results (df, score_name='validation_accuracy', run_number=[0,2])\n",
    "    display (result)\n",
    "    assert result==[0,2]\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_runs_with_results, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## get_parameters_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_unique(df):\n",
    "    assert df.columns.nlevels == 3\n",
    "    df_all = df\n",
    "    df = df[dflt.parameters_col]\n",
    "    parameters = []\n",
    "    for k in df.columns:\n",
    "        if len(df[k].unique()) > 1:\n",
    "            parameters += [k]\n",
    "    df_parameters = df[parameters]\n",
    "    columns = pd.MultiIndex.from_tuples([(dflt.parameters_col, *c) for c in parameters])\n",
    "    df_parameters.columns = columns\n",
    "    all_cols = df_all.columns.get_level_values(0).unique()\n",
    "    no_par_cols = all_cols [all_cols != dflt.parameters_col]\n",
    "    df_no_par = df_all[no_par_cols]\n",
    "    df_all = pd.concat([df_parameters, df_no_par], axis=1)\n",
    "    return columns, df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_parameters_unique ():\n",
    "    em = generate_data_exp_utils ('get_parameters_unique')\n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    # keeps only those parameters with more than one value,\n",
    "    # removing 'noise' in this case, since it has the same value in all rows\n",
    "    result = get_parameters_unique (df)\n",
    "    assert result[1].shape==(18,28)\n",
    "    assert result[0].tolist() == [(dflt.parameters_col, 'epochs', ''), (dflt.parameters_col, 'offset', ''), \n",
    "                         (dflt.parameters_col, 'rate', '')]\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_parameters_unique, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true",
    "tags": []
   },
   "source": [
    "## compact_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compact_parameters (df, number_characters=1):\n",
    "    par_or = df.columns.get_level_values(1)\n",
    "    par_new = [''.join(y[0].upper()+y[1:number_characters] for y in x.split('_')) for x in par_or]\n",
    "    dict_rename = {k:v for k,v in zip(par_or, par_new)}\n",
    "    if df.columns.nlevels==3:\n",
    "        df.columns = pd.MultiIndex.from_arrays ([df.columns.get_level_values(0), par_new, \n",
    "                                             df.columns.get_level_values(2)])\n",
    "    else:\n",
    "        df.columns = pd.MultiIndex.from_arrays ([df.columns.get_level_values(0), par_new])\n",
    "    \n",
    "    return df, dict_rename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_compact_parameters ():\n",
    "    em = generate_data_exp_utils ('compact_parameters')\n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    result = compact_parameters (df, number_characters=2)\n",
    "    display (result[0].head())\n",
    "    assert result[0].columns.tolist() == [('parameters',   'Ep', ''),\n",
    "            ('parameters',   'No', ''),\n",
    "            ('parameters',   'Of', ''),\n",
    "            ('parameters',   'Ra', ''),\n",
    "            (  'run_info',   'Da',  0),\n",
    "            (  'run_info',   'Da',  1),\n",
    "            (  'run_info',   'Da',  2),\n",
    "            (  'run_info',   'Da',  3),\n",
    "            (  'run_info',   'Da',  4),\n",
    "            (  'run_info',   'Fi',  0),\n",
    "            (  'run_info',   'Fi',  1),\n",
    "            (  'run_info',   'Fi',  2),\n",
    "            (  'run_info',   'Fi',  3),\n",
    "            (  'run_info',   'Fi',  4),\n",
    "            (  'run_info',   'Ti',  0),\n",
    "            (  'run_info',   'Ti',  1),\n",
    "            (  'run_info',   'Ti',  2),\n",
    "            (  'run_info',   'Ti',  3),\n",
    "            (  'run_info',   'Ti',  4),\n",
    "            (    'scores', 'TeAc',  0),\n",
    "            (    'scores', 'TeAc',  1),\n",
    "            (    'scores', 'TeAc',  2),\n",
    "            (    'scores', 'TeAc',  3),\n",
    "            (    'scores', 'TeAc',  4),\n",
    "            (    'scores', 'VaAc',  0),\n",
    "            (    'scores', 'VaAc',  1),\n",
    "            (    'scores', 'VaAc',  2),\n",
    "            (    'scores', 'VaAc',  3),\n",
    "            (    'scores', 'VaAc',  4)]\n",
    "\n",
    "    assert result[1]=={'epochs': 'Ep', 'noise': 'No', 'offset': 'Of', 'rate': 'Ra', 'date': 'Da', \n",
    "                       'finished': 'Fi', 'time': 'Ti', 'test_accuracy': 'TeAc', 'validation_accuracy': 'VaAc'}\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_compact_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## replace_with_default_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_with_default_values (df, parameters={}):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    parameters_names = get_parameters_columns (df)\n",
    "    \n",
    "    for k in df.columns:\n",
    "        experiments_idx=np.argwhere(df[k].isna().ravel()).ravel()\n",
    "        experiments=df.index[experiments_idx]\n",
    "        for experiment in experiments:\n",
    "            parameters = df.loc[experiment, parameters_names].copy()\n",
    "            parameters[parameters.isna().values] = None\n",
    "            parameters = parameters.to_dict()\n",
    "            parameters = {c[1]:parameters[c] for c in parameters if parameters[c] is not None}\n",
    "            defaults = get_default_parameters(parameters)\n",
    "            df.loc[experiment, k] = defaults.get(k[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_replace_with_default_values ():\n",
    "    em = generate_data_exp_utils ('replace_with_default_values')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    df=replace_with_default_values(df)\n",
    "    mi_epoch = (dflt.parameters_col, 'epochs', '')\n",
    "    assert (df[mi_epoch].values == ([5.]*3 + [10.]*3 + [100.]*3)*2).all()\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_replace_with_default_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_defaults (parameters):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    defaults = get_default_parameters(parameters)\n",
    "    for key in defaults.keys():\n",
    "        if key in parameters.keys() and (parameters[key] == defaults[key]):\n",
    "            del parameters[key]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_remove_defaults ():\n",
    "    em = init_em ('remove_defaults')\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.1, 'rate': 0.05})\n",
    "    assert parameters=={'offset':0.1, 'rate': 0.05}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.1, 'rate': 0.01, 'epochs': 10})\n",
    "    assert parameters=={'offset':0.1}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.5, 'rate': 0.000001, 'epochs': 10})\n",
    "    assert parameters=={'rate': 0.000001, 'epochs': 10}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.5, 'rate': 0.000001, 'epochs': 100})\n",
    "    assert parameters=={'rate': 0.000001}\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_remove_defaults, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## find_rows_with_parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_rows_with_parameters_dict (experiment_data, parameters_dict, create_if_not_exists=True, \n",
    "                                    exact_match=True, ignore_keys=[], precision = 1e-10):\n",
    "    \"\"\"\n",
    "    Finds rows that match parameters. \n",
    "    \n",
    "    If the dataframe doesn't have any parameter with that name, a new column \n",
    "    is created and changed_dataframe is set to True.\n",
    "    \"\"\"\n",
    "    changed_dataframe = False\n",
    "    matching_all_condition = pd.Series([True]*experiment_data.shape[0])\n",
    "    existing_keys = [par for par in parameters_dict.keys() if par not in ignore_keys]\n",
    "    for parameter in existing_keys:\n",
    "        mi_parameter = (dflt.parameters_col, parameter, '')\n",
    "        if mi_parameter not in experiment_data.columns:\n",
    "            if create_if_not_exists:\n",
    "                experiment_data[mi_parameter] = None\n",
    "                changed_dataframe = True\n",
    "            else:\n",
    "                raise ValueError ('parameter %s not found in experiment_data' %parameter)\n",
    "        if parameters_dict[parameter] is None:\n",
    "            matching_condition = experiment_data[mi_parameter].isnull()\n",
    "        elif experiment_data[mi_parameter].isnull().all():\n",
    "            matching_condition = ~experiment_data[mi_parameter].isnull()\n",
    "        elif (type(parameters_dict[parameter]) == float) or (type(parameters_dict[parameter]) == np.float32) or (type(parameters_dict[parameter]) == np.float64):\n",
    "            if parameters_dict[parameter] == np.floor(parameters_dict[parameter]):\n",
    "                matching_condition = experiment_data[mi_parameter]==parameters_dict[parameter]\n",
    "            else:\n",
    "                matching_condition = experiment_data[mi_parameter]==parameters_dict[parameter]\n",
    "                for idx, v in enumerate(experiment_data[mi_parameter]):\n",
    "                    if (type(v) == float or type(v) == np.float32 or type(v) == np.float64) and (np.abs(v-parameters_dict[parameter]) < precision):\n",
    "                        matching_condition.iloc[idx]=True\n",
    "                    else:\n",
    "                        matching_condition.iloc[idx]=False\n",
    "        else:\n",
    "            matching_condition = experiment_data[mi_parameter]==parameters_dict[parameter]\n",
    "\n",
    "        matching_all_condition = matching_all_condition & matching_condition.values\n",
    "            \n",
    "    if exact_match:\n",
    "        rest_parameters = experiment_data[dflt.parameters_col].columns.get_level_values(0)\n",
    "        rest_parameters = [par for par in rest_parameters if par not in parameters_dict.keys()]\n",
    "        rest_parameters = [par for par in rest_parameters if par not in ignore_keys]\n",
    "        for parameter in rest_parameters:\n",
    "            mi_parameter = (dflt.parameters_col, parameter, '')\n",
    "            matching_condition = experiment_data[mi_parameter].isnull()\n",
    "            matching_all_condition = matching_all_condition & matching_condition.values\n",
    "    \n",
    "    matching_rows = matching_all_condition.index[matching_all_condition].tolist()\n",
    "    \n",
    "    return matching_rows, changed_dataframe, matching_all_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_find_rows_with_parameters_dict ():\n",
    "    em = generate_data_exp_utils ('find_rows_with_parameters_dict')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001))\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[]\n",
    "    assert not changed_dataframe\n",
    "    \n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows == [9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    \n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001, epochs=5, offset=0.6), exact_match=False,\n",
    "                                        ignore_keys=['epochs'])\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[11, 14, 17]\n",
    "\n",
    "    mi_rate = (dflt.parameters_col, 'rate', '')\n",
    "    df.loc[16, mi_rate]=0.00011\n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[9, 10, 11, 12, 13, 14, 15, 17]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (rate=0.0001), exact_match=False, precision = 0.0001)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert matching_rows==[9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "\n",
    "    result = find_rows_with_parameters_dict (df, dict (new_par=4), exact_match=False)\n",
    "    matching_rows, changed_dataframe, matching_all_condition = result\n",
    "    assert changed_dataframe\n",
    "    assert df.shape == (18, 30)\n",
    "    assert matching_rows==[]\n",
    "    assert (dflt.parameters_col, 'new_par', '') in df.columns\n",
    "    assert matching_rows==[]\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_find_rows_with_parameters_dict\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_find_rows_with_parameters_dict, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarize_results(path_experiments = None, \n",
    "                      folder_experiments = None,\n",
    "                      intersection = False, \n",
    "                      experiments = None, \n",
    "                      score_name=None, \n",
    "                      min_results=0, \n",
    "                      run_number = None, \n",
    "                      parameters = None,\n",
    "                      output='all',\n",
    "                      data = None,\n",
    "                      ascending=False,\n",
    "                      #stats = ['mean','median','rank','min','max','std'],\n",
    "                      stats = ['mean','median','min','max','std']):\n",
    "    \"\"\"\n",
    "    Obtains summary scores for the desired list of experiments. \n",
    "    \n",
    "    Uses the experiment_data csv for that purpose.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if data is None:\n",
    "        experiment_data = get_experiment_data (path_experiments=path_experiments, folder_experiments=folder_experiments)\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        if experiments is not None:\n",
    "            experiment_data = experiment_data.loc[experiments,:]\n",
    "        if parameters is not None:\n",
    "            experiment_rows, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, create_if_not_exists=False, exact_match=False)\n",
    "            experiment_data = experiment_data.loc[experiment_rows]\n",
    "    else:\n",
    "        experiment_data = data.copy()\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        \n",
    "    # Determine the columnns that provide evaluation scores. \n",
    "    result_columns = get_scores_columns (experiment_data, score_name=score_name, run_number=run_number)\n",
    "    \n",
    "    #num_results = (~experiment_data.loc[:,result_columns].isnull()).sum(axis=1, level=1)\n",
    "    num_results = (~experiment_data.loc[:,result_columns].isnull()).groupby(axis=1, level=1).sum()\n",
    "    num_results.columns = pd.MultiIndex.from_product ([[dflt.stats_col], num_results.columns.tolist(), ['num_results']])\n",
    "    experiment_data = pd.concat([experiment_data, num_results], axis=1)\n",
    "    num_results_columns = experiment_data.columns[\n",
    "        experiment_data.columns.get_level_values(2) == 'num_results'\n",
    "    ]\n",
    "    min_num_results = experiment_data[num_results_columns].min(axis=1)\n",
    "    if min_results > 0:\n",
    "        number_before = experiment_data.shape[0]\n",
    "        experiment_data = experiment_data[min_num_results>=min_results]\n",
    "        print (f'{experiment_data.shape[0]} out of {number_before} experiments have {min_results} runs completed')\n",
    "    \n",
    "    # Take only those run_number where all experiments provide some score\n",
    "    if intersection:\n",
    "        number_before = len(result_columns)\n",
    "        all_have_results = ~experiment_data.loc[:,result_columns].isnull().any(axis=0)\n",
    "        result_columns = (np.array(result_columns)[all_have_results]).tolist()\n",
    "        print (f'{len(result_columns)} out of {number_before} runs for whom all the '\n",
    "                'selected experiments have completed')\n",
    "        \n",
    "    print (f'total data examined: {experiment_data.shape[0]} experiments '\n",
    "           f'with at least {min_num_results.min()} runs done for each one')\n",
    "        \n",
    "    scores = experiment_data.loc[:, result_columns]\n",
    "    scores[scores.isna()]=np.nan\n",
    "    scores = -scores.values\n",
    "    # TODO: make it work across different metrics\n",
    "    #rank = np.argsort(scores,axis=0)\n",
    "    #rank = np.argsort(rank,axis=0).astype(np.float32)\n",
    "    #rank[experiment_data.loc[:,result_columns].isnull()]=np.nan\n",
    "    \n",
    "    parameters = get_parameters_columns(experiment_data, True)\n",
    "    parameters.extend (num_results_columns.tolist())\n",
    "    scores_to_return={}\n",
    "    stat_df_all = []\n",
    "    stats_columns=[]\n",
    "    #for stat in ['mean', 'min', 'max', 'std', 'median', 'rank']:\n",
    "    for stat in ['mean', 'min', 'max', 'std', 'median']:\n",
    "        stat_df = experiment_data.loc[:,result_columns].groupby (level=1, axis=1).agg(stat)\n",
    "        #stat_df = experiment_data.loc[:,result_columns].agg(stat, axis=1, level=1)\n",
    "        stat_df.columns = pd.MultiIndex.from_product (\n",
    "            [[dflt.stats_col], stat_df.columns.tolist(), [stat]])\n",
    "        scores_to_return[stat] = stat_df.columns.tolist()\n",
    "        stats_columns.extend (stat_df.columns.tolist())\n",
    "        stat_df_all.append (stat_df)\n",
    "    experiment_data = pd.concat ([experiment_data]+stat_df_all, axis=1)\n",
    "    if score_name is None: \n",
    "        score_name = experiment_data[dflt.scores_col].columns.get_level_values(0).unique()\n",
    "        score_name = score_name[0]\n",
    "    elif isinstance (score_name, list):\n",
    "        score_name = score_name[0]\n",
    "    stat2tuple = lambda stat: (dflt.stats_col, score_name, stat)\n",
    "    if output == 'all':\n",
    "        summary = dict (mean=experiment_data.loc[:,parameters+scores_to_return['mean']].sort_values(by=stat2tuple('mean'),ascending=ascending),\n",
    "                        median=experiment_data.loc[:,parameters+scores_to_return['median']].sort_values(by=stat2tuple('median'),ascending=ascending),\n",
    "                        #rank=experiment_data.loc[:,parameters+scores_to_return['rank']].sort_values(by=(dflt.stats_col, '', 'rank')),\n",
    "                        stats=experiment_data.loc[:,parameters+stats_columns].sort_values(by=stat2tuple('mean'),ascending=ascending),\n",
    "                        unordered=experiment_data.loc[:,parameters],\n",
    "                        allcols=experiment_data,\n",
    "                        original=experiment_data_original\n",
    "                        )\n",
    "        for k in summary:\n",
    "            summary[k] = summary[k][summary[k].columns.sort_values()]\n",
    "    elif output == 'stats':\n",
    "        summary = experiment_data.loc[:,parameters+stats_columns].sort_values(by=stat2tuple('mean'),ascending=ascending)\n",
    "    elif output == 'unordered':\n",
    "        summary = experiment_data.loc[:,parameters]\n",
    "    elif output == 'allcols':\n",
    "        summary = experiment_data\n",
    "    elif output == 'original':\n",
    "        summary = experiment_data_original\n",
    "    else:\n",
    "        summary = experiment_data.loc[:,parameters+[output]].sort_values(by=output, ascending=ascending)\n",
    "    if output != 'all':\n",
    "        summary = summary[summary.columns.sort_values()]\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_summarize_results ():\n",
    "    em = init_em ('summarize_results')\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.1, rate=0.01), nruns=3)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.2, rate=0.001), nruns=5)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.3, rate=0.02), nruns=2)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    d = summarize_results ()\n",
    "    display (d['mean'])\n",
    "    mi_num_results = (dflt.stats_col, 'validation_accuracy', 'num_results')\n",
    "    assert d['mean'][mi_num_results].sum() == 10\n",
    "    assert d['mean'].shape==(3, 6)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- The metric is indicated with `_` at the beginning: ')\n",
    "    d = summarize_results (score_name='validation_accuracy')\n",
    "    display (d['mean'])\n",
    "    assert d['mean'][mi_num_results].sum() == 10\n",
    "    assert d['mean'].shape==(3, 4)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can filter those results that have less than X runs: ')\n",
    "    d = summarize_results (score_name='validation_accuracy', min_results=5)\n",
    "    display (d['mean'])\n",
    "    assert d['mean'][mi_num_results].sum() == 5\n",
    "    assert d['mean'].shape==(1, 4)\n",
    "    \n",
    "    md ('\\n\\n')\n",
    "    md ('- We can filter by experiment number and/or number of results, and retrieve the original dataframe,'\n",
    "        'plus new columns with stats: ')\n",
    "    d = summarize_results (score_name='validation_accuracy', experiments=[0,2], output='allcols')\n",
    "    display (d)\n",
    "    assert d.shape==(2, 33)\n",
    "    assert all(d.index==[0,2])\n",
    "    assert d['stats'].columns.get_level_values(0).unique().tolist()==['validation_accuracy']\n",
    "    assert d['stats','validation_accuracy'].columns.tolist()==['max', 'mean', 'median', 'min', 'num_results', 'std']\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_summarize_results\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.11\n",
      "epoch 1: accuracy: 0.12\n",
      "epoch 2: accuracy: 0.13\n",
      "epoch 3: accuracy: 0.14\n",
      "epoch 4: accuracy: 0.15000000000000002\n",
      "epoch 5: accuracy: 0.16000000000000003\n",
      "epoch 6: accuracy: 0.17000000000000004\n",
      "epoch 7: accuracy: 0.18000000000000005\n",
      "epoch 8: accuracy: 0.19000000000000006\n",
      "epoch 9: accuracy: 0.20000000000000007\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.11\n",
      "epoch 1: accuracy: 0.12\n",
      "epoch 2: accuracy: 0.13\n",
      "epoch 3: accuracy: 0.14\n",
      "epoch 4: accuracy: 0.15000000000000002\n",
      "epoch 5: accuracy: 0.16000000000000003\n",
      "epoch 6: accuracy: 0.17000000000000004\n",
      "epoch 7: accuracy: 0.18000000000000005\n",
      "epoch 8: accuracy: 0.19000000000000006\n",
      "epoch 9: accuracy: 0.20000000000000007\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.11\n",
      "epoch 1: accuracy: 0.12\n",
      "epoch 2: accuracy: 0.13\n",
      "epoch 3: accuracy: 0.14\n",
      "epoch 4: accuracy: 0.15000000000000002\n",
      "epoch 5: accuracy: 0.16000000000000003\n",
      "epoch 6: accuracy: 0.17000000000000004\n",
      "epoch 7: accuracy: 0.18000000000000005\n",
      "epoch 8: accuracy: 0.19000000000000006\n",
      "epoch 9: accuracy: 0.20000000000000007\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.32\n",
      "epoch 1: accuracy: 0.34\n",
      "epoch 2: accuracy: 0.36000000000000004\n",
      "epoch 3: accuracy: 0.38000000000000006\n",
      "epoch 4: accuracy: 0.4000000000000001\n",
      "epoch 5: accuracy: 0.4200000000000001\n",
      "epoch 6: accuracy: 0.4400000000000001\n",
      "epoch 7: accuracy: 0.46000000000000013\n",
      "epoch 8: accuracy: 0.48000000000000015\n",
      "epoch 9: accuracy: 0.5000000000000001\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.32\n",
      "epoch 1: accuracy: 0.34\n",
      "epoch 2: accuracy: 0.36000000000000004\n",
      "epoch 3: accuracy: 0.38000000000000006\n",
      "epoch 4: accuracy: 0.4000000000000001\n",
      "epoch 5: accuracy: 0.4200000000000001\n",
      "epoch 6: accuracy: 0.4400000000000001\n",
      "epoch 7: accuracy: 0.46000000000000013\n",
      "epoch 8: accuracy: 0.48000000000000015\n",
      "epoch 9: accuracy: 0.5000000000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- We need to indicate the metric to be retrieved, otherwise it will count as many results as num_results*num_metrics: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data examined: 3 experiments with at least 6 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset   rate  num_results      mean\n",
       "2     0.3  0.020            6  0.633333\n",
       "1     0.2  0.001           15  0.440000\n",
       "0     0.1    NaN            9  0.433333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- The metric is indicated with `_` at the beginning: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data examined: 3 experiments with at least 2 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset   rate  num_results  mean\n",
       "2     0.3  0.020            2  0.50\n",
       "1     0.2  0.001            5  0.21\n",
       "0     0.1    NaN            3  0.20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- We can filter those results that have less than X runs: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 3 experiments have 5 runs completed\n",
      "total data examined: 1 experiments with at least 5 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset   rate  num_results  mean\n",
       "1     0.2  0.001            5  0.21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- We can filter by experiment number and/or number of results, and retrieve the original dataframe,plus new columns with stats: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data examined: 2 experiments with at least 2 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>1_validation_accuracy</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>time_1</th>\n",
       "      <th>1_finished</th>\n",
       "      <th>...</th>\n",
       "      <th>time_4</th>\n",
       "      <th>4_finished</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>median</th>\n",
       "      <th>rank</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>15:54:43.969089</td>\n",
       "      <td>True</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>15:54:44.225272</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  0_validation_accuracy  0_test_accuracy    time_0             date  \\\n",
       "0     0.1                    0.2              0.1  0.002003  15:54:43.969089   \n",
       "2     0.3                    0.5              0.4  0.002099  15:54:44.225272   \n",
       "\n",
       "  0_finished  1_validation_accuracy  1_test_accuracy    time_1 1_finished  \\\n",
       "0       True                    0.2              0.1  0.001967       True   \n",
       "2       True                    0.5              0.4  0.002191       True   \n",
       "\n",
       "   ...  time_4  4_finished  num_results mean  min  max  std  median      rank  \\\n",
       "0  ...     NaN         NaN            3  0.2  0.2  0.2  0.0     0.2  0.666667   \n",
       "2  ...     NaN         NaN            2  0.5  0.5  0.5  0.0     0.5  0.000000   \n",
       "\n",
       "   good  \n",
       "0     3  \n",
       "2     2  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_summarize_results, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def query (path_experiments = None, \n",
    "              folder_experiments = None,\n",
    "              intersection = False, \n",
    "              experiments = None, \n",
    "              score_name=None, \n",
    "              min_results=0, \n",
    "              run_number=None, \n",
    "              parameters_fixed = {},\n",
    "              parameters_variable = {},\n",
    "              parameters_all = [],\n",
    "              exact_match = True,\n",
    "              output='all',\n",
    "              ascending=False,\n",
    "              stats = ['mean','median','rank','min','max','std'],\n",
    "              query_other_parameters=False):\n",
    "  \n",
    "    if path_experiments is None:\n",
    "        from hpsearch.config.hpconfig import get_path_experiments\n",
    "        path_experiments = get_path_experiments()\n",
    "    \n",
    "    path_pickle = None\n",
    "    if query_other_parameters:\n",
    "        path_csv = '%s/other_parameters.csv' %path_experiments\n",
    "    else:\n",
    "        path_pickle = '%s/experiments_data.pk' %path_experiments\n",
    "        if not os.path.exists(path_pickle):\n",
    "            path_pickle = None\n",
    "            path_csv = '%s/experiments_data.csv' %path_experiments\n",
    "    if path_pickle is not None:\n",
    "        experiment_data = pd.read_pickle(path_pickle)\n",
    "    else:\n",
    "        experiment_data = pd.read_csv(path_csv, index_col=0)\n",
    "    \n",
    "    non_valid_pars = set(\n",
    "        [(dflt.parameters_col, c, '') for c in parameters_fixed.keys()]\n",
    "    ).difference(set(experiment_data.columns))\n",
    "    non_valid_pars = non_valid_pars.union(\n",
    "        set([(dflt.parameters_col, c, '') for c in parameters_variable.keys()]\n",
    "    ).difference(set(experiment_data.columns)))\n",
    "    \n",
    "    if len(non_valid_pars) > 0:\n",
    "        print (f'\\n**The following query parameters are not valid: {list(non_valid_pars)}**')\n",
    "        print (f'\\nValid parameters:\\n{sorted(get_parameters_columns(experiment_data))}\\n')\n",
    "    \n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_variable))\n",
    "    experiment_numbers = []\n",
    "    for (i, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_fixed)\n",
    "        parameters_none = {k:v for k,v in parameters.items() if v is None}\n",
    "        parameters_not_none = {k:v for k,v in parameters.items() if v is not None}\n",
    "\n",
    "        parameters = remove_defaults (parameters_not_none)\n",
    "        parameters.update(parameters_none)\n",
    "    \n",
    "        experiment_numbers_i, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, ignore_keys=parameters_all, exact_match = exact_match)\n",
    "        experiment_numbers += experiment_numbers_i\n",
    "    \n",
    "    experiment_data = experiment_data.iloc[experiment_numbers]\n",
    "    \n",
    "    if experiments is not None:\n",
    "        experiment_data = experiment_data.loc[experiments]\n",
    "        \n",
    "    if query_other_parameters:\n",
    "        return experiment_data\n",
    "  \n",
    "    d=summarize_results(path_experiments=path_experiments, \n",
    "                      folder_experiments=folder_experiments,\n",
    "                      intersection=intersection, \n",
    "                      experiments=experiments, \n",
    "                      score_name=score_name, \n",
    "                      min_results=min_results, \n",
    "                      run_number=run_number, \n",
    "                      parameters=None,\n",
    "                      output='all',\n",
    "                      data=experiment_data,\n",
    "                      ascending=ascending,\n",
    "                      stats=stats)\n",
    "                      \n",
    "    return d['mean'], d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_query ():\n",
    "    em = generate_data_exp_utils ('query')\n",
    "        \n",
    "    dmean, d = query (parameters_fixed=dict (rate=0.0001))\n",
    "    assert dmean.empty\n",
    "    \n",
    "    md ('the dataframe only has mean. Results are sorted by mean score')\n",
    "    dmean, d = query (parameters_fixed=dict (rate=0.0001), exact_match=False)\n",
    "    par = lambda parameter: (dflt.parameters_col, parameter, '')\n",
    "    stat = lambda statv: (dflt.stats_col, 'test_accuracy', statv)\n",
    "    assert (dmean.shape[0]==9 and (dmean[par('rate')]==0.0001).all() and \n",
    "            len(dmean[par('offset')].unique())==3 and \n",
    "            dmean[stat('mean')].iloc[0]>dmean[stat('mean')].iloc[1] \n",
    "            and dmean[stat('mean')].iloc[1] > dmean[stat('mean')].iloc[2])\n",
    "    \n",
    "    display (dmean)\n",
    "    md ('The second output d contains a field \"stats\" which is a dataframe. Results are sorted by mean score')\n",
    "    assert (d['stats']['stats','validation_accuracy'].columns.tolist()==[\n",
    "        'max', 'mean', 'median', 'min', 'num_results', 'std'])\n",
    "    assert d['stats'].shape==(9,16)\n",
    "    display (d['stats'])\n",
    "    \n",
    "    md ('We can request parameter be in specific list of values')   \n",
    "    dmean, d = query (parameters_fixed=dict(rate=0.0001), exact_match=False, \n",
    "                  parameters_variable=dict(epochs=[5,10], offset=[0.1, 0.3]))\n",
    "    assert sorted(dmean[par('epochs')].unique()) == [5,10]\n",
    "    assert sorted(dmean[par('offset')].unique()) == [0.1, 0.3]\n",
    "    assert dmean.shape[0]==4\n",
    "    display (dmean)\n",
    "    \n",
    "    md ('If we want a value that is the default, we need to indicate None')\n",
    "    dmean, d = query (parameters_fixed=dict(rate=0.0001), exact_match=False, \n",
    "              parameters_variable=dict(epochs=[10, None], offset=[0.1, 0.3]))\n",
    "    assert dmean.shape[0]==4\n",
    "    assert dmean[par('epochs')].isna().sum() == 2\n",
    "    assert (dmean[par('epochs')] == 10).sum() == 2\n",
    "    display (dmean)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_query\n",
      "total data examined: 0 experiments with at least nan runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "the dataframe only has mean. Results are sorted by mean score"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data examined: 9 experiments with at least 15 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>noise</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.731242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.693155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.679728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.548471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.505715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.481352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.432128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.386496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.380060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  offset    rate  noise  num_results      mean\n",
       "11     5.0     0.6  0.0001    0.1           15  0.731242\n",
       "14    10.0     0.6  0.0001    0.1           15  0.693155\n",
       "17     NaN     0.6  0.0001    0.1           15  0.679728\n",
       "10     5.0     0.3  0.0001    0.1           15  0.548471\n",
       "13    10.0     0.3  0.0001    0.1           15  0.505715\n",
       "16     NaN     0.3  0.0001    0.1           15  0.481352\n",
       "9      5.0     0.1  0.0001    0.1           15  0.432128\n",
       "12    10.0     0.1  0.0001    0.1           15  0.386496\n",
       "15     NaN     0.1  0.0001    0.1           15  0.380060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The second output d contains a field \"stats\" which is a dataframe. Results are sorted by mean score"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>noise</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>rank</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.731242</td>\n",
       "      <td>0.717476</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.216686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.693155</td>\n",
       "      <td>0.581141</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>0.288745</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.260617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.679728</td>\n",
       "      <td>0.670236</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.263702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.548471</td>\n",
       "      <td>0.426426</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>0.095076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.505715</td>\n",
       "      <td>0.315252</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.126844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.481352</td>\n",
       "      <td>0.298419</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>0.065535</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.432128</td>\n",
       "      <td>0.236860</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.386496</td>\n",
       "      <td>0.119425</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.380060</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  offset    rate  noise  num_results      mean    median      rank  \\\n",
       "11     5.0     0.6  0.0001    0.1           15  0.731242  0.717476  1.066667   \n",
       "14    10.0     0.6  0.0001    0.1           15  0.693155  0.581141  2.533333   \n",
       "17     NaN     0.6  0.0001    0.1           15  0.679728  0.670236  3.666667   \n",
       "10     5.0     0.3  0.0001    0.1           15  0.548471  0.426426  2.733333   \n",
       "13    10.0     0.3  0.0001    0.1           15  0.505715  0.315252  4.200000   \n",
       "16     NaN     0.3  0.0001    0.1           15  0.481352  0.298419  5.466667   \n",
       "9      5.0     0.1  0.0001    0.1           15  0.432128  0.236860  4.000000   \n",
       "12    10.0     0.1  0.0001    0.1           15  0.386496  0.119425  5.533333   \n",
       "15     NaN     0.1  0.0001    0.1           15  0.380060  0.152559  6.800000   \n",
       "\n",
       "         min  max       std  \n",
       "11  0.412303  1.0  0.216686  \n",
       "14  0.288745  1.0  0.260617  \n",
       "17  0.312746  1.0  0.263702  \n",
       "10  0.095076  1.0  0.346302  \n",
       "13  0.126844  1.0  0.367088  \n",
       "16  0.065535  1.0  0.390327  \n",
       "9   0.000000  1.0  0.425926  \n",
       "12  0.000000  1.0  0.451545  \n",
       "15  0.000000  1.0  0.457517  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "We can request parameter be in specific list of values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data examined: 4 experiments with at least 15 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>noise</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.548471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.505715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.432128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.386496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  offset    rate  noise  num_results      mean\n",
       "10     5.0     0.3  0.0001    0.1           15  0.548471\n",
       "13    10.0     0.3  0.0001    0.1           15  0.505715\n",
       "9      5.0     0.1  0.0001    0.1           15  0.432128\n",
       "12    10.0     0.1  0.0001    0.1           15  0.386496"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "If we want a value that is the default, we need to indicate None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data examined: 4 experiments with at least 15 runs done for each one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>noise</th>\n",
       "      <th>num_results</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.505715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.481352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.386496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.380060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  offset    rate  noise  num_results      mean\n",
       "13    10.0     0.3  0.0001    0.1           15  0.505715\n",
       "16     NaN     0.3  0.0001    0.1           15  0.481352\n",
       "12    10.0     0.1  0.0001    0.1           15  0.386496\n",
       "15     NaN     0.1  0.0001    0.1           15  0.380060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_query, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summary (df, experiments = None, score=None, compact=True):\n",
    "    if experiments is not None:\n",
    "        df = df.loc[experiments]\n",
    "    if compact:\n",
    "        _, df = get_parameters_unique(df)\n",
    "    parameters_columns = get_parameters_columns(df, True)\n",
    "    df_pars = df[parameters_columns]\n",
    "    df_pars.columns = df_pars.columns.get_level_values(level=1)\n",
    "    df_scores = get_experiment_scores (df, score_name=score, remove_score_name=True)\n",
    "    df = pd.concat([df_pars, df_scores], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_summary ():\n",
    "    em = init_em ('summary')\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.1, rate=0.01), nruns=3)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.2, rate=0.001), nruns=5)\n",
    "    em.run_multiple_repetitions (parameters=dict(offset=0.3, rate=0.02), nruns=2)\n",
    "    df = em.get_experiment_data()\n",
    "    result = summary (df, score='validation_accuracy')\n",
    "    display (result)\n",
    "    assert result.columns.tolist() == ['offset', 'rate', 0, 1, 2, 3, 4]\n",
    "    assert result.shape == (3, 7)\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_summary\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.11\n",
      "epoch 1: accuracy: 0.12\n",
      "epoch 2: accuracy: 0.13\n",
      "epoch 3: accuracy: 0.14\n",
      "epoch 4: accuracy: 0.15000000000000002\n",
      "epoch 5: accuracy: 0.16000000000000003\n",
      "epoch 6: accuracy: 0.17000000000000004\n",
      "epoch 7: accuracy: 0.18000000000000005\n",
      "epoch 8: accuracy: 0.19000000000000006\n",
      "epoch 9: accuracy: 0.20000000000000007\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.11\n",
      "epoch 1: accuracy: 0.12\n",
      "epoch 2: accuracy: 0.13\n",
      "epoch 3: accuracy: 0.14\n",
      "epoch 4: accuracy: 0.15000000000000002\n",
      "epoch 5: accuracy: 0.16000000000000003\n",
      "epoch 6: accuracy: 0.17000000000000004\n",
      "epoch 7: accuracy: 0.18000000000000005\n",
      "epoch 8: accuracy: 0.19000000000000006\n",
      "epoch 9: accuracy: 0.20000000000000007\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.11\n",
      "epoch 1: accuracy: 0.12\n",
      "epoch 2: accuracy: 0.13\n",
      "epoch 3: accuracy: 0.14\n",
      "epoch 4: accuracy: 0.15000000000000002\n",
      "epoch 5: accuracy: 0.16000000000000003\n",
      "epoch 6: accuracy: 0.17000000000000004\n",
      "epoch 7: accuracy: 0.18000000000000005\n",
      "epoch 8: accuracy: 0.19000000000000006\n",
      "epoch 9: accuracy: 0.20000000000000007\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.201\n",
      "epoch 1: accuracy: 0.202\n",
      "epoch 2: accuracy: 0.203\n",
      "epoch 3: accuracy: 0.20400000000000001\n",
      "epoch 4: accuracy: 0.20500000000000002\n",
      "epoch 5: accuracy: 0.20600000000000002\n",
      "epoch 6: accuracy: 0.20700000000000002\n",
      "epoch 7: accuracy: 0.20800000000000002\n",
      "epoch 8: accuracy: 0.20900000000000002\n",
      "epoch 9: accuracy: 0.21000000000000002\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.32\n",
      "epoch 1: accuracy: 0.34\n",
      "epoch 2: accuracy: 0.36000000000000004\n",
      "epoch 3: accuracy: 0.38000000000000006\n",
      "epoch 4: accuracy: 0.4000000000000001\n",
      "epoch 5: accuracy: 0.4200000000000001\n",
      "epoch 6: accuracy: 0.4400000000000001\n",
      "epoch 7: accuracy: 0.46000000000000013\n",
      "epoch 8: accuracy: 0.48000000000000015\n",
      "epoch 9: accuracy: 0.5000000000000001\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.32\n",
      "epoch 1: accuracy: 0.34\n",
      "epoch 2: accuracy: 0.36000000000000004\n",
      "epoch 3: accuracy: 0.38000000000000006\n",
      "epoch 4: accuracy: 0.4000000000000001\n",
      "epoch 5: accuracy: 0.4200000000000001\n",
      "epoch 6: accuracy: 0.4400000000000001\n",
      "epoch 7: accuracy: 0.46000000000000013\n",
      "epoch 8: accuracy: 0.48000000000000015\n",
      "epoch 9: accuracy: 0.5000000000000001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset   rate     0     1     2     3     4\n",
       "0     0.1    NaN  0.20  0.20  0.20   NaN   NaN\n",
       "1     0.2  0.001  0.21  0.21  0.21  0.21  0.21\n",
       "2     0.3  0.020  0.50  0.50   NaN   NaN   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_summary, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python (test_hpsearch)",
   "language": "python",
   "name": "test_hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
