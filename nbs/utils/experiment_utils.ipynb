{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp utils.experiment_utils\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Utils\n",
    "\n",
    "> Helper functions for querying and retrieving results from past experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from hpsearch.examples.dummy_experiment_manager import (DummyExperimentManager, \n",
    "                                                        run_multiple_experiments)\n",
    "from hpsearch.examples.complex_dummy_experiment_manager import generate_data, init_em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_data (path_experiments=None, folder_experiments=None, experiments=None):\n",
    "    \"\"\"\n",
    "    Returns data stored from previous experiments in the form DataFrame. \n",
    "    \n",
    "    If path_experiments is not given, it uses the default one. \n",
    "    \"\"\"\n",
    "    from hpsearch.config.hpconfig import get_experiment_data\n",
    "    return get_experiment_data (path_experiments=path_experiments, folder_experiments=folder_experiments,\n",
    "                                experiments=experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_experiment_data ():\n",
    "    path_experiments = 'test_get_experiment_data'\n",
    "    em = generate_data (path_experiments)\n",
    "    \n",
    "    df = get_experiment_data ()\n",
    "    reference = em.get_experiment_data ()\n",
    "    pd.testing.assert_frame_equal (df, reference)\n",
    "    \n",
    "    em.remove_previous_experiments ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_get_experiment_data\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_get_experiment_data, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### generate_data_exp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.utils.test_experiment_utils\n",
    "def generate_data_exp_utils (name_folder):\n",
    "    path_experiments = f'test_{name_folder}'\n",
    "    manager_path = f'{path_experiments}/managers'\n",
    "    em = DummyExperimentManager (path_experiments=path_experiments, manager_path=manager_path,\n",
    "                                 verbose=0)\n",
    "    em.remove_previous_experiments ()\n",
    "    run_multiple_experiments(em=em, nruns=5, noise=0.1, verbose=False,\n",
    "                             values_to_explore=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 10, 100]))\n",
    "    run_multiple_experiments(em=em, nruns=5, noise=0.1, verbose=False, rate=0.0001,\n",
    "                             values_to_explore=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 10, 100]))\n",
    "    return em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_defaults (parameters):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    defaults = get_default_parameters(parameters)\n",
    "    for key in defaults.keys():\n",
    "        if key in parameters.keys() and (parameters[key] == defaults[key]):\n",
    "            del parameters[key]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_remove_defaults ():\n",
    "    em = init_em ('remove_defaults')\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.1, 'rate': 0.05})\n",
    "    assert parameters=={'offset':0.1, 'rate': 0.05}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.1, 'rate': 0.01, 'epochs': 10})\n",
    "    assert parameters=={'offset':0.1}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.5, 'rate': 0.000001, 'epochs': 10})\n",
    "    assert parameters=={'rate': 0.000001, 'epochs': 10}\n",
    "    \n",
    "    parameters = remove_defaults ({'offset':0.5, 'rate': 0.000001, 'epochs': 100})\n",
    "    assert parameters=={'rate': 0.000001}\n",
    "    \n",
    "    em.remove_previous_experiments ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_remove_defaults\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_remove_defaults, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def query (path_experiments = None, \n",
    "              folder_experiments = None,\n",
    "              intersection = False, \n",
    "              experiments = None, \n",
    "              suffix_results='', \n",
    "              min_results=0, \n",
    "              classes = None, \n",
    "              parameters_fixed = {},\n",
    "              parameters_variable = {},\n",
    "              parameters_all = [],\n",
    "              exact_match = True,\n",
    "              output='all',\n",
    "              ascending=False,\n",
    "              suffix_test_set = None,\n",
    "              stats = ['mean','median','rank','min','max','std'],\n",
    "              query_other_parameters=False):\n",
    "  \n",
    "    if path_experiments is None:\n",
    "        from hpsearch.config.hpconfig import get_path_experiments\n",
    "        path_experiments = get_path_experiments(path_experiments=path_experiments, folder = folder_experiments)\n",
    "    \n",
    "    path_pickle = None\n",
    "    if query_other_parameters:\n",
    "        path_csv = '%s/other_parameters.csv' %path_experiments\n",
    "    else:\n",
    "        path_pickle = '%s/experiments_data.pk' %path_experiments\n",
    "        if not os.path.exists(path_pickle):\n",
    "            path_pickle = None\n",
    "            path_csv = '%s/experiments_data.csv' %path_experiments\n",
    "    if path_pickle is not None:\n",
    "        experiment_data = pd.read_pickle(path_pickle)\n",
    "    else:\n",
    "        experiment_data = pd.read_csv(path_csv, index_col=0)\n",
    "    \n",
    "    non_valid_pars = set(parameters_fixed.keys()).difference(set(experiment_data.columns))\n",
    "    if len(non_valid_pars) > 0:\n",
    "        print (f'\\n**The following query parameters are not valid: {list(non_valid_pars)}**')\n",
    "        print (f'\\nValid parameters:\\n{sorted(get_parameters_columns(experiment_data))}\\n')\n",
    "    \n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_variable))\n",
    "    experiment_numbers = []\n",
    "    for (i, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_fixed)\n",
    "        parameters_none = {k:v for k,v in parameters.items() if v is None}\n",
    "        parameters_not_none = {k:v for k,v in parameters.items() if v is not None}\n",
    "\n",
    "        parameters = remove_defaults (parameters_not_none)\n",
    "        parameters.update(parameters_none)\n",
    "    \n",
    "        experiment_numbers_i, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, ignore_keys=parameters_all, exact_match = exact_match)\n",
    "        experiment_numbers += experiment_numbers_i\n",
    "    \n",
    "    experiment_data = experiment_data.iloc[experiment_numbers]\n",
    "    \n",
    "    if experiments is not None:\n",
    "        experiment_data = experiment_data.loc[experiments]\n",
    "        \n",
    "    if query_other_parameters:\n",
    "        return experiment_data\n",
    "  \n",
    "    d=summarize_results(path_experiments=path_experiments, \n",
    "                      folder_experiments=folder_experiments,\n",
    "                      intersection=intersection, \n",
    "                      experiments=experiments, \n",
    "                      suffix_results=suffix_results, \n",
    "                      min_results=min_results, \n",
    "                      class_ids=classes, \n",
    "                      parameters=None,\n",
    "                      output='all',\n",
    "                      data=experiment_data,\n",
    "                      ascending=ascending,\n",
    "                      suffix_test_set=suffix_test_set,\n",
    "                      stats=stats)\n",
    "                      \n",
    "    return d['mean'], d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarize_results(path_experiments = None, \n",
    "                      folder_experiments = None,\n",
    "                      intersection = False, \n",
    "                      experiments = None, \n",
    "                      suffix_results='', \n",
    "                      min_results=0, \n",
    "                      class_ids = None, \n",
    "                      parameters = None,\n",
    "                      output='all',\n",
    "                      data = None,\n",
    "                      ascending=False,\n",
    "                      suffix_test_set = None,\n",
    "                      stats = ['mean','median','rank','min','max','std']):\n",
    "    \"\"\"Obtains summary scores for the desired list of experiments. Uses the experiment_data csv for that purpose\n",
    "    \n",
    "    Example use: \n",
    "        - restricting class_ids:\n",
    "            summarize_results(class_ids= [1058,1059],suffix_results='_m3');\n",
    "    \n",
    "        - with a predetermined list of class_ids:\n",
    "            summarize_results(class_ids='qualified',suffix_results='_m3',min_results=96);\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if data is None:\n",
    "        experiment_data = get_experiment_data (path_experiments=path_experiments, folder_experiments=folder_experiments)\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        if experiments is not None:\n",
    "            experiment_data = experiment_data.loc[experiments,:]\n",
    "        if parameters is not None:\n",
    "            experiment_rows, _, _ = find_rows_with_parameters_dict (experiment_data, parameters, create_if_not_exists=False, exact_match=False)\n",
    "            experiment_data = experiment_data.loc[experiment_rows]\n",
    "    else:\n",
    "        experiment_data = data.copy()\n",
    "        experiment_data_original = experiment_data.copy()\n",
    "        \n",
    "    # Determine the columnns that provide evaluation scores. \n",
    "    result_columns = get_scores_columns (experiment_data, suffix_results=suffix_results, class_ids=class_ids)\n",
    "    \n",
    "    experiment_data.loc[:,'num_results'] = np.sum(~experiment_data.loc[:,result_columns].isnull(),axis=1)\n",
    "    if min_results > 0:\n",
    "        number_before = experiment_data.shape[0]\n",
    "        experiment_data = experiment_data[experiment_data.num_results>=min_results]\n",
    "        print (f'{experiment_data.shape[0]} out of {number_before} experiments have {min_results} runs completed')\n",
    "    \n",
    "    # Take only those class_ids where all experiments provide some score\n",
    "    if intersection:\n",
    "        number_before = len(result_columns)\n",
    "        all_have_results = ~experiment_data.loc[:,result_columns].isnull().any(axis=0)\n",
    "        result_columns = (np.array(result_columns)[all_have_results]).tolist()\n",
    "        print (f'{len(result_columns)} out of {number_before} runs for whom all the selected experiments have completed')\n",
    "        \n",
    "    print (f'total data examined: {experiment_data.shape[0]} experiments with at least {experiment_data[\"num_results\"].min()} runs done for each one')\n",
    "        \n",
    "    scores = -experiment_data.loc[:,result_columns].values\n",
    "    rank = np.argsort(scores,axis=0)\n",
    "    rank = np.argsort(rank,axis=0).astype(np.float32)\n",
    "    rank[experiment_data.loc[:,result_columns].isnull()]=np.nan\n",
    "    \n",
    "    parameters = get_parameters_columns(experiment_data, True)\n",
    "    experiment_data.loc[:,'mean'] = experiment_data.loc[:,result_columns].mean(axis=1)\n",
    "    experiment_data.loc[:,'min'] = experiment_data.loc[:,result_columns].min(axis=1)\n",
    "    experiment_data.loc[:,'max'] = experiment_data.loc[:,result_columns].max(axis=1)\n",
    "    experiment_data.loc[:,'std'] = experiment_data.loc[:,result_columns].std(axis=1)\n",
    "    experiment_data.loc[:,'median'] = experiment_data.loc[:,result_columns].median(axis=1)\n",
    "    experiment_data.loc[:,'rank'] = np.nanmean(rank,axis=1)\n",
    "    experiment_data.loc[:,'good'] = (experiment_data.loc[:,result_columns]>=0.1666666).sum(axis=1)\n",
    "    \n",
    "    scores_to_return = dict(mean=['mean'], median=['median'], rank=['rank'], good=['good'])\n",
    "    if suffix_test_set is not None:\n",
    "        def add_score_to_return (suffix_test_set_i):\n",
    "            result_columns_test_set = get_scores_columns (experiment_data, suffix_results=suffix_test_set_i, class_ids=class_ids)\n",
    "            experiment_data.loc[:,'mean%s' %suffix_test_set_i] = experiment_data.loc[:,result_columns_test_set].mean(axis=1)\n",
    "            experiment_data.loc[:,'median%s' %suffix_test_set_i] = experiment_data.loc[:,result_columns_test_set].median(axis=1)\n",
    "            scores_test_set = -experiment_data.loc[:,result_columns_test_set].values\n",
    "            rank_test_set = np.argsort(scores_test_set,axis=0)\n",
    "            rank_test_set = np.argsort(rank_test_set,axis=0).astype(np.float32)\n",
    "            rank_test_set[experiment_data.loc[:,result_columns].isnull()]=np.nan\n",
    "            experiment_data.loc[:,'rank%s' %suffix_test_set_i] = np.nanmean(rank_test_set,axis=1)\n",
    "            experiment_data.loc[:,'good%s' %suffix_test_set_i] = (experiment_data.loc[:,result_columns_test_set]>=0.1666666).sum(axis=1)\n",
    "            for k in scores_to_return.keys():\n",
    "                scores_to_return[k] += ['%s%s' %(k, suffix_test_set_i)]\n",
    "        if type(suffix_test_set) == str:\n",
    "            suffix_test_set = [suffix_test_set]\n",
    "        for suffix_test_set_i in suffix_test_set:\n",
    "            add_score_to_return(suffix_test_set_i)\n",
    "        \n",
    "    if output == 'all':\n",
    "        summary = dict (mean = experiment_data.loc[:,parameters+scores_to_return['mean']].sort_values(by='mean',ascending=ascending),\n",
    "                        median = experiment_data.loc[:,parameters+scores_to_return['median']].sort_values(by='median',ascending=ascending),\n",
    "                        rank = experiment_data.loc[:,parameters+scores_to_return['rank']].sort_values(by='rank'),\n",
    "                        good = experiment_data.loc[:,parameters+scores_to_return['good']].sort_values(by='good',ascending=False),\n",
    "                        stats = experiment_data.loc[:,parameters+stats].sort_values(by='mean',ascending=ascending),\n",
    "                        unordered = experiment_data.loc[:,parameters],\n",
    "                        allcols = experiment_data,\n",
    "                        original = experiment_data_original\n",
    "                        )\n",
    "    elif output == 'stats':\n",
    "        summary = experiment_data.loc[:,parameters+['mean','median','rank']]\n",
    "    elif output == 'unordered':\n",
    "        summary = experiment_data.loc[:,parameters]\n",
    "    elif output == 'allcols':\n",
    "        summary = experiment_data\n",
    "    elif output == 'original':\n",
    "        summary = experiment_data_original\n",
    "    else:\n",
    "        summary = experiment_data.loc[:,parameters+[output]].sort_values(by=output, ascending=output=='rank')\n",
    "        \n",
    "\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summary (df, experiments = None, score=None, compact=True):\n",
    "    if experiments is not None:\n",
    "        df = df.loc[experiments]\n",
    "    if compact:\n",
    "        _, df = get_parameters_unique(df)\n",
    "    parameters_columns = get_parameters_columns(df, True)\n",
    "    scores_columns = ut.get_scores_columns (df, suffix_results=score)\n",
    "    df = df.loc[experiments,parameters_columns + scores_columns]\n",
    "    df = df.rename (columns={'0_%s' %score: score})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_parameters_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_columns (experiment_data, only_not_null=False):\n",
    "    parameters =  [par for par in experiment_data.columns if not par[0].isdigit() and (par.find('time_')<0) and (par.find('date')<0)]\n",
    "    if only_not_null:\n",
    "        parameters = np.array(parameters)[~experiment_data.loc[:,parameters].isnull().all(axis=0)].tolist()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_parameters (experiment_data, only_not_null=False):\n",
    "    return experiment_data[get_parameters_columns (experiment_data, only_not_null=only_not_null)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_scores_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_columns (experiment_data=None, suffix_results='', class_ids = None):\n",
    "    \"\"\"\n",
    "    Determine the columnns that provide evaluation scores. \n",
    "    \n",
    "    We assume that they start with the class number, and that the other columns \n",
    "    do not start with a digit.\n",
    "    \"\"\"\n",
    "    if class_ids is not None:\n",
    "        scores_columns = ['%d%s' %(col,suffix_results) for col in class_ids]\n",
    "    else:\n",
    "        if experiment_data is None:\n",
    "            raise ValueError ('Either experiment_data or class_ids should be different than None')\n",
    "        scores_columns = [col for col in experiment_data.columns if col[0].isdigit()]\n",
    "        # For some experiments, we have multiple scores per class (e.g., due to different evaluation criteria). The argument suffix_results can be used to select the appropriate score.\n",
    "        if len(suffix_results) > 0:\n",
    "            scores_columns = [col for col in scores_columns if (len(col.split(suffix_results))==2) and (len(col.split(suffix_results)[1])==0) and (col.split(suffix_results)[0].isdigit()) ]\n",
    "        else:\n",
    "            # We assume that default scores are in columns whose names only have the class number \n",
    "            scores_columns = [col for col in scores_columns if (len(col.split('_'))>=1)]\n",
    "    return scores_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_scores_columns ():\n",
    "    path_experiments = 'test_get_scores_columns'\n",
    "    em = generate_data (path_experiments)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    # ************************************************************\n",
    "    # First usage example: we do not indicate the name of the score\n",
    "    # ************************************************************\n",
    "    assert get_scores_columns (df)==['0_validation_accuracy', '0_test_accuracy', '0_finished', \n",
    "                                     '1_validation_accuracy', '1_test_accuracy', '1_finished', \n",
    "                                     '2_validation_accuracy', '2_test_accuracy', '2_finished', \n",
    "                                     '3_validation_accuracy', '3_test_accuracy', '3_finished', \n",
    "                                     '4_validation_accuracy', '4_test_accuracy', '4_finished']\n",
    "    \n",
    "    # ************************************************************\n",
    "    # Second usage: we indicate the name of the score\n",
    "    # ************************************************************\n",
    "    result = get_scores_columns (df, class_ids=range(5), suffix_results='_validation_accuracy')\n",
    "    assert result == ['0_validation_accuracy', '1_validation_accuracy', '2_validation_accuracy',\n",
    "                     '3_validation_accuracy', '4_validation_accuracy']\n",
    "    em.remove_previous_experiments ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_get_scores_columns\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_get_scores_columns, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_scores (experiment_data = None, suffix_results = '', class_ids = None, remove_suffix=False):\n",
    "    df = experiment_data[get_scores_columns (experiment_data, suffix_results=suffix_results, class_ids=class_ids)]\n",
    "    if remove_suffix:\n",
    "        df.columns=[c.split('_')[0] for c in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_scores_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_names (experiment_data=None, run_number=None, experiment=None, only_valid=True):\n",
    "    ''' Determine the names of the scores included in experiment data. \n",
    "    \n",
    "        We assume that the score columns start with the class number, and that the other columns do not start with a digit.\n",
    "\n",
    "        If run_number is provided, we provide the scores stored for that run number. If, in addition to this, \n",
    "        experiment is provided, and only_valid=True, we provide only the scores that are not NaN for the given \n",
    "        experiment number.\n",
    "    '''\n",
    "    \n",
    "    if run_number is None:\n",
    "        scores_names = np.unique([('_'.join(col.split('_')[1:]) if (len(col.split('_')) > 1) else '') \n",
    "                                    for col in experiment_data.columns if col[0].isdigit()])\n",
    "        \n",
    "    else:\n",
    "        scores_names = [col.split(f'{run_number}')[1] for col in experiment_data.columns if col.startswith(str(run_number))]\n",
    "        scores_names = [('_'.join(col.split('_')[1:]) if (len(col.split('_')) > 1) else '')\n",
    "                                    for col in scores_names]\n",
    "        if (experiment is not None) and only_valid:\n",
    "            scores_names = [name for name in scores_names if not np.isnan(experiment_data.loc[experiment, f'{run_number}_{name}'])]\n",
    "        scores_names = list(np.sort(scores_names))\n",
    "    # remove special names\n",
    "    scores_names = [name for name in scores_names if name != 'finished']\n",
    "    return scores_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_scores_names ():\n",
    "    em = generate_data_exp_utils ('get_scores_names')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    scores_names = get_scores_names (df)\n",
    "    print (scores_names)\n",
    "    assert scores_names == ['test_accuracy', 'validation_accuracy']\n",
    "    \n",
    "    scores_names=get_scores_names (df, run_number=3, experiment=7)\n",
    "    print(scores_names)\n",
    "    assert list(np.sort(scores_names))==['test_accuracy', 'validation_accuracy']\n",
    "\n",
    "    # test when only some scores are valid\n",
    "    df2 = df.copy()\n",
    "    df2.loc[7, '3_test_accuracy']=np.nan\n",
    "    scores_names=get_scores_names (df2, run_number=3, experiment=7)\n",
    "    print (scores_names)\n",
    "    assert scores_names==['validation_accuracy']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_scores_names, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_monitored_training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_monitored_training_metrics (experiment, run_number=0, history_file_name='model_history.pk', path_results=None, \n",
    "                                    root_path=None, root_folder=None):\n",
    "    if path_results is None:\n",
    "        from hpsearch.config.hpconfig import get_path_results\n",
    "        path_results = get_path_results(experiment, run_number, root_path=root_path, root_folder=root_folder)\n",
    "    path_history = f'{path_results}/{history_file_name}'\n",
    "    if os.path.exists(path_history):\n",
    "        history=pickle.load(open(path_history,'rb'))\n",
    "        return list(history.keys())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_get_monitored_training_metrics ():\n",
    "    em = generate_data_exp_utils ('get_monitored_training_metrics')\n",
    "    \n",
    "    monitored_metrics = get_monitored_training_metrics (0)\n",
    "    print (monitored_metrics)\n",
    "    assert monitored_metrics==['validation_accuracy', 'test_accuracy', 'accuracy']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_monitored_training_metrics, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_rows_with_parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_rows_with_parameters_dict (experiment_data, parameters_dict, create_if_not_exists=True, exact_match=True, ignore_keys=[], precision = 1e-10):\n",
    "    \"\"\" Finds rows that match parameters. If the dataframe doesn't have any parameter with that name, a new column is created and changed_dataframe is set to True.\"\"\"\n",
    "    changed_dataframe = False\n",
    "    matching_all_condition = pd.Series([True]*experiment_data.shape[0])\n",
    "    existing_keys = [par for par in parameters_dict.keys() if par not in ignore_keys]\n",
    "    for parameter in existing_keys:\n",
    "        if parameter not in experiment_data.columns:\n",
    "            if create_if_not_exists:\n",
    "                experiment_data[parameter] = None\n",
    "                changed_dataframe = True\n",
    "            else:\n",
    "                raise ValueError ('parameter %s not found in experiment_data' %parameter)\n",
    "        if parameters_dict[parameter] is None:\n",
    "            matching_condition = experiment_data[parameter].isnull()\n",
    "        elif experiment_data[parameter].isnull().all():\n",
    "            matching_condition = ~experiment_data[parameter].isnull()\n",
    "        elif (type(parameters_dict[parameter]) == float) or (type(parameters_dict[parameter]) == np.float32) or (type(parameters_dict[parameter]) == np.float64):\n",
    "            if parameters_dict[parameter] == np.floor(parameters_dict[parameter]):\n",
    "                matching_condition = experiment_data[parameter]==parameters_dict[parameter]\n",
    "            else:\n",
    "                matching_condition = experiment_data[parameter]==parameters_dict[parameter]\n",
    "                for idx, v in enumerate(experiment_data[parameter]):\n",
    "                    if (type(v) == float or type(v) == np.float32 or type(v) == np.float64) and (np.abs(v-parameters_dict[parameter]) < precision):\n",
    "                        matching_condition.iloc[idx]=True\n",
    "                    else:\n",
    "                        matching_condition.iloc[idx]=False\n",
    "        else:\n",
    "            matching_condition = experiment_data[parameter]==parameters_dict[parameter]\n",
    "\n",
    "        matching_all_condition = matching_all_condition & matching_condition.values\n",
    "            \n",
    "    # We assume that all the columns correspond to parameters, except for those that start with a digit (corresponding to the class evaluated) and those that start with time (giving an estimation of the computational cost)\n",
    "    if exact_match:\n",
    "        rest_parameters = get_parameters_columns (experiment_data)\n",
    "        rest_parameters = [par for par in rest_parameters if par not in parameters_dict.keys()]\n",
    "        rest_parameters = [par for par in rest_parameters if par not in ignore_keys]\n",
    "        for parameter in rest_parameters:\n",
    "            matching_condition = experiment_data[parameter].isnull()\n",
    "            matching_all_condition = matching_all_condition & matching_condition.values\n",
    "    \n",
    "    matching_rows = matching_all_condition.index[matching_all_condition].tolist()\n",
    "    \n",
    "    return matching_rows, changed_dataframe, matching_all_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_classes_with_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_classes_with_results (experiment_data = None, suffix_results = '', class_ids = None):\n",
    "    '''Gets the list of class_ids for whom there are results in experiment_data.\n",
    "    \n",
    "    Example usage with summarize_results:\n",
    "        import hpsearch.utils.vc_experiment_utils as ut\n",
    "        d=ut.summarize_results(class_ids='qualified',suffix_results='_auc_roc', min_results=10);\n",
    "        ch=ut.get_classes_with_results(d['original'].loc[d['mean'].index],suffix_results='_auc_roc');\n",
    "    '''\n",
    "    result_columns = get_scores_columns (experiment_data, suffix_results=suffix_results, class_ids=class_ids)\n",
    "    completed_results = ~experiment_data.loc[:,result_columns].isnull()\n",
    "    completed_results = completed_results.all(axis=0)\n",
    "    completed_results = completed_results.iloc[np.where(completed_results)]\n",
    "    completed_results = completed_results.index\n",
    "\n",
    "    return [int(x[:-len(suffix_results)]) for x in completed_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_parameters_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_unique(df):\n",
    "    parameters = []\n",
    "    for k in df.columns:\n",
    "        if len(df[k].unique()) > 1:\n",
    "            parameters += [k]\n",
    "    return parameters, df[parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compact_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compact_parameters (df, number_characters=1):\n",
    "    par_or = df.columns\n",
    "    par_new = [''.join(y[0].upper()+y[1:number_characters] for y in x.split('_')) for x in par_or]\n",
    "    dict_rename = {k:v for k,v in zip(par_or, par_new)}\n",
    "    df = df.rename (columns = dict_rename)\n",
    "    \n",
    "    return df, dict_rename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace_with_default_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_with_default_values (df, parameters={}):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    parameters_names = get_parameters_columns (df)\n",
    "    \n",
    "    for k in df.columns:\n",
    "        experiments_idx=np.argwhere(df[k].isna().ravel()).ravel()\n",
    "        experiments=df.index[experiments_idx]\n",
    "        for experiment in experiments:\n",
    "            parameters = df.loc[experiment, parameters_names].copy()\n",
    "            parameters[parameters.isna().values] = None\n",
    "            parameters = parameters.to_dict()\n",
    "            parameters = {k:parameters[k] for k in parameters if parameters[k] is not None}\n",
    "            defaults = get_default_parameters(parameters)\n",
    "            df.loc[experiment, k] = defaults.get(k)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_experiment_utils\n",
    "def test_replace_with_default_values ():\n",
    "    em = generate_data_exp_utils ('replace_with_default_values')\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    df=replace_with_default_values(df)\n",
    "    assert (df.epochs.values == ([5.]*3 + [10.]*3 + [100.]*3)*2).all()\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_replace_with_default_values, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpsearch)",
   "language": "python",
   "name": "hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
