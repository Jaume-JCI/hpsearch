{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp utils.convert_legacy\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "#tst = TestRunner (targets=['dummy'])\n",
    "tst = TestRunner (targets=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convert legacy data format\n",
    "\n",
    "> Utilities for converting data to/from legacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from hpsearch.config import hp_defaults as dflt\n",
    "from hpsearch.utils.experiment_utils import read_df, write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "from dsblocks.utils.nbdev_utils import md\n",
    "from dsblocks.utils.utils import remove_previous_results\n",
    "\n",
    "from hpsearch.utils.experiment_utils import read_df, write_df\n",
    "from hpsearch.config import hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert `experiment_data` from and to older version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## legacy data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_parameters_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_parameters_columns (experiment_data, only_not_null=False):\n",
    "    parameters =  [par for par in experiment_data.columns if not par[0].isdigit() and (par.find('time_')<0) and (par.find('date')<0)]\n",
    "    if only_not_null:\n",
    "        parameters = np.array(parameters)[~experiment_data.loc[:,parameters].isnull().all(axis=0)].tolist()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_scores_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_columns (experiment_data=None, suffix_results='', class_ids = None):\n",
    "    \"\"\"\n",
    "    Determine the columnns that provide evaluation scores. \n",
    "    \n",
    "    We assume that they start with the class number, and that the other columns \n",
    "    do not start with a digit.\n",
    "    \"\"\"\n",
    "    if class_ids is not None:\n",
    "        scores_columns = ['%d%s' %(col,suffix_results) for col in class_ids]\n",
    "    else:\n",
    "        if experiment_data is None:\n",
    "            raise ValueError ('Either experiment_data or class_ids should be different than None')\n",
    "        scores_columns = [col for col in experiment_data.columns if col[0].isdigit()]\n",
    "        # For some experiments, we have multiple scores per class (e.g., due to different evaluation criteria). The argument suffix_results can be used to select the appropriate score.\n",
    "        if len(suffix_results) > 0:\n",
    "            scores_columns = [col for col in scores_columns if (len(col.split(suffix_results))==2) and (len(col.split(suffix_results)[1])==0) and (col.split(suffix_results)[0].isdigit()) ]\n",
    "        else:\n",
    "            # We assume that default scores are in columns whose names only have the class number \n",
    "            scores_columns = [col for col in scores_columns if (len(col.split('_'))>=1)]\n",
    "    return scores_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### get_scores_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scores_names (experiment_data=None, run_number=None, experiment=None, only_valid=True, \n",
    "                      sort=False):\n",
    "    \"\"\" \n",
    "    Determine the names of the scores included in experiment data. \n",
    "    \n",
    "    We assume that the score columns start with the class number, and that the other columns do not start with a digit.\n",
    "\n",
    "    If run_number is provided, we provide the scores stored for that run number. If, in addition to this, \n",
    "    experiment is provided, and only_valid=True, we provide only the scores that are not NaN for the given \n",
    "    experiment number.\n",
    "    \"\"\"\n",
    "    \n",
    "    if run_number is None:\n",
    "        scores_names = np.unique([('_'.join(col.split('_')[1:]) if (len(col.split('_')) > 1) else '') \n",
    "                                    for col in experiment_data.columns if col[0].isdigit()])\n",
    "        \n",
    "    else:\n",
    "        scores_names = [col.split(f'{run_number}')[1] for col in experiment_data.columns if col.startswith(str(run_number))]\n",
    "        scores_names = [('_'.join(col.split('_')[1:]) if (len(col.split('_')) > 1) else '')\n",
    "                                    for col in scores_names]\n",
    "        if (experiment is not None) and only_valid:\n",
    "            scores_names = [name for name in scores_names if not np.isnan(experiment_data.loc[experiment, f'{run_number}_{name}'])]\n",
    "        if sort:\n",
    "            scores_names = list(np.sort(scores_names))\n",
    "    # remove special names\n",
    "    scores_names = [name for name in scores_names if name != 'finished']\n",
    "    return scores_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `update_data_format`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_data_format (df):\n",
    "    par_cols_src = get_parameters_columns(df)\n",
    "    par_cols_dst = pd.MultiIndex.from_product ([[dflt.parameters_col], par_cols_src, ['']])\n",
    "    par_df = df[par_cols_src]\n",
    "    par_df.columns = par_cols_dst\n",
    "\n",
    "    score_cols_src = [c for c in get_scores_columns (df) if not c.endswith('finished')]\n",
    "    score_cols_src = np.sort(score_cols_src).tolist()\n",
    "    run_number = np.unique([c.split('_')[0] for c in score_cols_src])\n",
    "    scores_names = get_scores_names (df)\n",
    "    scores_names = np.sort(scores_names).tolist()\n",
    "    score_cols_dst = pd.MultiIndex.from_product ([[dflt.scores_col], scores_names, run_number.tolist()])\n",
    "    scores_dst_sort = np.sort(pd.MultiIndex.from_tuples([(t[0], t[2], t[1]) for t in score_cols_dst]))\n",
    "    score_cols_dst = pd.MultiIndex.from_tuples([(t[0], t[2], t[1]) for t in scores_dst_sort])\n",
    "    score_df = df[score_cols_src]\n",
    "    score_df.columns = score_cols_dst\n",
    "\n",
    "    finished_cols_src = [c for c in get_scores_columns (df) if c.endswith('finished')]\n",
    "    finished_cols_src = np.sort(finished_cols_src).tolist()\n",
    "    finished_cols_dst = pd.MultiIndex.from_product ([[dflt.run_info_col], ['finished'], run_number.tolist()])\n",
    "    finished_df = df[finished_cols_src]\n",
    "    finished_df.columns = finished_cols_dst\n",
    "\n",
    "    time_cols_src = [c for c in df.columns if c.startswith('time')]\n",
    "    time_cols_src = np.sort(time_cols_src).tolist()\n",
    "    time_cols_dst = pd.MultiIndex.from_product ([[dflt.run_info_col], ['time'], run_number.tolist()])\n",
    "    time_df = df[time_cols_src]\n",
    "    time_df.columns = time_cols_dst\n",
    "\n",
    "    date_cols_src = [c for c in df.columns if c.startswith('date')]\n",
    "    date_cols_src = np.sort(date_cols_src).tolist()\n",
    "    date_cols_dst = pd.MultiIndex.from_product ([[dflt.run_info_col], ['date'], run_number.tolist()])\n",
    "    date_df = df[date_cols_src*len(date_cols_dst)]\n",
    "    date_df.columns = date_cols_dst\n",
    "\n",
    "    df = pd.concat ([par_df, score_df, finished_df, time_df, date_df], axis=1)\n",
    "    df = df[df.columns.sort_values()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_convert_legacy\n",
    "def generate_data ():\n",
    "    df = pd.DataFrame ([[0.1, 0.05, 0.6, 0.5, 0.0034384727478027344,\n",
    "            datetime.time(10, 42, 26, 630428), True, 0.61, 0.51,\n",
    "            0.002204418182373047, True, 0.62, 0.52, 0.002073526382446289, True],\n",
    "           [0.2, 0.05, 0.7000000000000001, 0.6000000000000001,\n",
    "            0.0020360946655273438, datetime.time(10, 42, 26, 669600), True,\n",
    "            None, None, None, None, None, None, None, None]])\n",
    "    df.columns = ['offset', 'rate', '0_validation_accuracy', '0_test_accuracy', 'time_0',\n",
    "                                   'date', '0_finished', '1_validation_accuracy', '1_test_accuracy',\n",
    "                                   'time_1', '1_finished', '2_validation_accuracy', '2_test_accuracy',\n",
    "                                   'time_2', '2_finished']\n",
    "    return df\n",
    "\n",
    "def test_update_data_format ():\n",
    "    # get data\n",
    "    df = generate_data ()\n",
    "    display (df)\n",
    "    \n",
    "    # run function\n",
    "    df = update_data_format (df)\n",
    "\n",
    "    # check results\n",
    "    np.testing.assert_array_equal (df[('scores','validation_accuracy')].values, \n",
    "                               np.array([[0.6, 0.61, 0.62], [0.7000000000000001, np.nan, np.nan]]))\n",
    "    np.testing.assert_array_equal (df[('scores','test_accuracy')].values, \n",
    "                               np.array([[0.5, 0.51, 0.52], [0.6000000000000001, np.nan, np.nan]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_update_data_format, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_run_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_run_numbers (df):\n",
    "    columns = df.columns\n",
    "    columns2 = []\n",
    "    for x in columns:\n",
    "        try:\n",
    "            column = (x[0], x[1], int(x[2]))\n",
    "        except ValueError:\n",
    "            column = (x[0], x[1], '')\n",
    "        columns2.append (column)\n",
    "    df.columns = pd.MultiIndex.from_tuples (columns2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_convert_legacy\n",
    "def test_convert_run_numbers ():\n",
    "    # get data\n",
    "    df = generate_data ()\n",
    "    df = update_data_format (df)\n",
    "    \n",
    "    # run function\n",
    "    df = convert_run_numbers (df)\n",
    "\n",
    "    # check results\n",
    "    rn = df[dflt.scores_col].columns.get_level_values(1)\n",
    "    assert type(rn[0]) is int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_convert_run_numbers, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `update_and_replace_experiment_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_and_replace_experiment_data (path_experiments):\n",
    "    df = read_df (path_experiments)\n",
    "    df = update_data_format (df)\n",
    "    df = convert_run_numbers (df)\n",
    "    write_df (df, path_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_convert_legacy\n",
    "def test_update_and_replace_experiment_data ():\n",
    "    path_experiments = 'test_update_and_replace_experiment_data'\n",
    "    os.makedirs (path_experiments, exist_ok=True)\n",
    "    \n",
    "    # get and write data\n",
    "    df = generate_data ()\n",
    "    write_df (df, path_experiments)\n",
    "    display (df)\n",
    "    \n",
    "    # run function\n",
    "    update_and_replace_experiment_data (path_experiments)\n",
    "    print ('\\nfiles written: ', os.listdir (path_experiments))\n",
    "    \n",
    "    # check results\n",
    "    df = read_df (path_experiments)\n",
    "    np.testing.assert_array_equal (df[('scores','validation_accuracy')].values, \n",
    "                               np.array([[0.6, 0.61, 0.62], [0.7000000000000001, np.nan, np.nan]]))\n",
    "    np.testing.assert_array_equal (df[('scores','test_accuracy')].values, \n",
    "                               np.array([[0.5, 0.51, 0.52], [0.6000000000000001, np.nan, np.nan]]))\n",
    "    \n",
    "    remove_previous_results (path_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_update_and_replace_experiment_data, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_run_numbers_and_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert_run_numbers_and_replace (path_experiments):\n",
    "    df = read_df (path_experiments)\n",
    "    df = convert_run_numbers (df)\n",
    "    write_df (df, path_experiments)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python (test_hpsearch)",
   "language": "python",
   "name": "test_hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
