{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp utils.organize_experiments\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize experiments\n",
    "\n",
    "> Routines for organizing the experiments folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from hpsearch.utils import experiment_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "from hpsearch.examples.dummy_experiment_manager import generate_data\n",
    "from hpsearch.config.hpconfig import get_path_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_experiments (path_source, path_destination, key_score=None):\n",
    "    experiment_data_source = pd.read_pickle ('%s/experiments_data.pk' %path_source)\n",
    "    experiment_data_destination = pd.read_pickle ('%s/experiments_data.pk' %path_destination)\n",
    "    experiment_data_source, changed_source = remove_defaults_from_experiment_data (experiment_data_source)\n",
    "    experiment_data_destination, changed_destination = remove_defaults_from_experiment_data (experiment_data_destination)\n",
    "    \n",
    "    for experiment_number_source in range(experiment_data_source.shape[0]):\n",
    "        path_experiment_source = '%s/experiments/%05d' %(path_source, experiment_number_source)\n",
    "        parameters_source, _ = pickle.load(open('%s/parameters.pk' %path_experiment_source,'rb'))\n",
    "        experiment_number_destination, changed_dataframe, _ = experiment_utils.find_rows_with_parameters_dict (experiment_data_destination, parameters_source)\n",
    "        path_experiment_destination = '%s/experiments/%05d' %(path_destination, experiment_number_destination)\n",
    "        if changed_dataframe:\n",
    "            # move folders\n",
    "            os.rename (path_experiment_source, path_experiment_destination)\n",
    "            # copy results to dataframe\n",
    "            missing_cols = [col for col in experiment_data_source.columns if col not in experiment_data_destination.columns]\n",
    "            for column in missing_cols:\n",
    "                experiment_data_destination[column] = None\n",
    "            experiment_data_destination.loc[experiment_number_destination] = experiment_data_source.loc[experiment_number_source]\n",
    "        else:\n",
    "            class_ids_source = [int(x) for x in os.listdir(path_experiment_source) if os.path.isdir('%s/%s' %(path_experiment_source, x))]\n",
    "            class_ids_destination = [int(x) for x in os.listdir(path_experiment_destination) if os.path.isdir('%s/%s' %(path_experiment_destination, x))]\n",
    "            last_id_destination = max(class_ids_destination)\n",
    "            \n",
    "            class_ids_both = [x for x in class_ids_source if x in class_ids_destination]\n",
    "            class_ids_source = [x for x in class_ids_source if x not in class_ids_both]\n",
    "            class_ids_destination = [x for x in class_ids_destination if x not in class_ids_both]\n",
    "            for (idx, class_id_source) in enumerate(class_ids_both):\n",
    "                if key_score is not None:\n",
    "                    scores_name = '%d_%s' %(class_id_source, key_score)\n",
    "                    if experiment_data_source.loc[experiment_number_source, scores_name] != experiment_data_destination.loc[experiment_number_destination, scores_name]:\n",
    "                        is_new = True\n",
    "                else:\n",
    "                    is_new = False\n",
    "                    scores_name_source = [x for x in experiment_data_source.columns if x.startswith('%d_' %class_id_source)]\n",
    "                    scores_name_source = [x for x in scores_name_source if not np.isnan(experiment_data_source.loc[experiment_number_source, x])]\n",
    "                    for scores_name in scores_name_source:\n",
    "                        if experiment_data_source.loc[experiment_number_source, scores_name] != experiment_data_destination.loc[experiment_number_destination, scores_name]:\n",
    "                            is_new = True\n",
    "                            break\n",
    "                if not is_new:\n",
    "                    del class_ids_both[idx]\n",
    "            class_ids_source += class_ids_both\n",
    "            class_ids_destination += class_ids_both\n",
    "                \n",
    "            last_id_source = len(class_ids_source)\n",
    "            new_ids_destination = range(last_id_destination+1, last_id_destination+last_id_source)\n",
    "            for (new_id_destination, class_id_source) in zip(new_ids_destination, class_ids_source):\n",
    "                # move folders\n",
    "                os.rename ('%s/%d' %(path_experiment_source, class_id_source), '%s/%d' %(path_experiment_destination, new_id_destination))\n",
    "                # copy results to dataframe\n",
    "                scores_name_source = [x for x in experiment_data_source.columns if x.startswith('%d_' %class_id_source)]\n",
    "                scores_name_destination = ['%d_%s' (new_id_destination, x[len('%d_' %class_id_source):]) for x in scores_name_source]\n",
    "                for score_name_source, score_name_destination in zip(scores_name_source, scores_name_destination):\n",
    "                    experiment_data_destination.loc[experiment_number_destination, score_name_destination] = experiment_data_source.loc[experiment_number_source, score_name_source]\n",
    "        \n",
    "        experiment_data_destination.to_csv ('%s/experiments_data.csv' %path_destination)\n",
    "        experiment_data_destination.to_pickle ('%s/experiments_data.pk' %path_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_defaults_from_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_defaults_from_experiment_data (experiment_data):\n",
    "    from hpsearch.config.hpconfig import get_default_parameters\n",
    "    \n",
    "    experiment_data_original = experiment_data.copy()\n",
    "    parameters_names = experiment_utils.get_parameters_columns (experiment_data)\n",
    "    parameters_data = experiment_data_original[parameters_names]\n",
    "    changed_df = False\n",
    "    for experiment_number in range(experiment_data.shape[0]):\n",
    "        good_params = ~(experiment_data.loc[experiment_number, parameters_names].isna()).values\n",
    "        parameters_names_i = np.array(parameters_names)[good_params]\n",
    "        parameters_names_i = parameters_names_i.tolist()\n",
    "        parameters = experiment_data.loc[experiment_number, parameters_names_i].to_dict()\n",
    "\n",
    "        defaults = get_default_parameters(parameters)\n",
    "        default_names = [default_name for default_name in defaults.keys() if default_name in parameters_names_i]\n",
    "        \n",
    "        for default_name in default_names:\n",
    "            has_default = experiment_data.loc[experiment_number, default_name] == defaults[default_name]\n",
    "            if has_default:\n",
    "                print ('found experiment with default in experiment_number {}, parameter {}, values: {}'.format(experiment_number, default_name, experiment_data.loc[experiment_number, default_name]))\n",
    "                changed_df = True\n",
    "                experiment_data.loc[experiment_number, default_name] = None\n",
    "    \n",
    "    # check if there are repeated experiments\n",
    "    if changed_df:\n",
    "        if experiment_data[parameters_names].duplicated().any():\n",
    "            print ('duplicated experiments: {}'.format(experiment_data[parameters_names].duplicated()))\n",
    "            experiment_data = experiment_data_original\n",
    "            changed_df = False\n",
    "        \n",
    "    return experiment_data, changed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_experiments (experiments=[], root_path=None, root_folder=None):\n",
    "    from hpsearch.config.hpconfig import get_path_experiment, get_path_experiments\n",
    "    \n",
    "    if type(experiments) is not list:\n",
    "        experiments = [experiments]\n",
    "    if root_path is None:\n",
    "        root_path = get_path_experiments(folder = root_folder)\n",
    "    \n",
    "    # 1. remove experiments from csv file\n",
    "    path_csv = f'{root_path}/experiments_data.csv'\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    experiment_data = pd.read_pickle (path_pickle)\n",
    "    experiment_data = experiment_data.drop (index=experiments)\n",
    "    \n",
    "    # 2. remove experiments folders\n",
    "    for experiment in experiments:\n",
    "        path_experiment = get_path_experiment (experiment, root_path=root_path, root_folder=root_folder)\n",
    "        shutil.rmtree(path_experiment)\n",
    "        \n",
    "    # 3. move experiment folders\n",
    "    for new_number, original_number in enumerate(experiment_data.index):\n",
    "        path_new_experiment = get_path_experiment (new_number, root_path=root_path, root_folder=root_folder)\n",
    "        path_original_experiment = get_path_experiment (original_number, root_path=root_path, root_folder=root_folder)\n",
    "        if path_new_experiment != path_original_experiment:\n",
    "            shutil.move (path_original_experiment, path_new_experiment)\n",
    "            \n",
    "    # 4. move experiment indexes\n",
    "    experiment_data.index = range(len(experiment_data.index))\n",
    "    \n",
    "    # 5. save experiment data\n",
    "    experiment_data.to_csv (path_csv)\n",
    "    experiment_data.to_pickle (path_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.utils.test_organize_experiments\n",
    "def test_remove_experiments ():\n",
    "    em = generate_data ('remove_experiments')\n",
    "    \n",
    "    path_experiments = get_path_experiments ()\n",
    "    print (f'path_experiments: {path_experiments}\\n')\n",
    "    print (f'experiments content: {os.listdir(path_experiments)}\\n')\n",
    "    print (f'experiments inside: {os.listdir(f\"{path_experiments}/experiments\")}\\n')\n",
    "\n",
    "    experiments_data = pd.read_pickle (f'{path_experiments}/experiments_data.pk')\n",
    "    old_experiments_data = experiments_data\n",
    "    print (f'csv data index {experiments_data.index}\\n')\n",
    "    print ('csv content:')\n",
    "\n",
    "    remove_experiments (experiments=[3,7])\n",
    "\n",
    "    # we check that the remaining experiments do not contain number 3 or 7\n",
    "    experiment_folders = os.listdir(f\"{path_experiments}/experiments\")\n",
    "    print (f'experiment folders after removal: {experiment_folders}\\n')\n",
    "    assert len(experiment_folders)==7 and ('00007' not in experiment_folders) and ('00008' not in experiment_folders)\n",
    "\n",
    "    # we check that neither 3 nor 7 are in the new dataframe index\n",
    "    experiments_data = pd.read_pickle (f'{path_experiments}/experiments_data.pk')\n",
    "    print (f'csv data index after removal: {experiments_data.index}\\n')\n",
    "    assert (experiments_data.index==range(7)).all()\n",
    "\n",
    "    assert (experiments_data.loc[3] == old_experiments_data.loc[4]).all() and (experiments_data.loc[6] == old_experiments_data.loc[8]).all()\n",
    "    assert (experiments_data.loc[4] == old_experiments_data.loc[5]).all()\n",
    "\n",
    "    print ('csv content:')\n",
    "    print (experiments_data)\n",
    "    \n",
    "    em.remove_previous_experiments ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_remove_experiments\n",
      "path_experiments: test_remove_experiments\n",
      "\n",
      "experiments content: ['other_parameters.csv', 'git_hash.json', 'experiments_data.csv', 'managers', 'current_experiment_number.pkl', 'experiments', 'parameters.pk', 'parameters.txt', 'experiments_data.pk', 'summary.txt']\n",
      "\n",
      "experiments inside: ['00000', '00002', '00004', '00007', '00006', '00008', '00003', '00005', '00001']\n",
      "\n",
      "csv data index RangeIndex(start=0, stop=9, step=1)\n",
      "\n",
      "csv content:\n",
      "experiment folders after removal: ['00000', '00002', '00004', '00006', '00003', '00005', '00001']\n",
      "\n",
      "csv data index after removal: RangeIndex(start=0, stop=7, step=1)\n",
      "\n",
      "csv content:\n",
      "   epochs  offset  rate  noise  0_validation_accuracy  0_test_accuracy  \\\n",
      "0     5.0     0.1  0.03    0.1               0.203053         0.404256   \n",
      "1     5.0     0.3  0.03    0.1               0.483126         0.647555   \n",
      "2     5.0     0.6  0.03    0.1               0.775755         0.842555   \n",
      "3    15.0     0.3  0.03    0.1               0.812412         0.712835   \n",
      "4    15.0     0.6  0.03    0.1               1.000000         1.000000   \n",
      "5    30.0     0.1  0.03    0.1               0.419808         0.285564   \n",
      "6    30.0     0.6  0.03    0.1               0.829499         0.794423   \n",
      "\n",
      "     time_0             date 0_finished  1_validation_accuracy  ...    time_2  \\\n",
      "0  0.000275  15:10:33.893240       True               0.159198  ...  0.000286   \n",
      "1  0.000283  15:10:34.075454       True               0.486164  ...  0.000263   \n",
      "2  0.000281  15:10:34.265383       True               0.864282  ...  0.000274   \n",
      "3  0.000317  15:10:34.674323       True               0.782272  ...  0.000322   \n",
      "4  0.000321  15:10:34.894592       True               1.000000  ...  0.000318   \n",
      "5  0.000344  15:10:35.132385       True               0.363639  ...  0.000352   \n",
      "6  0.000365  15:10:35.611516       True               0.839049  ...  0.000348   \n",
      "\n",
      "   2_finished 3_validation_accuracy  3_test_accuracy    time_3  3_finished  \\\n",
      "0        True              0.117181         0.369686  0.000366        True   \n",
      "1        True              0.397024         0.601327  0.000270        True   \n",
      "2        True              0.831352         0.726914  0.000291        True   \n",
      "3        True              0.867930         0.656752  0.000308        True   \n",
      "4        True              0.862921         0.914849  0.000320        True   \n",
      "5        True              0.498432         0.278601  0.000363        True   \n",
      "6        True              0.949097         0.873488  0.000364        True   \n",
      "\n",
      "  4_validation_accuracy  4_test_accuracy    time_4  4_finished  \n",
      "0              0.284362         0.173696  0.000301        True  \n",
      "1              0.450511         0.526541  0.000266        True  \n",
      "2              0.802194         0.879698  0.000282        True  \n",
      "3              0.698613         0.544079  0.000304        True  \n",
      "4              1.000000         0.987730  0.000498        True  \n",
      "5              0.520651         0.218306  0.000344        True  \n",
      "6              1.000000         0.859253  0.000362        True  \n",
      "\n",
      "[7 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_remove_experiments, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpsearch)",
   "language": "python",
   "name": "hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
