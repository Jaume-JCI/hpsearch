{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp experiment_manager\n",
    "from nbdev.showdoc import *\n",
    "from block_types.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Manager\n",
    "\n",
    "> Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# coding: utf-8\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.utils import Bunch\n",
    "import platform\n",
    "import pprint\n",
    "import subprocess\n",
    "import json\n",
    "from multiprocessing import Process\n",
    "import logging\n",
    "import traceback\n",
    "import shutil\n",
    "\n",
    "# hpsearch core API\n",
    "from hpsearch.config.manager_factory import ManagerFactory\n",
    "from hpsearch.utils.resume_from_checkpoint import make_resume_from_checkpoint, exists_current_checkpoint, obtain_last_result\n",
    "from hpsearch.utils import experiment_utils\n",
    "from hpsearch.utils.experiment_utils import remove_defaults\n",
    "from hpsearch.utils.organize_experiments import remove_defaults_from_experiment_data\n",
    "import hpsearch.config.hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "from block_types.utils.nbdev_utils import md\n",
    "\n",
    "from hpsearch.examples.complex_dummy_experiment_manager import ComplexDummyExperimentManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperimentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExperimentManager (object):\n",
    "\n",
    "    def __init__ (self, \n",
    "                  allow_base_class=False,\n",
    "                  path_experiments='results/hpsearch',\n",
    "                  defaults={},\n",
    "                  root='',\n",
    "                  metric='accuracy',\n",
    "                  op='max',\n",
    "                  path_alternative=None,\n",
    "                  path_data=None,\n",
    "                  name_model_history='model_history.pk'):\n",
    "        self.path_experiments = path_experiments\n",
    "        self.defaults = defaults\n",
    "        self.default_operations = dict(root=root,\n",
    "                                       metric=metric,\n",
    "                                       op=op)\n",
    "        self.key_score = metric\n",
    "        self.path_alternative = path_alternative\n",
    "        self.path_data = path_data\n",
    "        self.name_model_history = name_model_history\n",
    "        \n",
    "        self.parameters_non_pickable = {}\n",
    "        self.allow_base_class = allow_base_class\n",
    "        self.manager_factory = ManagerFactory(allow_base_class=allow_base_class)\n",
    "        self.manager_factory.register_manager (self)\n",
    "\n",
    "    def get_default_parameters (self, parameters):\n",
    "        if not self.allow_base_class:\n",
    "            raise ImportError ('call get_default_parameters from base class is not allowed')\n",
    "        return self.defaults\n",
    "    \n",
    "    def get_default_operations (self):\n",
    "        return self.default_operations\n",
    "\n",
    "    def get_path_experiments (self, path_experiments = None, folder = None):\n",
    "        \"\"\"Gives the root path to the folder where results of experiments are stored.\"\"\"\n",
    "\n",
    "        path_experiments = (path_experiments if path_experiments is not None \n",
    "                            else self.path_experiments)\n",
    "        if folder != None: path_experiments = f'{path_experiments}/{folder}'\n",
    "\n",
    "        return path_experiments\n",
    "\n",
    "    def get_path_alternative (self, path_results):\n",
    "        if self.path_alternative is None:\n",
    "            path_alternative = path_results\n",
    "\n",
    "        return path_alternative\n",
    "\n",
    "    def get_path_data (self, run_number, root_path=None, parameters={}):\n",
    "        if self.path_data is None:\n",
    "            if root_path is None:\n",
    "                root_path = self.get_path_experiments()\n",
    "            return f'{root_path}/data'\n",
    "        else:\n",
    "            return self.path_data\n",
    "\n",
    "    def get_path_experiment (self, experiment_id, root_path=None, root_folder=None):\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        path_experiment = f'%s/experiments/%05d' %(root_path,experiment_id)\n",
    "        return path_experiment\n",
    "\n",
    "    def get_path_results (self, experiment_id, run_number, root_path=None, root_folder=None):\n",
    "        path_experiment = self.get_path_experiment (experiment_id, root_path=root_path, root_folder=root_folder)\n",
    "        path_results = '%s/%d' %(path_experiment,run_number)\n",
    "        return path_results\n",
    "    \n",
    "    def get_experiment_data (self, path_experiments=None, folder_experiments=None, experiments=None):\n",
    "        path_experiments = self.get_path_experiments(path_experiments=path_experiments, \n",
    "                                                    folder=folder_experiments)\n",
    "        path_csv = '%s/experiments_data.csv' %path_experiments\n",
    "        path_pickle = path_csv.replace('csv', 'pk')\n",
    "        if os.path.exists (path_pickle):\n",
    "            experiment_data = pd.read_pickle (path_pickle)\n",
    "        else:\n",
    "            experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "        if experiments is not None:\n",
    "            experiment_data = experiment_data.loc[experiments,:]\n",
    "            \n",
    "        return experiment_data\n",
    "    \n",
    "    def get_key_score (self, other_parameters):\n",
    "        key_score = other_parameters.get('key_score')\n",
    "        suffix_results = other_parameters.get('suffix_results', '')\n",
    "        if key_score is None and (len(suffix_results) > 0):\n",
    "            if suffix_results[0] == '_':\n",
    "                key_score = suffix_results[1:]\n",
    "            else:\n",
    "                key_score = suffix_results\n",
    "        key_score = self.key_score if key_score is None else key_score\n",
    "        \n",
    "        return key_score\n",
    "    \n",
    "    def remove_previous_experiments (self, path_experiments = None, folder = None):\n",
    "        path_experiments = self.get_path_experiments (path_experiments=path_experiments, \n",
    "                                                      folder=folder)\n",
    "        if os.path.exists (path_experiments):\n",
    "            shutil.rmtree (path_experiments)\n",
    "\n",
    "    def experiment_visualization (self, **kwargs):\n",
    "        raise ValueError ('this type of experiment visualization is not recognized')\n",
    "\n",
    "    def run_experiment_pipeline (self, run_number=0, path_results='./results', parameters = {}):\n",
    "        \"\"\" Runs complete learning pipeline: loading / generating data, building and learning model, applying it to data,\n",
    "        and evaluating it.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # record all parameters except for non-pickable ones\n",
    "        record_parameters (path_results, parameters)\n",
    "\n",
    "        # integrate non-pickable parameters into global dictionary\n",
    "        parameters.update (self.parameters_non_pickable)\n",
    "        self.parameters_non_pickable = {}\n",
    "\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "\n",
    "        # #####################################\n",
    "        # Evaluation\n",
    "        # #####################################\n",
    "        if not parameters.get('just_visualize', False):\n",
    "            time_before = time.time()\n",
    "            score_dict = self._run_experiment (parameters=parameters, path_results=path_results, run_number=run_number)\n",
    "            logger.info ('time spent on this experiment: {}'.format(time.time()-time_before))\n",
    "        else:\n",
    "            score_dict = None\n",
    "\n",
    "        # #####################################\n",
    "        # Visualization\n",
    "        # #####################################\n",
    "        if parameters.get('visualization', False):\n",
    "            raise ValueError ('not implemented')\n",
    "\n",
    "        # #####################################\n",
    "        # Final scores\n",
    "        # #####################################\n",
    "        score_name = parameters.get('suffix_results','')\n",
    "        if len(score_name) > 0:\n",
    "            if score_name[0] == '_':\n",
    "                score_name = score_name[1:]\n",
    "            if score_dict.get(score_name) is not None:\n",
    "                logger.info ('score: %f' %(score_dict.get(score_name)))\n",
    "\n",
    "        spent_time = time.time() - time_before\n",
    "\n",
    "        return score_dict, spent_time\n",
    "\n",
    "    # *************************************************************************\n",
    "    #   run_experiment methods\n",
    "    # *************************************************************************\n",
    "    def _run_experiment (self, parameters={}, path_results='./results', run_number=None):\n",
    "\n",
    "        parameters['run_number'] = run_number\n",
    "\n",
    "        # wrap parameters\n",
    "        parameters = Bunch(**parameters)\n",
    "\n",
    "        if parameters.get('use_process', False):\n",
    "            return self.run_experiment_in_separate_process (parameters, path_results)\n",
    "        else:\n",
    "            return self.run_experiment (parameters=parameters, path_results=path_results)\n",
    "\n",
    "    def run_experiment_in_separate_process (self, parameters={}, path_results='./results'):\n",
    "\n",
    "        parameters['return_results']=False\n",
    "        p = Process(target=self.run_experiment_saving_results, args=(parameters, path_results))\n",
    "        p.start()\n",
    "        p.join()\n",
    "\n",
    "        dict_results = pickle.load (open ('%s/dict_results.pk' %path_results, 'rb'))\n",
    "\n",
    "        return dict_results\n",
    "\n",
    "    def run_experiment_saving_results (self, parameters={}, path_results='./results'):\n",
    "        dict_results = self.run_experiment (parameters=parameters, path_results=path_results)\n",
    "        pickle.dump (dict_results, open ('%s/dict_results.pk' %path_results, 'wb'))\n",
    "\n",
    "    def run_experiment (self, parameters={}, path_results='./results'):\n",
    "        raise NotImplementedError ('This method needs to be defined in subclass')\n",
    "\n",
    "\n",
    "    # *************************************************************************\n",
    "    # *************************************************************************\n",
    "    def create_experiment_and_run (self, parameters = {}, other_parameters = {}, root_path=None, \n",
    "                                   run_number=0, log_message=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # ****************************************************\n",
    "        #  preliminary set-up: logger and root_path\n",
    "        # ****************************************************\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "        if log_message is not None:\n",
    "            logger.info ('**************************************************')\n",
    "            logger.info (log_message)\n",
    "            logger.info ('**************************************************')\n",
    "            other_parameters['log_message'] = log_message\n",
    "\n",
    "        # insert path to experiment script file that called the experiment manager\n",
    "        other_parameters = other_parameters.copy()\n",
    "        insert_experiment_script_path (other_parameters, logger)\n",
    "\n",
    "        # get root_path and create directories\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder = other_parameters.get('root_folder'))\n",
    "        os.makedirs (root_path, exist_ok = True)\n",
    "\n",
    "        # ****************************************************\n",
    "        # register (subclassed) manager so that it can be used by decoupled modules\n",
    "        # ****************************************************\n",
    "        self.register_and_store_subclassed_manager ()\n",
    "\n",
    "        # ****************************************************\n",
    "        #   get experiment number given parameters\n",
    "        # ****************************************************\n",
    "        parameters = remove_defaults (parameters)\n",
    "\n",
    "        path_csv = '%s/experiments_data.csv' %root_path\n",
    "        path_pickle = path_csv.replace('csv', 'pk')\n",
    "        experiment_number, experiment_data = load_or_create_experiment_values (\n",
    "            path_csv, parameters, precision=other_parameters.get('precision', 1e-15)\n",
    "        )\n",
    "\n",
    "        #save_other_parameters (experiment_number, other_parameters, root_path)\n",
    "\n",
    "        # if old experiment, we can require that given parameters match with experiment number\n",
    "        if (other_parameters.get('experiment_number') is not None \n",
    "            and experiment_number != other_parameters.get('experiment_number')):\n",
    "            raise ValueError (f'expected number: {other_parameters.get(\"experiment_number\")}, '\n",
    "                              f'found: {experiment_number}')\n",
    "        other_parameters['experiment_number'] = experiment_number\n",
    "\n",
    "        # ****************************************************\n",
    "        # get key_score and suffix_results\n",
    "        # ****************************************************\n",
    "        key_score = self.get_key_score (other_parameters)\n",
    "        if key_score is not None:\n",
    "            suffix_results = f'_{key_score}'\n",
    "            other_parameters['suffix_results'] = suffix_results\n",
    "\n",
    "        # ****************************************************\n",
    "        #   get run_id, if not given\n",
    "        # ****************************************************\n",
    "        if run_number is None:\n",
    "            run_number = 0\n",
    "            name_score = '%d%s' %(run_number, suffix_results)\n",
    "            while not isnull(experiment_data, experiment_number, name_score):\n",
    "                logger.info ('found previous run for experiment number {}, run {}, with score {} = {}'.format(experiment_number, run_number, key_score, experiment_data.loc[experiment_number, name_score]))\n",
    "                run_number += 1\n",
    "                name_score = '%d%s' %(run_number, suffix_results)\n",
    "            logger.info ('starting experiment {} with run number {}'.format(experiment_number, run_number))\n",
    "\n",
    "        else:\n",
    "            name_score = '%d%s' %(run_number, suffix_results)\n",
    "            if not isnull(experiment_data, experiment_number, name_score):\n",
    "                previous_result = experiment_data.loc[experiment_number, name_score]\n",
    "                logger.info ('found completed: experiment number: %d, run number: %d - score: %f' %(experiment_number, run_number, previous_result))\n",
    "                logger.info (parameters)\n",
    "                if other_parameters.get('repeat_experiment', False):\n",
    "                    logger.info ('redoing experiment')\n",
    "\n",
    "        # ****************************************************\n",
    "        #   remove unfinished experiments\n",
    "        # ****************************************************\n",
    "        if other_parameters.get('remove_not_finished', False):\n",
    "            name_finished = '%d_finished' %run_number\n",
    "            if not isnull(experiment_data, experiment_number, name_finished):\n",
    "                finished = experiment_data.loc[experiment_number, name_finished]\n",
    "                logger.info (f'experiment {experiment_number}, run number {run_number}, finished {finished}')\n",
    "                if not finished:\n",
    "                    experiment_data.loc[experiment_number, name_score] = None\n",
    "                    experiment_data.to_csv (path_csv)\n",
    "                    experiment_data.to_pickle (path_pickle)\n",
    "                    logger.info (f'removed experiment {experiment_number}, '\n",
    "                                 f'run number {run_number}, finished {finished}')\n",
    "            if other_parameters.get('only_remove_not_finished', False):\n",
    "                return None, {}\n",
    "\n",
    "        unfinished_flag = False\n",
    "\n",
    "        # ****************************************************\n",
    "        #   check conditions for skipping experiment\n",
    "        # ****************************************************\n",
    "        if (not isnull(experiment_data, experiment_number, name_score) \n",
    "            and not other_parameters.get('repeat_experiment', False)):\n",
    "            if (other_parameters.get('check_finished', False) \n",
    "                and not self.finished_all_epochs (\n",
    "                    parameters, \n",
    "                    self.get_path_results (experiment_number, run_number=run_number, root_path=root_path), \n",
    "                    other_parameters.get('name_epoch','epochs'))):\n",
    "                unfinished_flag = True\n",
    "            else:\n",
    "                logger.info ('skipping...')\n",
    "                return previous_result, {key_score: previous_result}\n",
    "        elif (isnull(experiment_data, experiment_number, name_score) \n",
    "              and other_parameters.get('recompute_metrics', False) \n",
    "              and not other_parameters.get('force_recompute_metrics', False)):\n",
    "            logger.info (f'experiment not found, skipping {run_number} due to only recompute_metrics')\n",
    "            return None, {}\n",
    "\n",
    "        # ****************************************************\n",
    "        # log info\n",
    "        # ****************************************************\n",
    "        logger.info ('running experiment %d' %experiment_number)\n",
    "        logger.info ('run number: %d' %run_number)\n",
    "        logger.info ('\\nparameters:\\n%s' %mypprint(parameters))\n",
    "\n",
    "        # ****************************************************\n",
    "        #  get paths\n",
    "        # ****************************************************\n",
    "        # path_root_experiment folder\n",
    "        path_root_experiment = '%s/experiments/%05d' %(root_path,experiment_number)\n",
    "        mymakedirs(path_root_experiment, exist_ok=True)\n",
    "\n",
    "        # path_experiment folder (where results are)\n",
    "        path_experiment = '%s/%d' %(path_root_experiment, run_number)\n",
    "        mymakedirs(path_experiment, exist_ok=True)\n",
    "\n",
    "        # path to save big files\n",
    "        path_experiment_big_size = self.get_path_alternative (path_experiment)\n",
    "        os.makedirs (path_experiment_big_size, exist_ok = True)\n",
    "        other_parameters['path_results_big'] = path_experiment_big_size\n",
    "\n",
    "        # ****************************************************\n",
    "        # get git and record parameters\n",
    "        # ****************************************************\n",
    "        # get git revision number\n",
    "        other_parameters['git_hash'] = get_git_revision_hash(root_path)\n",
    "\n",
    "        # write parameters in root experiment folder\n",
    "        record_parameters (path_root_experiment, parameters, other_parameters)\n",
    "\n",
    "        # store hyper_parameters in dictionary that maps experiment_number with hyper_parameter values\n",
    "        store_parameters (root_path, experiment_number, parameters)\n",
    "\n",
    "        # ****************************************************************\n",
    "        # loggers\n",
    "        # ****************************************************************\n",
    "        logger_experiment = set_logger (\"experiment\", path_experiment)\n",
    "        logger_experiment.info ('script: {}, line number: {}'.format(other_parameters['script_path'], other_parameters['lineno']))\n",
    "        if os.path.exists(other_parameters['script_path']):\n",
    "            shutil.copy (other_parameters['script_path'], path_experiment)\n",
    "            shutil.copy (other_parameters['script_path'], path_root_experiment)\n",
    "\n",
    "        # summary logger\n",
    "        logger_summary = set_logger (\"summary\", root_path, mode='w', stdout=False, just_message=True, filename='summary.txt')\n",
    "        logger_summary.info ('\\n\\n{}\\nexperiment: {}, run: {}\\nscript: {}, line number: {}\\nparameters:\\n{}{}'.format('*'*100, experiment_number, run_number, other_parameters['script_path'], other_parameters['lineno'], mypprint(parameters), '*'*100))\n",
    "        if other_parameters.get('rerun_script') is not None:\n",
    "            logger_summary.info ('\\nre-run:\\n{}'.format(other_parameters['rerun_script']))\n",
    "        # same file in path_experiments\n",
    "        logger_summary2 = set_logger (\"summary\", path_experiment, mode='w', stdout=False, just_message=True, filename='summary.txt')\n",
    "        logger_summary2.info ('\\n\\n{}\\nexperiment: {}, run: {}\\nscript: {}, line number: {}\\nparameters:\\n{}{}'.format('*'*100, experiment_number, run_number, other_parameters['script_path'], other_parameters['lineno'], mypprint(parameters), '*'*100))\n",
    "\n",
    "        # ****************************************************************\n",
    "        # Do final adjustments to parameters\n",
    "        # ****************************************************************\n",
    "        parameters = parameters.copy()\n",
    "        original_parameters = parameters.copy()\n",
    "        parameters.update(other_parameters)\n",
    "\n",
    "        # add default parameters - their values are overwritten by input values, if given\n",
    "        parameters_with_defaults = self.get_default_parameters(parameters)\n",
    "        parameters_with_defaults.update(parameters)\n",
    "        parameters = parameters_with_defaults\n",
    "\n",
    "        # ***********************************************************\n",
    "        # resume from previous experiment \n",
    "        # ***********************************************************\n",
    "        resuming_from_prev_epoch_flag = False\n",
    "        if parameters.get('prev_epoch', False):\n",
    "            logger.info('trying prev_epoch')\n",
    "            name_epoch = parameters.get('name_epoch',dflt.name_epoch)\n",
    "            experiment_data2 = experiment_data.copy()\n",
    "            if (((not unfinished_flag) and (other_parameters.get('repeat_experiment', False) or other_parameters.get('just_visualize', False))) \n",
    "                or other_parameters.get('avoid_resuming', False) \n",
    "                or isnull(experiment_data, experiment_number, name_score)):\n",
    "                    experiment_data2 = experiment_data2.drop(experiment_number,axis=0)\n",
    "            prev_experiment_number = self.find_closest_epoch (experiment_data2, original_parameters, \n",
    "                                                              name_epoch=name_epoch)\n",
    "            if prev_experiment_number is not None:\n",
    "                logger.info('using prev_epoch: %d' %prev_experiment_number)\n",
    "                prev_path_results = self.get_path_results (prev_experiment_number, run_number=run_number, root_path=root_path)\n",
    "                found = make_resume_from_checkpoint (parameters, prev_path_results)\n",
    "                if found:\n",
    "                    logger.info ('found previous exp: %d' %prev_experiment_number)\n",
    "                    if prev_experiment_number == experiment_number:\n",
    "                        other_parameters['use_previous_best'] = parameters.get('use_previous_best', True)\n",
    "                        logger.info ('using previous best')\n",
    "                    else:\n",
    "                        name_epoch = parameters.get('name_epoch', dflt.name_epoch)\n",
    "                        parameters[name_epoch] = parameters[name_epoch] - int(experiment_data.loc[prev_experiment_number, name_epoch])\n",
    "\n",
    "                resuming_from_prev_epoch_flag = found\n",
    "                \n",
    "\n",
    "        if not resuming_from_prev_epoch_flag and parameters.get('from_exp', None) is not None:\n",
    "            prev_experiment_number = parameters.get('from_exp', None)\n",
    "            logger.info('using previous experiment %d' %prev_experiment_number)\n",
    "            prev_path_results = self.get_path_results (prev_experiment_number, run_number=run_number, root_path=root_path)\n",
    "            make_resume_from_checkpoint (parameters, prev_path_results, use_best=True)\n",
    "\n",
    "        # ****************************************************************\n",
    "        #   Analyze if experiment was interrupted\n",
    "        # ****************************************************************\n",
    "        if parameters.get('skip_interrupted', False):\n",
    "            was_interrumpted = exists_current_checkpoint (parameters, path_experiment)\n",
    "            was_interrumpted = was_interrumpted or obtain_last_result (parameters, path_experiment) is not None\n",
    "            if was_interrumpted:\n",
    "                logger.info ('found intermediate results, skipping...')\n",
    "                return None, {}\n",
    "\n",
    "        # ****************************************************************\n",
    "        # retrieve last results in interrupted experiments\n",
    "        # ****************************************************************\n",
    "        run_pipeline = True\n",
    "        if parameters.get('use_last_result', False):\n",
    "            experiment_result = obtain_last_result (parameters, path_experiment)\n",
    "            if experiment_result is None and parameters.get('run_if_not_interrumpted', False):\n",
    "                run_pipeline = True\n",
    "            elif experiment_result is None:\n",
    "                return None, {}\n",
    "            else:\n",
    "                run_pipeline = False\n",
    "\n",
    "        # ****************************************************************\n",
    "        # run experiment\n",
    "        # ****************************************************************\n",
    "        if run_pipeline:\n",
    "            experiment_result, time_spent = self.run_experiment_pipeline (run_number,\n",
    "                                        path_experiment,\n",
    "                                        parameters=parameters)\n",
    "            finished = True\n",
    "        else:\n",
    "            finished = False\n",
    "\n",
    "        if other_parameters.get('just_visualize', False):\n",
    "            return None, {}\n",
    "        # ****************************************************************\n",
    "        #  Retrieve and store results\n",
    "        # ****************************************************************\n",
    "        if type(experiment_result)==dict:\n",
    "            dict_results = experiment_result\n",
    "            for key in dict_results.keys():\n",
    "                if key != '':\n",
    "                    experiment_data.loc[experiment_number, '%d_%s' %(run_number, key)]=dict_results[key]\n",
    "                else:\n",
    "                    experiment_data.loc[experiment_number, '%d' %run_number]=dict_results[key]\n",
    "                logger.info('{} - {}: {}'.format(run_number, key, dict_results[key]))\n",
    "        else:\n",
    "            experiment_data.loc[experiment_number, name_score]=experiment_result\n",
    "            logger.info('{} - {}: {}'.format(run_number, name_score, experiment_result))\n",
    "            dict_results = {name_score:experiment_result}\n",
    "\n",
    "        if isnull(experiment_data, experiment_number, 'time_'+str(run_number)) and finished:\n",
    "            experiment_data.loc[experiment_number,'time_'+str(run_number)]=time_spent\n",
    "        experiment_data.loc[experiment_number, 'date']=datetime.datetime.time(datetime.datetime.now())\n",
    "        experiment_data.loc[experiment_number, '%d_finished' %run_number]=finished\n",
    "\n",
    "        experiment_data.to_csv(path_csv)\n",
    "        experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "        save_other_parameters (experiment_number, other_parameters, root_path)\n",
    "\n",
    "        logger_summary2.info ('\\nresults:\\n{}'.format(dict_results))\n",
    "        logger.info ('finished experiment %d' %experiment_number)\n",
    "\n",
    "        # return final score\n",
    "        result = dict_results.get(name_score)\n",
    "        return result, dict_results\n",
    "\n",
    "    def grid_search (self, parameters_multiple_values={}, parameters_single_value={}, other_parameters = {},\n",
    "                     root_path=None, run_numbers=[0], random_search=False,\n",
    "                     load_previous=False, log_message='', nruns = None, keep='multiple'):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "        path_results_base = root_path\n",
    "\n",
    "        mymakedirs(path_results_base,exist_ok=True)\n",
    "\n",
    "        if keep=='multiple':\n",
    "            parameters_single_value = {k:parameters_single_value[k] for k in parameters_single_value.keys() if k not in parameters_multiple_values}\n",
    "        elif keep=='single':\n",
    "            parameters_multiple_values = {k:parameters_multiple_values[k] for k in parameters_multiple_values.keys() if k not in parameters_single_value}\n",
    "        else:\n",
    "            raise ValueError ('parameter keep {} not recognized: it must be either multiple or single'.format(keep))\n",
    "\n",
    "        parameters_multiple_values_all = parameters_multiple_values\n",
    "        parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))\n",
    "\n",
    "        logger = set_logger (\"experiment_manager\", path_results_base)\n",
    "        if log_message != '':\n",
    "            other_parameters['log_message'] = log_message\n",
    "        insert_experiment_script_path (other_parameters, logger)\n",
    "\n",
    "        if random_search:\n",
    "            path_random_hp = '%s/random_hp.pk' %path_results_base\n",
    "            if load_previous and os.path.exists(path_random_hp):\n",
    "                parameters_multiple_values_all = pickle.load(open(path_random_hp,'rb'))\n",
    "            else:\n",
    "                parameters_multiple_values_all = list(np.random.permutation(parameters_multiple_values_all))\n",
    "                pickle.dump (parameters_multiple_values_all, open(path_random_hp,'wb'))\n",
    "        for (i_hp, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "            parameters = parameters_multiple_values.copy()\n",
    "            parameters.update(parameters_single_value)\n",
    "\n",
    "            for (i_run, run_number) in enumerate(run_numbers):\n",
    "                logger.info('processing hyper-parameter %d out of %d' %(i_hp, len(parameters_multiple_values_all)))\n",
    "                logger.info('doing run %d out of %d' %(i_run, len(run_numbers)))\n",
    "                logger.info('%s' %log_message)\n",
    "\n",
    "                self.create_experiment_and_run (parameters=parameters, other_parameters = other_parameters,\n",
    "                                           run_number=run_number, root_path=path_results_base)\n",
    "\n",
    "        # This solves an intermitent issue found in TensorFlow (reported as bug by community)\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    def run_multiple_repetitions (self, parameters={}, other_parameters = {},\n",
    "                     root_path=None, log_message='', nruns = None, run_numbers=[0]):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        logger = set_logger (\"experiment_manager\", root_path)\n",
    "        results = np.zeros((len(run_numbers),))\n",
    "        for (i_run, run_number) in enumerate(run_numbers):\n",
    "                logger.info('doing run %d out of %d' %(i_run, len(run_numbers)))\n",
    "                logger.info('%s' %log_message)\n",
    "\n",
    "                results[i_run], dict_results  = self.create_experiment_and_run (parameters=parameters, other_parameters = other_parameters,\n",
    "                                           run_number=run_number, root_path=root_path)\n",
    "                if dict_results.get('is_pruned', False):\n",
    "                    break\n",
    "\n",
    "        mu, std = results.mean(), results.std()\n",
    "        logger.info ('mean {}: {}, std: {}'.format(other_parameters.get('key_score',''), mu, std))\n",
    "\n",
    "        dict_results[other_parameters.get('key_score','cost')] = mu\n",
    "\n",
    "        return mu, std, dict_results\n",
    "\n",
    "\n",
    "    def hp_optimization (self, parameter_sampler = None, root_path=None, log_message=None, parameters={}, other_parameters={}, nruns=None):\n",
    "\n",
    "        import optuna\n",
    "        from optuna.pruners import SuccessiveHalvingPruner, MedianPruner\n",
    "        from optuna.samplers import RandomSampler, TPESampler\n",
    "        from optuna.integration.skopt import SkoptSampler\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        os.makedirs(root_path, exist_ok=True)\n",
    "        logger = set_logger (\"experiment_manager\", root_path)\n",
    "        if log_message != '':\n",
    "            other_parameters['log_message'] = log_message\n",
    "        insert_experiment_script_path (other_parameters, logger)\n",
    "\n",
    "        # n_warmup_steps: Disable pruner until the trial reaches the given number of step.\n",
    "        sampler_method = other_parameters.get('sampler_method', 'random')\n",
    "        pruner_method = other_parameters.get('pruner_method', 'halving')\n",
    "        n_evaluations = other_parameters.get('n_evaluations', 20)\n",
    "        seed = other_parameters.get('seed', 0)\n",
    "        if sampler_method == 'random':\n",
    "            sampler = RandomSampler(seed=seed)\n",
    "        elif sampler_method == 'tpe':\n",
    "            sampler = TPESampler(n_startup_trials=other_parameters.get('n_startup_trials', 5), seed=seed)\n",
    "        elif sampler_method == 'skopt':\n",
    "            # cf https://scikit-optimize.github.io/#skopt.Optimizer\n",
    "            # GP: gaussian process\n",
    "            # Gradient boosted regression: GBRT\n",
    "            sampler = SkoptSampler(skopt_kwargs={'base_estimator': \"GP\", 'acq_func': 'gp_hedge'})\n",
    "        else:\n",
    "            raise ValueError('Unknown sampler: {}'.format(sampler_method))\n",
    "\n",
    "        if pruner_method == 'halving':\n",
    "            pruner = SuccessiveHalvingPruner(min_resource=1, reduction_factor=4, min_early_stopping_rate=0)\n",
    "        elif pruner_method == 'median':\n",
    "            pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=n_evaluations // 3)\n",
    "        elif pruner_method == 'none':\n",
    "            # Do not prune\n",
    "            pruner = MedianPruner(n_startup_trials=other_parameters.get('n_trials', 10), n_warmup_steps=n_evaluations)\n",
    "        else:\n",
    "            raise ValueError('Unknown pruner: {}'.format(pruner_method))\n",
    "\n",
    "        logger.info (\"Sampler: {} - Pruner: {}\".format(sampler_method, pruner_method))\n",
    "\n",
    "        #study = optuna.create_study(sampler=sampler, pruner=pruner)\n",
    "        study_name = other_parameters.get('study_name', 'hp_study')  # Unique identifier of the study.\n",
    "        study = optuna.create_study(study_name=study_name, storage='sqlite:///%s/%s.db' %(root_path, study_name), sampler=sampler, pruner=pruner, load_if_exists=True)\n",
    "\n",
    "        def objective(trial):\n",
    "\n",
    "            hp_parameters = parameters.copy()\n",
    "            self.parameters_non_pickable = dict(trial=trial)\n",
    "\n",
    "            if parameter_sampler is not None:\n",
    "                hp_parameters.update(parameter_sampler(trial))\n",
    "\n",
    "            if nruns is None:\n",
    "                _, dict_results = self.create_experiment_and_run (parameters=hp_parameters, other_parameters = other_parameters, root_path=root_path, run_number=other_parameters.get('run_number'))\n",
    "            else:\n",
    "                mu_best, std_best, dict_results = self.run_multiple_repetitions (parameters=hp_parameters, other_parameters = other_parameters, root_path=root_path, nruns=nruns)\n",
    "\n",
    "            if dict_results.get('is_pruned', False):\n",
    "                raise optuna.structs.TrialPruned()\n",
    "\n",
    "            return dict_results[other_parameters.get('key_score', 'cost')]\n",
    "\n",
    "        optuna.logging.disable_propagation()\n",
    "        study.optimize(objective, n_trials=other_parameters.get('n_trials', 10), n_jobs=other_parameters.get('n_jobs', 1))\n",
    "\n",
    "        logger.info ('Number of finished trials: {}'.format(len(study.trials)))\n",
    "        logger.info ('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        logger.info ('Value: {}'.format(trial.value))\n",
    "        logger.info ('best params: {}'.format (study.best_params))\n",
    "        best_value = trial.value\n",
    "\n",
    "        nruns_best = other_parameters.get('nruns_best', 0)\n",
    "        if nruns_best > 0:\n",
    "            logger.info ('running best configuration %d times' %nruns_best)\n",
    "            parameters.update (study.best_params)\n",
    "            mu_best, std_best, _ = self.run_multiple_repetitions (parameters=parameters, other_parameters = other_parameters,\n",
    "                                            root_path=root_path, nruns=nruns_best)\n",
    "            best_value = mu_best\n",
    "\n",
    "        return best_value\n",
    "\n",
    "    def rerun_experiment (self, experiments=[], run_numbers=[0], nruns = None, root_path=None, root_folder = None,\n",
    "                          other_parameters={}, parameters = {}, parameter_sampler = None, parameters_multiple_values = None,\n",
    "                          log_message='', only_if_exists=False):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        if root_folder is not None:\n",
    "            other_parameters['root_folder'] = root_folder\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "\n",
    "        logger = set_logger (\"experiment_manager\", root_path)\n",
    "\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        parameters_original = parameters\n",
    "        other_parameters_original = other_parameters\n",
    "        for experiment_id in experiments:\n",
    "            check_experiment_matches = (parameters_multiple_values is None) and (parameter_sampler is None)\n",
    "            parameters, other_parameters = load_parameters (experiment=experiment_id, root_path=root_path, root_folder = root_folder,\n",
    "                                                            other_parameters=other_parameters_original, parameters = parameters_original,\n",
    "                                                            check_experiment_matches=check_experiment_matches)\n",
    "\n",
    "            # we need to set the following flag to False, since otherwise when we request to store the intermediate results\n",
    "            # and the experiment did not start, we do not run the experiment\n",
    "            if other_parameters.get('use_last_result', False) and not other_parameters_original.get('use_last_result', False):\n",
    "                logger.debug ('changing other_parameters[\"use_last_result\"] to False')\n",
    "                other_parameters['use_last_result'] = False\n",
    "            logger.info (f'running experiment {experiment_id} with parameters:\\n{parameters}\\nother_parameters:\\n{other_parameters}')\n",
    "\n",
    "            if parameter_sampler is not None:\n",
    "                logger.info ('running hp_optimization')\n",
    "                insert_experiment_script_path (other_parameters, logger)\n",
    "                self.hp_optimization (parameter_sampler = parameter_sampler, root_path=root_path, log_message=log_message,\n",
    "                                        parameters=parameters, other_parameters=other_parameters)\n",
    "            elif parameters_multiple_values is not None:\n",
    "                self.grid_search (parameters_multiple_values=parameters_multiple_values, parameters_single_value=parameters,\n",
    "                                    other_parameters = other_parameters, root_path=root_path, run_numbers=run_numbers, log_message=log_message)\n",
    "            else:\n",
    "                if only_if_exists:\n",
    "                    run_numbers = [run_number for run_number in run_numbers if os.path.exists('%s/%d' %(path_root_experiment, run_number))]\n",
    "\n",
    "                script_parameters = {}\n",
    "                insert_experiment_script_path (script_parameters, logger)\n",
    "                other_parameters['rerun_script'] = script_parameters\n",
    "                self.run_multiple_repetitions (parameters=parameters, other_parameters = other_parameters, root_path=root_path,\n",
    "                                                log_message=log_message, run_numbers=run_numbers)\n",
    "\n",
    "    def rerun_experiment_pipeline (self, experiments, run_numbers=None, root_path=None, root_folder=None, new_parameters={}, save_results=False):\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        for experiment_id in experiments:\n",
    "            path_root_experiment = self.get_path_experiment (experiment_id, root_path=root_path)\n",
    "\n",
    "            parameters, other_parameters=pickle.load(open('%s/parameters.pk' %path_root_experiment,'rb'))\n",
    "            parameters = parameters.copy()\n",
    "            parameters.update(other_parameters)\n",
    "            parameters.update(new_parameters)\n",
    "            for run_number in run_numbers:\n",
    "                path_experiment = '%s/%d/' %(path_root_experiment, run_number)\n",
    "                path_data = self.get_path_data (run_number, root_path, parameters)\n",
    "                score, _ = self.run_experiment_pipeline (run_number, path_experiment, parameters = parameters)\n",
    "\n",
    "                if save_results:\n",
    "                    experiment_number = experiment_id\n",
    "                    path_csv = '%s/experiments_data.csv' %root_path\n",
    "                    path_pickle = path_csv.replace('csv', 'pk')\n",
    "                    if os.path.exists(path_pickle):\n",
    "                        experiment_data = pd.read_pickle (path_pickle)\n",
    "                    else:\n",
    "                        experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "                    if type(score)==dict:\n",
    "                        for key in score.keys():\n",
    "                            if key != '':\n",
    "                                experiment_data.loc[experiment_number, '%d_%s' %(run_number, key)]=score[key]\n",
    "                            else:\n",
    "                                experiment_data.loc[experiment_number, '%d' %run_number]=score[key]\n",
    "                    else:\n",
    "                        experiment_data.loc[experiment_number, name_score]=score\n",
    "                    experiment_data.to_csv(path_csv)\n",
    "                    experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "    def rerun_experiment_par (self, experiments, run_numbers=None, root_path=None, root_folder=None, parameters={}):\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        for experiment_id in experiments:\n",
    "            path_root_experiment = self.get_path_experiment (experiment_id, root_path=root_path)\n",
    "\n",
    "            for run_number in run_numbers:\n",
    "                path_experiment = '%s/%d/' %(path_root_experiment, run_number)\n",
    "                self.run_experiment_pipeline (run_number, path_experiment, parameters = parameters)\n",
    "\n",
    "    def record_intermediate_results (self, experiments=range(100), run_numbers=range(100), root_path=None, root_folder=None, new_parameters={}, remove=False):\n",
    "\n",
    "        if remove:\n",
    "            new_parameters.update (remove_not_finished=True, only_remove_not_finished=True)\n",
    "        else:\n",
    "            new_parameters.update (use_last_result=True)\n",
    "\n",
    "        self.rerun_experiment_and_save(experiments=experiments, run_numbers=run_numbers,\n",
    "            root_path=root_path, root_folder=root_folder,\n",
    "            new_parameters=new_parameters)\n",
    "        \n",
    "    def find_closest_epoch (self, experiment_data, parameters, name_epoch=dflt.name_epoch):\n",
    "        '''Finds experiment with same parameters except for number of epochs, and takes the epochs that are closer but lower than the one in parameters.'''\n",
    "\n",
    "        experiment_numbers, _, _ = experiment_utils.find_rows_with_parameters_dict (experiment_data, parameters, ignore_keys=[name_epoch,'prev_epoch'])\n",
    "\n",
    "        defaults = self.get_default_parameters(parameters)\n",
    "        current_epoch = parameters.get(name_epoch, defaults.get(name_epoch))\n",
    "        if current_epoch is None:\n",
    "            current_epoch = -1\n",
    "        if len(experiment_numbers) > 1:\n",
    "            epochs = experiment_data.loc[experiment_numbers,name_epoch]\n",
    "            epochs[epochs.isnull()]=defaults.get(name_epoch)\n",
    "            epochs = epochs.loc[epochs<=current_epoch]\n",
    "            if epochs.shape[0] == 0:\n",
    "                return None\n",
    "            else:\n",
    "                return epochs.astype(int).idxmax()\n",
    "        elif len(experiment_numbers) == 1:\n",
    "            return experiment_numbers[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def finished_all_epochs (self, parameters, path_results, name_epoch=dflt.name_epoch):\n",
    "        finished = True\n",
    "        defaults = self.get_default_parameters (parameters)\n",
    "        current_epoch = parameters.get(name_epoch, defaults.get(name_epoch))\n",
    "\n",
    "        name_model_history = parameters.get('name_model_history', self.name_model_history)\n",
    "        path_model_history = f'{path_results}/{name_model_history}'\n",
    "\n",
    "        if os.path.exists(path_model_history):\n",
    "            summary = pickle.load(open(path_model_history, 'rb'))\n",
    "            prev_epoch = summary.get('last_epoch')\n",
    "            if prev_epoch is None:\n",
    "                key_score = self.get_key_score (parameters)\n",
    "                if key_score in summary and (isinstance(summary[key_score], list) \n",
    "                                             or isinstance(summary[key_score], np.array)):\n",
    "                    prev_epoch = (~np.isnan(summary[key_score])).sum()\n",
    "                else:\n",
    "                    prev_epoch = 0\n",
    "            if prev_epoch >= current_epoch:\n",
    "                finished = True\n",
    "            else:\n",
    "                finished = False\n",
    "        else:\n",
    "            finished = False\n",
    "\n",
    "        return finished\n",
    "    \n",
    "    def register_and_store_subclassed_manager (self):\n",
    "        #self.logger.debug ('registering')\n",
    "        self.manager_factory.register_manager (self)\n",
    "        self.manager_factory.write_manager (self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### create_experiment_and_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the experiment manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def init_em (name_folder):\n",
    "    em = ComplexDummyExperimentManager (path_experiments=f'test_{name_folder}')\n",
    "\n",
    "    em.remove_previous_experiments()\n",
    "\n",
    "    with pytest.raises (FileNotFoundError):\n",
    "        os.listdir(em.get_path_experiments())\n",
    "    \n",
    "    return em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_experiment_and_run` is the main function of the `ExperimentManager`. All other functions make use of it adding additional functionalities.\n",
    "\n",
    "In order to call `create_experiment_and_run`, we pass a dictionary of parameters characterizing the experiment we want to run, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_basic_usage ():\n",
    "    em = init_em ('basic')\n",
    "    \n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    # The output is a tuple of two objects:\n",
    "    #1. The main result metric. In our case, we didn't indicate the name of this metric, \n",
    "    #and therefore we get None.\n",
    "    #1. A dictionary containing all the performance metrics for this experiment.\n",
    "    \n",
    "    assert result is None\n",
    "    assert dict_results == {'validation_accuracy': 1.0, 'test_accuracy': 1.0}\n",
    "\n",
    "    # Eight files  are stored in *path_experiments*, and the `experiments` folder is created:\n",
    "    \n",
    "    files_stored = ['current_experiment_number.pkl',\n",
    "             'experiments',\n",
    "             'experiments_data.csv',\n",
    "             'experiments_data.pk',\n",
    "             'git_hash.json',\n",
    "             'other_parameters.csv',\n",
    "             'parameters.pk',\n",
    "             'parameters.txt',\n",
    "             'summary.txt']\n",
    "    \n",
    "    display(files_stored)\n",
    "\n",
    "    path_experiments = em.get_path_experiments()\n",
    "\n",
    "    assert (sorted(os.listdir (path_experiments))==\n",
    "            files_stored)\n",
    "\n",
    "    # TODO TEST: test content of the above files\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_pickle (f'{path_experiments}/experiments_data.pk')\n",
    "\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    md (f'folder created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    md ('This folder has one sub-folder per run, since '\n",
    "        'multiple runs can be done with the same parameters.')\n",
    "    \n",
    "    list_run = os.listdir (f'{path_experiments}/experiments/00000')\n",
    "    \n",
    "    md (f'contents of current run at `{path_experiments}/experiments/00000`:'); print(list_run)\n",
    "    \n",
    "    # the same data frame can be obtained by doing:\n",
    "    df_bis = em.get_experiment_data ()\n",
    "    \n",
    "    pd.testing.assert_frame_equal(df,df_bis)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2000445097.py, line number: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_basic_usage\n",
      "model not found in test_basic/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['current_experiment_number.pkl',\n",
       " 'experiments',\n",
       " 'experiments_data.csv',\n",
       " 'experiments_data.pk',\n",
       " 'git_hash.json',\n",
       " 'other_parameters.csv',\n",
       " 'parameters.pk',\n",
       " 'parameters.txt',\n",
       " 'summary.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>22:53:17.769971</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.002288   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:17.769971       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "folder created in `test_basic/experiments`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This folder has one sub-folder per run, since multiple runs can be done with the same parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "contents of current run at `test_basic/experiments/00000`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other_parameters.json', 'parameters.pk', 'parameters.txt', '0', 'parameters.json']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_basic_usage, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running second experiment with same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_same_values ():\n",
    "    em = init_em ('same_values')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "        \n",
    "    em.raise_error_if_run = True\n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used \n",
    "    # are already present in the first experiment.\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3928506153.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_same_values\n",
      "model not found in test_same_values/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>22:53:17.889629</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.002182   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:17.889629       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folders created in `test_same_values/experiments`:\n",
      "['00000']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_same_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running second experiment with *almost* same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_almost_same_values ():\n",
    "    em = init_em ('almost_same_values')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "        \n",
    "    em.raise_error_if_run = True\n",
    "    # second experiment: the difference between the values of rate parameter is 1.e-16: \n",
    "    # too small to be considered different\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1+1e-16})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    # consider 1.e-17 difference big enough\n",
    "    em.raise_error_if_run = False\n",
    "    # second experiment: the difference between the values of rate parameter is 1.e-16: \n",
    "    # too small to be considered different\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1+1e-16},\n",
    "                                                        other_parameters={'precision': 1e-17})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    display (df)\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    assert list_exp == ['00000', '00001']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2527698657.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_almost_same_values\n",
      "model not found in test_almost_same_values/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2527698657.py, line number: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_almost_same_values/experiments/00001/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>22:53:17.995462</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>22:53:18.037420</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.002102   \n",
       "1     1.0   0.1                    1.0              1.0  0.001925   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:17.995462       True  \n",
       "1  22:53:18.037420       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_almost_same_values, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding new runs on previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_runs ():\n",
    "    em = init_em ('new_runs')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "        \n",
    "    # second experiment: in order to run another experiment with same parametres, we increase\n",
    "    # the run number. The default run number used in the first experiment is 0, so we indicate\n",
    "    # run_number=1\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1}, \n",
    "                                                         run_number=1)\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished', '1_validation_accuracy',\n",
    "                                            '1_test_accuracy', 'time_1','1_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    # another adding a new run number is to indicate run_number=None. This will make the experiment\n",
    "    # manager find the next run number automatically. Since we have used run numbers 0 and 1, \n",
    "    # the next run number will be 2\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1}, \n",
    "                                                         run_number=None)\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished', '1_validation_accuracy',\n",
    "                                            '1_test_accuracy', 'time_1','1_finished',\n",
    "                                            '2_validation_accuracy', '2_test_accuracy', 'time_2', \n",
    "                                            '2_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used \n",
    "    # are already present in the first experiment.\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    list_runs = os.listdir (f'{path_experiments}/experiments/00000')\n",
    "    assert sorted(list_runs) == ['0',\n",
    "                                 '1',\n",
    "                                 '2',\n",
    "                                 'other_parameters.json',\n",
    "                                 'parameters.json',\n",
    "                                 'parameters.pk',\n",
    "                                 'parameters.txt']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1468815602.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_new_runs\n",
      "model not found in test_new_runs/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1468815602.py, line number: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_new_runs/experiments/00000/1\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>1_validation_accuracy</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>time_1</th>\n",
       "      <th>1_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>22:53:18.183820</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.002816   \n",
       "\n",
       "              date 0_finished  1_validation_accuracy  1_test_accuracy  \\\n",
       "0  22:53:18.183820       True                    1.0              1.0   \n",
       "\n",
       "     time_1 1_finished  \n",
       "0  0.002168       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1468815602.py, line number: 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_new_runs/experiments/00000/2\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>1_validation_accuracy</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>time_1</th>\n",
       "      <th>1_finished</th>\n",
       "      <th>2_validation_accuracy</th>\n",
       "      <th>2_test_accuracy</th>\n",
       "      <th>time_2</th>\n",
       "      <th>2_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>22:53:18.232111</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.002816   \n",
       "\n",
       "              date 0_finished  1_validation_accuracy  1_test_accuracy  \\\n",
       "0  22:53:18.232111       True                    1.0              1.0   \n",
       "\n",
       "     time_1 1_finished  2_validation_accuracy  2_test_accuracy    time_2  \\\n",
       "0  0.002168       True                    1.0              1.0  0.001884   \n",
       "\n",
       "  2_finished  \n",
       "0       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folders created in `test_new_runs/experiments`:\n",
      "['00000']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_new_runs, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding second experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_second_experiment ():\n",
    "    em = init_em ('second')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    md ('If we run a second experiment with new parameters, a new row is '\n",
    "        'added to the dataframe, and a new folder is created:')\n",
    "    \n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.7, 'rate': 0.2})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    md (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000','00001']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/4141196873.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_second_experiment\n",
      "model not found in test_second/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "If we run a second experiment with new parameters, a new row is added to the dataframe, and a new folder is created:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/4141196873.py, line number: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_second/experiments/00001/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.8999999999999999\n",
      "epoch 1: accuracy: 1.0999999999999999\n",
      "epoch 2: accuracy: 1.2999999999999998\n",
      "epoch 3: accuracy: 1.4999999999999998\n",
      "epoch 4: accuracy: 1.6999999999999997\n",
      "epoch 5: accuracy: 1.8999999999999997\n",
      "epoch 6: accuracy: 2.0999999999999996\n",
      "epoch 7: accuracy: 2.3\n",
      "epoch 8: accuracy: 2.5\n",
      "epoch 9: accuracy: 2.7\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>22:53:18.335839</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>22:53:18.370692</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.001868   \n",
       "1     0.7   0.2                    1.0              1.0  0.002266   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:18.335839       True  \n",
       "1  22:53:18.370692       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "folders created in `test_second/experiments`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000', '00001']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_second_experiment, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding another parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_parameter ():\n",
    "    em = init_em ('another_parameter')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    # second experiment:\n",
    "    # same parameters as before plus new parameter 'epochs' not indicated in first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1, 'epochs': 5})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # a new experiment is added, and a new parameter `epochs` is added as additional column at the end\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished','epochs']).all()\n",
    "    \n",
    "    assert (df.index==[0,1]).all()\n",
    "    \n",
    "    # the new parameter has None value for all previous experiments that did not indicated its value\n",
    "    # In our case, the first experiment has None value for parameter `epochs`\n",
    "    # This means that the default value of epochs is used for that parameter.\n",
    "    # In our case, if we look at the implementation of DummyExperimentManager, we can see that \n",
    "    # the default value for epochs is 10.\n",
    "    assert df.loc[0,'epochs'] is None\n",
    "    \n",
    "    assert df.loc[1,'epochs'] == 5.0\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_new_parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2817657710.py, line number: 7\n",
      "script: /tmp/ipykernel_79481/2817657710.py, line number: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_another_parameter/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n",
      "model not found in test_another_parameter/experiments/00001/0\n",
      "model not found in \n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>22:53:18.476123</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>22:53:18.505670</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.001902   \n",
       "1     1.0   0.1                    1.0              1.0  0.001280   \n",
       "\n",
       "              date 0_finished epochs  \n",
       "0  22:53:18.476123       True   None  \n",
       "1  22:53:18.505670       True    5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_new_parameter, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding another parameter with default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_parameter_default ():\n",
    "    em = init_em ('another_parameter_default')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    # second experiment:\n",
    "    # same parameters as before plus new parameter 'epochs' not indicated in first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1, 'epochs': 10})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3810307236.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_new_parameter_default\n",
      "model not found in test_another_parameter_default/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>22:53:18.600491</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.001882   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:18.600491       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_new_parameter_default, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicating parameters that don't affect the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_other_parameters ():\n",
    "    em = init_em ('other_parameters')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment: \n",
    "    # we use the other_parameters argument to indicate a parameter that does not affect the outcome \n",
    "    # of the experiment\n",
    "    # in this example, we change the level of verbosity. This parameter should not affect how the \n",
    "    # experiment runs, and therefore we tell our experiment manager to not create a new experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1},\n",
    "                                                         other_parameters={'verbose': False})\n",
    "    \n",
    "    # second experiment:\n",
    "    # same parameters as before except for the verbosity parameter. Our experiment manager considers\n",
    "    # this experiment the same as before, and therefore it does not run it, but outputs the same results \n",
    "    # obtained before\n",
    "    em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1493082648.py, line number: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_other_parameters\n",
      "model not found in test_other_parameters/experiments/00000/0\n",
      "model not found in \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>22:53:18.706828</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.000589   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:18.706828       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_other_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove_not_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_remove_not_finished ():\n",
    "    em = init_em ('remove_not_finished')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment: we simulate that a halt before finishing\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1},\n",
    "                                                         other_parameters={'halt':True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    #assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "    #                                       'time_0', 'date', '0_finished']).all()\n",
    "    display(df)\n",
    "    \n",
    "    # second experiment: remove unfinished\n",
    "    #em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.2})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    \n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.3},\n",
    "                                                         other_parameters={'remove_not_finished':True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    #assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "    #                                       'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    #assert (df.index==[0]).all()\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_remove_not_finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1165343148.py, line number: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_remove_not_finished/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate\n",
       "0     1.0   0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1165343148.py, line number: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_remove_not_finished/experiments/00001/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.2\n",
      "epoch 1: accuracy: 1.4\n",
      "epoch 2: accuracy: 1.5999999999999999\n",
      "epoch 3: accuracy: 1.7999999999999998\n",
      "epoch 4: accuracy: 1.9999999999999998\n",
      "epoch 5: accuracy: 2.1999999999999997\n",
      "epoch 6: accuracy: 2.4\n",
      "epoch 7: accuracy: 2.6\n",
      "epoch 8: accuracy: 2.8000000000000003\n",
      "epoch 9: accuracy: 3.0000000000000004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>22:53:18.849443</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    NaN              NaN       NaN   \n",
       "1     1.0   0.2                    1.0              1.0  0.002019   \n",
       "\n",
       "              date 0_finished  \n",
       "0              NaN        NaN  \n",
       "1  22:53:18.849443       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/1165343148.py, line number: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_remove_not_finished/experiments/00002/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.3\n",
      "epoch 1: accuracy: 1.6\n",
      "epoch 2: accuracy: 1.9000000000000001\n",
      "epoch 3: accuracy: 2.2\n",
      "epoch 4: accuracy: 2.5\n",
      "epoch 5: accuracy: 2.8\n",
      "epoch 6: accuracy: 3.0999999999999996\n",
      "epoch 7: accuracy: 3.3999999999999995\n",
      "epoch 8: accuracy: 3.6999999999999993\n",
      "epoch 9: accuracy: 3.999999999999999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>22:53:18.849443</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>22:53:18.891594</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    NaN              NaN       NaN   \n",
       "1     1.0   0.2                    1.0              1.0  0.002019   \n",
       "2     1.0   0.3                    1.0              1.0  0.001970   \n",
       "\n",
       "              date 0_finished  \n",
       "0              NaN        NaN  \n",
       "1  22:53:18.849443       True  \n",
       "2  22:53:18.891594       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_remove_not_finished, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_repeat_experiment ():\n",
    "    em = init_em ('repeat_experiment')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    date = df.date.values[0]\n",
    "    \n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.1},\n",
    "                                                         other_parameters={'repeat_experiment': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert df.date.values[0] != date\n",
    "    \n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3493601434.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_repeat_experiment\n",
      "model not found in test_repeat_experiment/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.1\n",
      "epoch 1: accuracy: 1.2000000000000002\n",
      "epoch 2: accuracy: 1.3000000000000003\n",
      "epoch 3: accuracy: 1.4000000000000004\n",
      "epoch 4: accuracy: 1.5000000000000004\n",
      "epoch 5: accuracy: 1.6000000000000005\n",
      "epoch 6: accuracy: 1.7000000000000006\n",
      "epoch 7: accuracy: 1.8000000000000007\n",
      "epoch 8: accuracy: 1.9000000000000008\n",
      "epoch 9: accuracy: 2.000000000000001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>22:53:18.987353</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.001885   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:18.987353       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3493601434.py, line number: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading model from test_repeat_experiment/experiments/00000/0/model_weights.pk\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 2.100000000000001\n",
      "epoch 1: accuracy: 2.200000000000001\n",
      "epoch 2: accuracy: 2.300000000000001\n",
      "epoch 3: accuracy: 2.4000000000000012\n",
      "epoch 4: accuracy: 2.5000000000000013\n",
      "epoch 5: accuracy: 2.6000000000000014\n",
      "epoch 6: accuracy: 2.7000000000000015\n",
      "epoch 7: accuracy: 2.8000000000000016\n",
      "epoch 8: accuracy: 2.9000000000000017\n",
      "epoch 9: accuracy: 3.0000000000000018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>22:53:19.024004</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     1.0   0.1                    1.0              1.0  0.001885   \n",
       "\n",
       "              date 0_finished  \n",
       "0  22:53:19.024004       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_repeat_experiment, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_check_finished ():\n",
    "    em = init_em ('check_finished')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment: we simulate that we only run for half the number of epochs\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},\n",
    "                                                         other_parameters={'actual_epochs': 5})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    date = df.date.values[0]\n",
    "    score = df['0_validation_accuracy'].values[0]\n",
    "    \n",
    "    # second experiment: same values in parameters dictionary, without other_parameters \n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    assert (date==df.date.values[0]) and (score==df['0_validation_accuracy'].values[0])\n",
    "    \n",
    "    # third experiment: same values in parameters dictionary, with other_parameters indicating check_finished\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},\n",
    "                                                         other_parameters={'check_finished':True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    assert (df.index==[0]).all()\n",
    "    assert (date!=df.date.values[0]) and (score!=df['0_validation_accuracy'].values[0])\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2482338563.py, line number: 8\n",
      "script: /tmp/ipykernel_79481/2482338563.py, line number: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_check_finished\n",
      "model not found in test_check_finished/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "reading model from test_check_finished/experiments/00000/0/model_weights.pk\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.39999999999999997\n",
      "epoch 1: accuracy: 0.44999999999999996\n",
      "epoch 2: accuracy: 0.49999999999999994\n",
      "epoch 3: accuracy: 0.5499999999999999\n",
      "epoch 4: accuracy: 0.6\n",
      "epoch 5: accuracy: 0.65\n",
      "epoch 6: accuracy: 0.7000000000000001\n",
      "epoch 7: accuracy: 0.7500000000000001\n",
      "epoch 8: accuracy: 0.8000000000000002\n",
      "epoch 9: accuracy: 0.8500000000000002\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_check_finished, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recompute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_recompute_metrics ():\n",
    "    em = init_em ('recompute_metrics')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    df = em.get_experiment_data ()    \n",
    "    # second experiment: new values \n",
    "    em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02},\n",
    "                                                         other_parameters={'recompute_metrics': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()    \n",
    "    assert np.isnan(df['0_validation_accuracy'].values[1])\n",
    "    \n",
    "    # third experiment: new values \n",
    "    em.raise_error_if_run = False\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02, 'epochs': 10},\n",
    "                                                         other_parameters={'recompute_metrics':True,\n",
    "                                                                           'force_recompute_metrics': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    assert (df.index==[0,1]).all()\n",
    "    assert df['0_validation_accuracy'].values[1]==0.3\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2340212191.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_recompute_metrics\n",
      "model not found in test_recompute_metrics/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/2340212191.py, line number: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_recompute_metrics/experiments/00001/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.12000000000000001\n",
      "epoch 1: accuracy: 0.14\n",
      "epoch 2: accuracy: 0.16\n",
      "epoch 3: accuracy: 0.18\n",
      "epoch 4: accuracy: 0.19999999999999998\n",
      "epoch 5: accuracy: 0.21999999999999997\n",
      "epoch 6: accuracy: 0.23999999999999996\n",
      "epoch 7: accuracy: 0.25999999999999995\n",
      "epoch 8: accuracy: 0.27999999999999997\n",
      "epoch 9: accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_recompute_metrics, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prev_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### export tests.test_experiment_manager\n",
    "def test_prev_epoch ():\n",
    "    em = init_em ('prev_epoch')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first 3 experiments\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 20})\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 15})\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    \n",
    "    # more epochs\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},\n",
    "                                                         other_parameters={'prev_epoch': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    #assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "    #                                       'time_0', 'date', '0_finished']).all()    \n",
    "    #assert np.isnan(df['0_validation_accuracy'].values[1])\n",
    "    \n",
    "   \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_79481/3387418513.py\u001b[0m(3)\u001b[0;36mtest_prev_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;31m###### export tests.test_experiment_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtest_prev_epoch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_em\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'prev_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m    \u001b[0mpath_experiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_79481/3387418513.py\u001b[0m(4)\u001b[0;36mtest_prev_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      2 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtest_prev_epoch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m    \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_em\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'prev_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 4 \u001b[0;31m    \u001b[0mpath_experiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m    \u001b[0;31m# first 3 experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b em.find_closest_epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 1 at /home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py:786\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3387418513.py, line number: 7\n",
      "script: /tmp/ipykernel_79481/3387418513.py, line number: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_prev_epoch/experiments/00000/0\n",
      "model not found in \n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "model not found in test_prev_epoch/experiments/00001/0\n",
      "model not found in \n",
      "fitting model with 20 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "epoch 10: accuracy: 0.65\n",
      "epoch 11: accuracy: 0.7000000000000001\n",
      "epoch 12: accuracy: 0.7500000000000001\n",
      "epoch 13: accuracy: 0.8000000000000002\n",
      "epoch 14: accuracy: 0.8500000000000002\n",
      "epoch 15: accuracy: 0.9000000000000002\n",
      "epoch 16: accuracy: 0.9500000000000003\n",
      "epoch 17: accuracy: 1.0000000000000002\n",
      "epoch 18: accuracy: 1.0500000000000003\n",
      "epoch 19: accuracy: 1.1000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3387418513.py, line number: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not found in test_prev_epoch/experiments/00002/0\n",
      "model not found in \n",
      "fitting model with 15 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "epoch 10: accuracy: 0.65\n",
      "epoch 11: accuracy: 0.7000000000000001\n",
      "epoch 12: accuracy: 0.7500000000000001\n",
      "epoch 13: accuracy: 0.8000000000000002\n",
      "epoch 14: accuracy: 0.8500000000000002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>22:53:23.694123</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>22:53:23.793085</td>\n",
       "      <td>True</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>22:53:23.916248</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                   0.60             0.50  0.003306   \n",
       "1     0.1  0.05                   1.00             1.00  0.005049   \n",
       "2     0.1  0.05                   0.85             0.75  0.004278   \n",
       "\n",
       "              date 0_finished epochs  \n",
       "0  22:53:23.694123       True   None  \n",
       "1  22:53:23.793085       True   20.0  \n",
       "2  22:53:23.916248       True   15.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_79481/3387418513.py, line number: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(789)\u001b[0;36mfind_closest_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    787 \u001b[0;31m        \u001b[0;34m'''Finds experiment with same parameters except for number of epochs, and takes the epochs that are closer but lower than the one in parameters.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    788 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 789 \u001b[0;31m        \u001b[0mexperiment_numbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_rows_with_parameters_dict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexperiment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prev_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    790 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    791 \u001b[0;31m        \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "2\n",
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(802)\u001b[0;36mfind_closest_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    800 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    801 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 802 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    803 \u001b[0;31m        \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_numbers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    804 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mexperiment_numbers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(418)\u001b[0;36mcreate_experiment_and_run\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    416 \u001b[0;31m            prev_experiment_number = self.find_closest_epoch (experiment_data2, original_parameters,\n",
      "\u001b[0m\u001b[0;32m    417 \u001b[0;31m                                                              name_epoch=name_epoch)\n",
      "\u001b[0m\u001b[0;32m--> 418 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mprev_experiment_number\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    419 \u001b[0;31m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'using prev_epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    420 \u001b[0;31m                \u001b[0mprev_path_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_results\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  prev_experiment_number \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(419)\u001b[0;36mcreate_experiment_and_run\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    417 \u001b[0;31m                                                              name_epoch=name_epoch)\n",
      "\u001b[0m\u001b[0;32m    418 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mprev_experiment_number\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 419 \u001b[0;31m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'using prev_epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    420 \u001b[0;31m                \u001b[0mprev_path_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_results\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    421 \u001b[0;31m                \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_resume_from_checkpoint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_path_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(420)\u001b[0;36mcreate_experiment_and_run\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    418 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mprev_experiment_number\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    419 \u001b[0;31m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'using prev_epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 420 \u001b[0;31m                \u001b[0mprev_path_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_results\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    421 \u001b[0;31m                \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_resume_from_checkpoint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_path_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    422 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(421)\u001b[0;36mcreate_experiment_and_run\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    419 \u001b[0;31m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'using prev_epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    420 \u001b[0;31m                \u001b[0mprev_path_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_results\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 421 \u001b[0;31m                \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_resume_from_checkpoint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_path_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    422 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    423 \u001b[0;31m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'found previous exp: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'found' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'resume'\n",
      "> \u001b[0;32m/home/jcidatascience/jaume/workspace/remote/hpsearch/hpsearch/experiment_manager.py\u001b[0m(421)\u001b[0;36mcreate_experiment_and_run\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    419 \u001b[0;31m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'using prev_epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    420 \u001b[0;31m                \u001b[0mprev_path_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path_results\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 421 \u001b[0;31m                \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_resume_from_checkpoint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_path_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    422 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    423 \u001b[0;31m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'found previous exp: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mprev_experiment_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  prev_path_results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'test_prev_epoch/experiments/00002/0'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  experiment_data2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
      "0     0.1  0.05                   0.60             0.50  0.003306   \n",
      "1     0.1  0.05                   1.00             1.00  0.005049   \n",
      "2     0.1  0.05                   0.85             0.75  0.004278   \n",
      "\n",
      "              date 0_finished epochs  \n",
      "0  22:53:23.694123       True   None  \n",
      "1  22:53:23.793085       True   20.0  \n",
      "2  22:53:23.916248       True   15.0  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_prev_epoch, tag='dummy', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## get_git_revision_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_git_revision_hash (root_path=None):\n",
    "    try:\n",
    "        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "        git_hash = str(git_hash)\n",
    "        json.dump(git_hash, open('%s/git_hash.json' %root_path, 'wt'))\n",
    "    except:\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "        if root_path is not None:\n",
    "            logger.info ('could not get git hash, retrieving it from disk...')\n",
    "            git_hash = json.load(open('%s/git_hash.json' %root_path, 'rt'))\n",
    "        else:\n",
    "            logger.info ('could not get git hash, using empty string...')\n",
    "            git_hash = ''\n",
    "\n",
    "    return str(git_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## record_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def record_parameters (path_save, parameters, other_parameters=None):\n",
    "    with open('%s/parameters.txt' %path_save, 'wt') as f:\n",
    "        f.write('%s\\n' %mypprint(parameters, dict_name='parameters'))\n",
    "        if other_parameters is not None:\n",
    "            f.write('\\n\\n%s\\n' %mypprint(other_parameters, dict_name='other_parameters'))\n",
    "    if other_parameters is not None:\n",
    "        pickle.dump ([parameters,other_parameters],open('%s/parameters.pk' %path_save, 'wb'))\n",
    "    else:\n",
    "        pickle.dump (parameters,open('%s/parameters.pk' %path_save, 'wb'))\n",
    "    try:\n",
    "        json.dump(parameters,open('%s/parameters.json' %path_save, 'wt'))\n",
    "    except:\n",
    "        pass\n",
    "    if other_parameters is not None:\n",
    "        try:\n",
    "            json.dump(parameters,open('%s/other_parameters.json' %path_save, 'wt'))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mypprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mypprint(parameters, dict_name=None):\n",
    "    if dict_name is not None:\n",
    "        text = '%s=dict(' %dict_name\n",
    "        tpad = ' ' * len(text)\n",
    "    else:\n",
    "        text = '\\t'\n",
    "        tpad = '\\t'\n",
    "    for idx, (key, value) in enumerate(sorted(parameters.items(), key=lambda x: x[0])):\n",
    "        if type(value) is str:\n",
    "            value = '%s%s%s' %(\"'\",value,\"'\")\n",
    "        text += '{}={}'.format(key, value)\n",
    "        if idx < (len(parameters)-1):\n",
    "            text += ',\\n{}'.format(tpad)\n",
    "\n",
    "    if dict_name is not None:\n",
    "        text += ')\\n'\n",
    "    else:\n",
    "        text += '\\n'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mymakedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mymakedirs (path, exist_ok=False):\n",
    "    '''work around for python 2.7'''\n",
    "    if exist_ok:\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_or_create_experiment_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_or_create_experiment_values (path_csv, parameters, precision=1e-15):\n",
    "\n",
    "    logger = logging.getLogger(\"experiment_manager\")\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    experiment_numbers = []\n",
    "    changed_dataframe = False\n",
    "\n",
    "    if os.path.exists (path_pickle) or os.path.exists (path_csv):\n",
    "        if os.path.exists (path_pickle):\n",
    "            experiment_data = pd.read_pickle (path_pickle)\n",
    "        else:\n",
    "            experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "            experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "        experiment_data, removed_defaults = remove_defaults_from_experiment_data (experiment_data)\n",
    "\n",
    "        # Finds rows that match parameters. If the dataframe doesn't have any parameter with that name, a new column is created and changed_dataframe is set to True\n",
    "        experiment_numbers, changed_dataframe, _ = experiment_utils.find_rows_with_parameters_dict (\n",
    "            experiment_data, parameters, precision = precision\n",
    "        )\n",
    "\n",
    "        changed_dataframe = changed_dataframe or removed_defaults\n",
    "\n",
    "        if len(experiment_numbers) > 1:\n",
    "            logger.info ('more than one matching experiment: ', experiment_numbers)\n",
    "    else:\n",
    "        experiment_data = pd.DataFrame()\n",
    "\n",
    "    if len(experiment_numbers) == 0:\n",
    "        experiment_data = experiment_data.append (parameters, ignore_index=True)\n",
    "        changed_dataframe = True\n",
    "        experiment_number = experiment_data.shape[0]-1\n",
    "    else:\n",
    "        experiment_number = experiment_numbers[0]\n",
    "\n",
    "    if changed_dataframe:\n",
    "        experiment_data.to_csv(path_csv)\n",
    "        experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "    return experiment_number, experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_parameters (root_path, experiment_number, parameters):\n",
    "    \"\"\" Keeps track of dictionary to map experiment number and parameters values for the different experiments.\"\"\"\n",
    "    path_hp_dictionary = '%s/parameters.pk' %root_path\n",
    "    if os.path.exists(path_hp_dictionary):\n",
    "        all_parameters = pickle.load (open(path_hp_dictionary,'rb'))\n",
    "    else:\n",
    "        all_parameters = {}\n",
    "    if experiment_number not in all_parameters.keys():\n",
    "        str_par = '\\n\\nExperiment %d => parameters: \\n%s\\n' %(experiment_number,mypprint(parameters))\n",
    "        f = open('%s/parameters.txt' %root_path, 'at')\n",
    "        f.write(str_par)\n",
    "        f.close()\n",
    "        all_parameters[experiment_number] = parameters\n",
    "        pickle.dump (all_parameters, open(path_hp_dictionary,'wb'))\n",
    "\n",
    "    # pickle number of current experiment, for visualization\n",
    "    pickle.dump(experiment_number, open('%s/current_experiment_number.pkl' %root_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isnull (experiment_data, experiment_number, name_column):\n",
    "    return (name_column not in experiment_data.columns) or (experiment_data.loc[experiment_number, name_column] is None) or np.isnan(experiment_data.loc[experiment_number, name_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_number (root_path, parameters = {}):\n",
    "\n",
    "    path_csv = '%s/experiments_data.csv' %root_path\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    experiment_number, _ = load_or_create_experiment_values (path_csv, parameters)\n",
    "\n",
    "    return experiment_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_numbers (path_results_base, parameters_single_value, parameters_multiple_values_all):\n",
    "\n",
    "    experiment_numbers = []\n",
    "\n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))\n",
    "\n",
    "    for (i_hp, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_single_value)\n",
    "        parameters = remove_defaults (parameters)\n",
    "\n",
    "        experiment_number = get_experiment_number (path_results_base, parameters=parameters)\n",
    "        experiment_numbers.append(experiment_number)\n",
    "\n",
    "    return experiment_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_logger (name, path_results, stdout=True, mode='a', just_message = False, filename='logs.txt'):\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    for hdlr in logger.handlers[:]:  # remove all old handlers\n",
    "        logger.removeHandler(hdlr)\n",
    "\n",
    "    #if not logger.hasHandlers():\n",
    "\n",
    "    # Create handlers\n",
    "    if stdout:\n",
    "        c_handler = logging.StreamHandler()\n",
    "        c_handler.setLevel(logging.DEBUG)\n",
    "        c_format = logging.Formatter('%(message)s')\n",
    "        c_handler.setFormatter(c_format)\n",
    "        logger.addHandler(c_handler)\n",
    "\n",
    "    f_handler = logging.FileHandler('%s/%s' %(path_results, filename), mode = mode)\n",
    "    f_handler.setLevel(logging.DEBUG)\n",
    "    if just_message:\n",
    "        f_format = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    else:\n",
    "        f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    f_handler.setFormatter(f_format)\n",
    "    logger.addHandler(f_handler)\n",
    "    logger.propagate = 0\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert_experiment_script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def insert_experiment_script_path (other_parameters, logger):\n",
    "    if other_parameters.get('script_path') is None:\n",
    "        stack_level = other_parameters.get('stack_level', -3)\n",
    "        stack = traceback.extract_stack()[stack_level]\n",
    "        other_parameters['script_path'] = stack.filename\n",
    "        other_parameters['lineno'] = stack.lineno\n",
    "        logger.info ('experiment script: {}, line: {}'.format(stack.filename, stack.lineno))\n",
    "        if 'stack_level' in other_parameters:\n",
    "            del other_parameters['stack_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_parameters (experiment=None, root_path=None, root_folder = None, other_parameters={}, parameters = {}, check_experiment_matches=True):\n",
    "\n",
    "    from hpsearch.config.hpconfig import get_path_experiments, get_path_experiment\n",
    "    if root_folder is not None:\n",
    "        other_parameters['root_folder'] = root_folder\n",
    "\n",
    "    if root_path is None:\n",
    "        root_path = get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "\n",
    "    path_root_experiment = get_path_experiment (experiment, root_path=root_path)\n",
    "\n",
    "    logger = set_logger (\"experiment_manager\", root_path)\n",
    "\n",
    "    if os.path.exists('%s/parameters.pk' %path_root_experiment):\n",
    "        parameters2, other_parameters2=pickle.load(open('%s/parameters.pk' %path_root_experiment,'rb'))\n",
    "\n",
    "        other_parameters2.update(other_parameters)\n",
    "        other_parameters = other_parameters2\n",
    "\n",
    "        # if we don't add or modify parameters, we require that the old experiment number matches the new one\n",
    "        if (len(parameters) == 0) and check_experiment_matches:\n",
    "            logger.info ('requiring experiment number to be {}'.format(experiment))\n",
    "            other_parameters['experiment_number'] = experiment\n",
    "        elif 'experiment_number' in other_parameters:\n",
    "            del other_parameters['experiment_number']\n",
    "\n",
    "        parameters2.update(parameters)\n",
    "        parameters = parameters2\n",
    "    else:\n",
    "        raise FileNotFoundError ('file {} not found'.format ('%s/parameters.pk' %path_root_experiment))\n",
    "\n",
    "    return parameters, other_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_other_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_other_parameters (experiment_number, other_parameters, root_path):\n",
    "    parameters_to_save = {}\n",
    "    for k in other_parameters.keys():\n",
    "        if type(other_parameters[k]) is str:\n",
    "            parameters_to_save[k] = other_parameters[k]\n",
    "        elif np.isscalar(other_parameters[k]) and np.isreal(other_parameters[k]):\n",
    "            parameters_to_save[k] = other_parameters[k]\n",
    "\n",
    "    path_csv = '%s/other_parameters.csv' %root_path\n",
    "    df = pd.DataFrame (index = [experiment_number], data=parameters_to_save)\n",
    "\n",
    "    if os.path.exists (path_csv):\n",
    "        df_all = pd.read_csv (path_csv, index_col=0)\n",
    "        df_all = pd.concat([df_all, df], sort=True)\n",
    "        df_all = df_all.loc[~df_all.index.duplicated(keep='last')]\n",
    "    else:\n",
    "        df_all = df\n",
    "    df_all.to_csv (path_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena_old)",
   "language": "python",
   "name": "athena_old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
