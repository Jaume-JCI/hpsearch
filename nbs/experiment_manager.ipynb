{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp experiment_manager\n",
    "from nbdev.showdoc import *\n",
    "from block_types.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Manager\n",
    "\n",
    "> Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# coding: utf-8\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.utils import Bunch\n",
    "import platform\n",
    "import pprint\n",
    "import subprocess\n",
    "import json\n",
    "from multiprocessing import Process\n",
    "import logging\n",
    "import traceback\n",
    "import shutil\n",
    "from fastcore.utils import store_attr\n",
    "\n",
    "# hpsearch core API\n",
    "from hpsearch.config.manager_factory import ManagerFactory\n",
    "from hpsearch.utils import experiment_utils\n",
    "from hpsearch.utils.experiment_utils import remove_defaults\n",
    "from hpsearch.utils.organize_experiments import remove_defaults_from_experiment_data\n",
    "import hpsearch.config.hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "from block_types.utils.nbdev_utils import md\n",
    "\n",
    "from hpsearch.examples.complex_dummy_experiment_manager import ComplexDummyExperimentManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperimentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExperimentManager (object):\n",
    "\n",
    "    def __init__ (self, \n",
    "                  allow_base_class=dflt.allow_base_class,\n",
    "                  path_experiments=dflt.path_experiments,\n",
    "                  defaults=dflt.defaults,\n",
    "                  root=dflt.root,\n",
    "                  metric=dflt.metric,\n",
    "                  op=dflt.op,\n",
    "                  path_alternative=None,\n",
    "                  path_data=None,\n",
    "                  name_model_history=dflt.name_model_history,\n",
    "                  model_file_name=dflt.model_file_name,\n",
    "                  name_epoch=dflt.name_epoch,\n",
    "                  result_file=dflt.result_file):\n",
    "        \n",
    "        #store_attr ()\n",
    "        if True:\n",
    "            self.allow_base_class = allow_base_class\n",
    "            self.path_experiments = path_experiments\n",
    "            self.defaults = defaults\n",
    "            self.key_score = metric\n",
    "            self.path_alternative = path_alternative\n",
    "            self.path_data = path_data\n",
    "            self.name_model_history = name_model_history\n",
    "            self.model_file_name = model_file_name\n",
    "            self.name_epoch = name_epoch\n",
    "            self.result_file = result_file\n",
    "        \n",
    "        self.key_score = metric\n",
    "        self.parameters_non_pickable = {}\n",
    "        self.default_operations = dict(root=root,\n",
    "                                       metric=metric,\n",
    "                                       op=op)\n",
    "        self.manager_factory = ManagerFactory(allow_base_class=allow_base_class)\n",
    "        self.manager_factory.register_manager (self)\n",
    "\n",
    "    def get_default_parameters (self, parameters):\n",
    "        if not self.allow_base_class:\n",
    "            raise ImportError ('call get_default_parameters from base class is not allowed')\n",
    "        return self.defaults\n",
    "    \n",
    "    def get_default_operations (self):\n",
    "        return self.default_operations\n",
    "\n",
    "    def get_path_experiments (self, path_experiments = None, folder = None):\n",
    "        \"\"\"Gives the root path to the folder where results of experiments are stored.\"\"\"\n",
    "\n",
    "        path_experiments = (path_experiments if path_experiments is not None \n",
    "                            else self.path_experiments)\n",
    "        if folder != None: path_experiments = f'{path_experiments}/{folder}'\n",
    "\n",
    "        return path_experiments\n",
    "\n",
    "    def get_path_alternative (self, path_results):\n",
    "        if self.path_alternative is None:\n",
    "            path_alternative = path_results\n",
    "\n",
    "        return path_alternative\n",
    "\n",
    "    def get_path_data (self, run_number, root_path=None, parameters={}):\n",
    "        if self.path_data is None:\n",
    "            if root_path is None:\n",
    "                root_path = self.get_path_experiments()\n",
    "            return f'{root_path}/data'\n",
    "        else:\n",
    "            return self.path_data\n",
    "\n",
    "    def get_path_experiment (self, experiment_id, root_path=None, root_folder=None):\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        path_experiment = f'%s/experiments/%05d' %(root_path,experiment_id)\n",
    "        return path_experiment\n",
    "\n",
    "    def get_path_results (self, experiment_id, run_number, root_path=None, root_folder=None):\n",
    "        path_experiment = self.get_path_experiment (experiment_id, root_path=root_path, root_folder=root_folder)\n",
    "        path_results = '%s/%d' %(path_experiment,run_number)\n",
    "        return path_results\n",
    "    \n",
    "    def get_experiment_data (self, path_experiments=None, folder_experiments=None, experiments=None):\n",
    "        path_experiments = self.get_path_experiments(path_experiments=path_experiments, \n",
    "                                                    folder=folder_experiments)\n",
    "        path_csv = '%s/experiments_data.csv' %path_experiments\n",
    "        path_pickle = path_csv.replace('csv', 'pk')\n",
    "        if os.path.exists (path_pickle):\n",
    "            experiment_data = pd.read_pickle (path_pickle)\n",
    "        else:\n",
    "            experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "        if experiments is not None:\n",
    "            experiment_data = experiment_data.loc[experiments,:]\n",
    "            \n",
    "        return experiment_data\n",
    "    \n",
    "    def get_key_score (self, other_parameters):\n",
    "        key_score = other_parameters.get('key_score')\n",
    "        suffix_results = other_parameters.get('suffix_results', '')\n",
    "        if key_score is None and (len(suffix_results) > 0):\n",
    "            if suffix_results[0] == '_':\n",
    "                key_score = suffix_results[1:]\n",
    "            else:\n",
    "                key_score = suffix_results\n",
    "        key_score = self.key_score if key_score is None else key_score\n",
    "        \n",
    "        return key_score\n",
    "    \n",
    "    def get_name_epoch (self, other_parameters):\n",
    "        return other_parameters.get ('name_epoch', self.name_epoch)\n",
    "    \n",
    "    def remove_previous_experiments (self, path_experiments = None, folder = None):\n",
    "        path_experiments = self.get_path_experiments (path_experiments=path_experiments, \n",
    "                                                      folder=folder)\n",
    "        if os.path.exists (path_experiments):\n",
    "            shutil.rmtree (path_experiments)\n",
    "\n",
    "    def experiment_visualization (self, **kwargs):\n",
    "        raise ValueError ('this type of experiment visualization is not recognized')\n",
    "\n",
    "    def run_experiment_pipeline (self, run_number=0, path_results='./results', parameters = {}):\n",
    "        \"\"\" Runs complete learning pipeline: loading / generating data, building and learning model, applying it to data,\n",
    "        and evaluating it.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # record all parameters except for non-pickable ones\n",
    "        record_parameters (path_results, parameters)\n",
    "\n",
    "        # integrate non-pickable parameters into global dictionary\n",
    "        parameters.update (self.parameters_non_pickable)\n",
    "        self.parameters_non_pickable = {}\n",
    "\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "\n",
    "        # #####################################\n",
    "        # Evaluation\n",
    "        # #####################################\n",
    "        time_before = time.time()\n",
    "        score_dict = self._run_experiment (parameters=parameters, path_results=path_results, run_number=run_number)\n",
    "        logger.info ('time spent on this experiment: {}'.format(time.time()-time_before))\n",
    "\n",
    "        # #####################################\n",
    "        # Final scores\n",
    "        # #####################################\n",
    "        score_name = parameters.get('suffix_results','')\n",
    "        if len(score_name) > 0:\n",
    "            if score_name[0] == '_':\n",
    "                score_name = score_name[1:]\n",
    "            if score_dict.get(score_name) is not None:\n",
    "                logger.info (f'score: {score_dict.get(score_name)}')\n",
    "\n",
    "        spent_time = time.time() - time_before\n",
    "\n",
    "        return score_dict, spent_time\n",
    "\n",
    "    # *************************************************************************\n",
    "    #   run_experiment methods\n",
    "    # *************************************************************************\n",
    "    def _run_experiment (self, parameters={}, path_results='./results', run_number=None):\n",
    "\n",
    "        parameters['run_number'] = run_number\n",
    "\n",
    "        # wrap parameters\n",
    "        parameters = Bunch(**parameters)\n",
    "\n",
    "        if parameters.get('use_process', False):\n",
    "            return self.run_experiment_in_separate_process (parameters, path_results)\n",
    "        else:\n",
    "            return self.run_experiment (parameters=parameters, path_results=path_results)\n",
    "\n",
    "    def run_experiment_in_separate_process (self, parameters={}, path_results='./results'):\n",
    "\n",
    "        parameters['return_results']=False\n",
    "        p = Process(target=self.run_experiment_saving_results, args=(parameters, path_results))\n",
    "        p.start()\n",
    "        p.join()\n",
    "\n",
    "        dict_results = pickle.load (open ('%s/dict_results.pk' %path_results, 'rb'))\n",
    "\n",
    "        return dict_results\n",
    "\n",
    "    def run_experiment_saving_results (self, parameters={}, path_results='./results'):\n",
    "        dict_results = self.run_experiment (parameters=parameters, path_results=path_results)\n",
    "        pickle.dump (dict_results, open ('%s/dict_results.pk' %path_results, 'wb'))\n",
    "\n",
    "    def run_experiment (self, parameters={}, path_results='./results'):\n",
    "        raise NotImplementedError ('This method needs to be defined in subclass')\n",
    "\n",
    "\n",
    "    # *************************************************************************\n",
    "    # *************************************************************************\n",
    "    def create_experiment_and_run (self, parameters = {}, other_parameters = {}, root_path=None, \n",
    "                                   run_number=0, log_message=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # ****************************************************\n",
    "        #  preliminary set-up: logger and root_path\n",
    "        # ****************************************************\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "        if log_message is not None:\n",
    "            logger.info ('**************************************************')\n",
    "            logger.info (log_message)\n",
    "            logger.info ('**************************************************')\n",
    "            other_parameters['log_message'] = log_message\n",
    "\n",
    "        # insert path to experiment script file that called the experiment manager\n",
    "        other_parameters = other_parameters.copy()\n",
    "        insert_experiment_script_path (other_parameters, logger)\n",
    "\n",
    "        # get root_path and create directories\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder = other_parameters.get('root_folder'))\n",
    "        os.makedirs (root_path, exist_ok = True)\n",
    "\n",
    "        # ****************************************************\n",
    "        # register (subclassed) manager so that it can be used by decoupled modules\n",
    "        # ****************************************************\n",
    "        self.register_and_store_subclassed_manager ()\n",
    "\n",
    "        # ****************************************************\n",
    "        #   get experiment number given parameters\n",
    "        # ****************************************************\n",
    "        parameters = remove_defaults (parameters)\n",
    "\n",
    "        path_csv = '%s/experiments_data.csv' %root_path\n",
    "        path_pickle = path_csv.replace('csv', 'pk')\n",
    "        experiment_number, experiment_data = load_or_create_experiment_values (\n",
    "            path_csv, parameters, precision=other_parameters.get('precision', 1e-15)\n",
    "        )\n",
    "\n",
    "        #save_other_parameters (experiment_number, other_parameters, root_path)\n",
    "\n",
    "        # if old experiment, we can require that given parameters match with experiment number\n",
    "        if (other_parameters.get('experiment_number') is not None \n",
    "            and experiment_number != other_parameters.get('experiment_number')):\n",
    "            raise ValueError (f'expected number: {other_parameters.get(\"experiment_number\")}, '\n",
    "                              f'found: {experiment_number}')\n",
    "        other_parameters['experiment_number'] = experiment_number\n",
    "\n",
    "        # ****************************************************\n",
    "        # get key_score and suffix_results\n",
    "        # ****************************************************\n",
    "        key_score = self.get_key_score (other_parameters)\n",
    "        if key_score is not None:\n",
    "            suffix_results = f'_{key_score}'\n",
    "            other_parameters['suffix_results'] = suffix_results\n",
    "\n",
    "        # ****************************************************\n",
    "        #   get run_id, if not given\n",
    "        # ****************************************************\n",
    "        if run_number is None:\n",
    "            run_number = 0\n",
    "            name_score = '%d%s' %(run_number, suffix_results)\n",
    "            while not isnull(experiment_data, experiment_number, name_score):\n",
    "                logger.info ('found previous run for experiment number {}, run {}, with score {} = {}'.format(experiment_number, run_number, key_score, experiment_data.loc[experiment_number, name_score]))\n",
    "                run_number += 1\n",
    "                name_score = '%d%s' %(run_number, suffix_results)\n",
    "            logger.info ('starting experiment {} with run number {}'.format(experiment_number, run_number))\n",
    "\n",
    "        else:\n",
    "            name_score = '%d%s' %(run_number, suffix_results)\n",
    "            if not isnull(experiment_data, experiment_number, name_score):\n",
    "                previous_result = experiment_data.loc[experiment_number, name_score]\n",
    "                logger.info ('found completed: experiment number: %d, run number: %d - score: %f' %(experiment_number, run_number, previous_result))\n",
    "                logger.info (parameters)\n",
    "                if other_parameters.get('repeat_experiment', False):\n",
    "                    logger.info ('redoing experiment')\n",
    "\n",
    "        # ****************************************************\n",
    "        #   remove unfinished experiments\n",
    "        # ****************************************************\n",
    "        if other_parameters.get('remove_not_finished', False):\n",
    "            name_finished = '%d_finished' %run_number\n",
    "            if not isnull(experiment_data, experiment_number, name_finished):\n",
    "                finished = experiment_data.loc[experiment_number, name_finished]\n",
    "                logger.info (f'experiment {experiment_number}, run number {run_number}, finished {finished}')\n",
    "                if not finished:\n",
    "                    experiment_data.loc[experiment_number, name_score] = None\n",
    "                    experiment_data.to_csv (path_csv)\n",
    "                    experiment_data.to_pickle (path_pickle)\n",
    "                    logger.info (f'removed experiment {experiment_number}, '\n",
    "                                 f'run number {run_number}, finished {finished}')\n",
    "            if other_parameters.get('only_remove_not_finished', False):\n",
    "                return None, {}\n",
    "\n",
    "        unfinished_flag = False\n",
    "        name_epoch = self.get_name_epoch(other_parameters)\n",
    "        current_path_results = self.get_path_results (experiment_number, run_number=run_number, \n",
    "                                                      root_path=root_path)\n",
    "\n",
    "        # ****************************************************\n",
    "        #   check conditions for skipping experiment\n",
    "        # ****************************************************\n",
    "        if (not isnull(experiment_data, experiment_number, name_score) \n",
    "            and not other_parameters.get('repeat_experiment', False)):\n",
    "            if (other_parameters.get('check_finished', False) \n",
    "                and not self.finished_all_epochs (parameters, current_path_results, name_epoch)):\n",
    "                unfinished_flag = True\n",
    "            else:\n",
    "                logger.info ('skipping...')\n",
    "                return previous_result, {key_score: previous_result}\n",
    "        elif (isnull(experiment_data, experiment_number, name_score) \n",
    "              and other_parameters.get('recompute_metrics', False) \n",
    "              and not other_parameters.get('force_recompute_metrics', False)):\n",
    "            logger.info (f'experiment not found, skipping {run_number} due to only recompute_metrics')\n",
    "            return None, {}\n",
    "        \n",
    "        # ****************************************************\n",
    "        # log info\n",
    "        # ****************************************************\n",
    "        logger.info ('running experiment %d' %experiment_number)\n",
    "        logger.info ('run number: %d' %run_number)\n",
    "        logger.info ('\\nparameters:\\n%s' %mypprint(parameters))\n",
    "\n",
    "        # ****************************************************\n",
    "        #  get paths\n",
    "        # ****************************************************\n",
    "        # path_root_experiment folder\n",
    "        path_root_experiment = '%s/experiments/%05d' %(root_path,experiment_number)\n",
    "        mymakedirs(path_root_experiment, exist_ok=True)\n",
    "\n",
    "        # path_experiment folder (where results are)\n",
    "        path_experiment = '%s/%d' %(path_root_experiment, run_number)\n",
    "        mymakedirs(path_experiment, exist_ok=True)\n",
    "\n",
    "        # path to save big files\n",
    "        path_experiment_big_size = self.get_path_alternative (path_experiment)\n",
    "        os.makedirs (path_experiment_big_size, exist_ok = True)\n",
    "        other_parameters['path_results_big'] = path_experiment_big_size\n",
    "\n",
    "        # ****************************************************\n",
    "        # get git and record parameters\n",
    "        # ****************************************************\n",
    "        # get git revision number\n",
    "        other_parameters['git_hash'] = get_git_revision_hash(root_path)\n",
    "\n",
    "        # write parameters in root experiment folder\n",
    "        record_parameters (path_root_experiment, parameters, other_parameters)\n",
    "\n",
    "        # store hyper_parameters in dictionary that maps experiment_number with hyper_parameter values\n",
    "        store_parameters (root_path, experiment_number, parameters)\n",
    "\n",
    "        # ****************************************************************\n",
    "        # loggers\n",
    "        # ****************************************************************\n",
    "        logger_experiment = set_logger (\"experiment\", path_experiment)\n",
    "        logger_experiment.info ('script: {}, line number: {}'.format(other_parameters['script_path'], other_parameters['lineno']))\n",
    "        if os.path.exists(other_parameters['script_path']):\n",
    "            shutil.copy (other_parameters['script_path'], path_experiment)\n",
    "            shutil.copy (other_parameters['script_path'], path_root_experiment)\n",
    "\n",
    "        # summary logger\n",
    "        logger_summary = set_logger (\"summary\", root_path, mode='w', stdout=False, just_message=True, filename='summary.txt')\n",
    "        logger_summary.info ('\\n\\n{}\\nexperiment: {}, run: {}\\nscript: {}, line number: {}\\nparameters:\\n{}{}'.format('*'*100, experiment_number, run_number, other_parameters['script_path'], other_parameters['lineno'], mypprint(parameters), '*'*100))\n",
    "        if other_parameters.get('rerun_script') is not None:\n",
    "            logger_summary.info ('\\nre-run:\\n{}'.format(other_parameters['rerun_script']))\n",
    "        # same file in path_experiments\n",
    "        logger_summary2 = set_logger (\"summary\", path_experiment, mode='w', stdout=False, just_message=True, filename='summary.txt')\n",
    "        logger_summary2.info ('\\n\\n{}\\nexperiment: {}, run: {}\\nscript: {}, line number: {}\\nparameters:\\n{}{}'.format('*'*100, experiment_number, run_number, other_parameters['script_path'], other_parameters['lineno'], mypprint(parameters), '*'*100))\n",
    "\n",
    "        # ****************************************************************\n",
    "        # Do final adjustments to parameters\n",
    "        # ****************************************************************\n",
    "        parameters = parameters.copy()\n",
    "        original_parameters = parameters.copy()\n",
    "        parameters.update(other_parameters)\n",
    "\n",
    "        # add default parameters - their values are overwritten by input values, if given\n",
    "        defaults = self.get_default_parameters(parameters)\n",
    "        parameters_with_defaults = defaults.copy()\n",
    "        parameters_with_defaults.update(parameters)\n",
    "        parameters = parameters_with_defaults\n",
    "\n",
    "        # ***********************************************************\n",
    "        # resume from previous experiment \n",
    "        # ***********************************************************\n",
    "        if (isnull(experiment_data, experiment_number, name_score) \n",
    "            and other_parameters.get('check_finished_if_interrupted', False)\n",
    "            and not self.finished_all_epochs (parameters, current_path_results, name_epoch)):\n",
    "            unfinished_flag = True\n",
    "        \n",
    "        resuming_from_prev_epoch_flag = False\n",
    "        if parameters.get('prev_epoch', False):\n",
    "            logger.info('trying prev_epoch')\n",
    "            experiment_data2 = experiment_data.copy()\n",
    "            if (not unfinished_flag \n",
    "                and (other_parameters.get('repeat_experiment', False) \n",
    "                     or isnull(experiment_data, experiment_number, name_score))):\n",
    "                    experiment_data2 = experiment_data2.drop(experiment_number,axis=0)\n",
    "            prev_experiment_number = self.find_closest_epoch (experiment_data2, original_parameters, \n",
    "                                                              name_epoch=name_epoch)\n",
    "            if prev_experiment_number is not None:\n",
    "                logger.info(f'using prev_epoch: {prev_experiment_number}')\n",
    "                prev_path_results = self.get_path_results (prev_experiment_number, \n",
    "                                                           run_number=run_number, \n",
    "                                                           root_path=root_path)\n",
    "                found = self.make_resume_from_checkpoint (parameters, prev_path_results)\n",
    "                if found:\n",
    "                    logger.info (f'found previous exp: {prev_experiment_number}')\n",
    "                    if prev_experiment_number == experiment_number:\n",
    "                        if 'use_previous_best' not in other_parameters:\n",
    "                            other_parameters['use_previous_best'] = parameters.get('use_previous_best', \n",
    "                                                                                   dflt.use_previous_best)\n",
    "                        if not other_parameters['use_previous_best'] and unfinished_flag:\n",
    "                            prev_epoch = self.get_last_epoch (parameters, \n",
    "                                                              current_path_results, \n",
    "                                                              name_epoch)\n",
    "                            prev_epoch = max (int(prev_epoch), 0)\n",
    "                            parameters[name_epoch] = parameters[name_epoch] - prev_epoch\n",
    "                        logger.info ('using previous best')\n",
    "                    else:\n",
    "                        prev_epoch = experiment_data.loc[prev_experiment_number,name_epoch]\n",
    "                        prev_epoch = (int(prev_epoch) if prev_epoch is not None \n",
    "                                      else defaults.get(name_epoch))\n",
    "                        parameters[name_epoch] = parameters[name_epoch] - prev_epoch\n",
    "\n",
    "                resuming_from_prev_epoch_flag = found\n",
    "                \n",
    "\n",
    "        if not resuming_from_prev_epoch_flag and parameters.get('from_exp', None) is not None:\n",
    "            prev_experiment_number = parameters.get('from_exp', None)\n",
    "            logger.info('using previous experiment %d' %prev_experiment_number)\n",
    "            prev_path_results = self.get_path_results (prev_experiment_number, run_number=run_number, \n",
    "                                                       root_path=root_path)\n",
    "            self.make_resume_from_checkpoint (parameters, prev_path_results, use_best=True)\n",
    "\n",
    "        # ****************************************************************\n",
    "        #   Analyze if experiment was interrupted\n",
    "        # ****************************************************************\n",
    "        if parameters.get('skip_interrupted', False):\n",
    "            was_interrumpted = self.exists_current_checkpoint (parameters, path_experiment)\n",
    "            was_interrumpted = (was_interrumpted or \n",
    "                                self.obtain_last_result (parameters, path_experiment) is not None)\n",
    "            if was_interrumpted:\n",
    "                logger.info ('found intermediate results, skipping...')\n",
    "                return None, {}\n",
    "\n",
    "        # ****************************************************************\n",
    "        # retrieve last results in interrupted experiments\n",
    "        # ****************************************************************\n",
    "        run_pipeline = True\n",
    "        if parameters.get('use_last_result', False):\n",
    "            experiment_result = self.obtain_last_result (parameters, path_experiment)\n",
    "            if experiment_result is None and parameters.get('run_if_not_interrumpted', False):\n",
    "                run_pipeline = True\n",
    "            elif experiment_result is None:\n",
    "                return None, {}\n",
    "            else:\n",
    "                run_pipeline = False\n",
    "\n",
    "        # ****************************************************************\n",
    "        # run experiment\n",
    "        # ****************************************************************\n",
    "        if run_pipeline:\n",
    "            experiment_result, time_spent = self.run_experiment_pipeline (run_number,\n",
    "                                        path_experiment,\n",
    "                                        parameters=parameters)\n",
    "            finished = True\n",
    "        else:\n",
    "            finished = False\n",
    "\n",
    "        # ****************************************************************\n",
    "        #  Retrieve and store results\n",
    "        # ****************************************************************\n",
    "        if type(experiment_result)==dict:\n",
    "            dict_results = experiment_result\n",
    "            for key in dict_results.keys():\n",
    "                if key != '':\n",
    "                    experiment_data.loc[experiment_number, '%d_%s' %(run_number, key)]=dict_results[key]\n",
    "                else:\n",
    "                    experiment_data.loc[experiment_number, '%d' %run_number]=dict_results[key]\n",
    "                logger.info('{} - {}: {}'.format(run_number, key, dict_results[key]))\n",
    "        else:\n",
    "            experiment_data.loc[experiment_number, name_score]=experiment_result\n",
    "            logger.info('{} - {}: {}'.format(run_number, name_score, experiment_result))\n",
    "            dict_results = {name_score:experiment_result}\n",
    "\n",
    "        if isnull(experiment_data, experiment_number, 'time_'+str(run_number)) and finished:\n",
    "            experiment_data.loc[experiment_number,'time_'+str(run_number)]=time_spent\n",
    "        experiment_data.loc[experiment_number, 'date']=datetime.datetime.time(datetime.datetime.now())\n",
    "        experiment_data.loc[experiment_number, '%d_finished' %run_number]=finished\n",
    "\n",
    "        experiment_data.to_csv(path_csv)\n",
    "        experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "        save_other_parameters (experiment_number, other_parameters, root_path)\n",
    "\n",
    "        logger_summary2.info ('\\nresults:\\n{}'.format(dict_results))\n",
    "        logger.info ('finished experiment %d' %experiment_number)\n",
    "\n",
    "        # return final score\n",
    "        result = dict_results.get(name_score)\n",
    "        return result, dict_results\n",
    "\n",
    "    def grid_search (self, parameters_multiple_values={}, parameters_single_value={}, other_parameters = {},\n",
    "                     root_path=None, run_numbers=[0], random_search=False,\n",
    "                     load_previous=False, log_message='', nruns = None, keep='multiple'):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "        path_results_base = root_path\n",
    "\n",
    "        mymakedirs(path_results_base,exist_ok=True)\n",
    "\n",
    "        if keep=='multiple':\n",
    "            parameters_single_value = {k:parameters_single_value[k] for k in parameters_single_value.keys() if k not in parameters_multiple_values}\n",
    "        elif keep=='single':\n",
    "            parameters_multiple_values = {k:parameters_multiple_values[k] for k in parameters_multiple_values.keys() if k not in parameters_single_value}\n",
    "        else:\n",
    "            raise ValueError ('parameter keep {} not recognized: it must be either multiple or single'.format(keep))\n",
    "\n",
    "        parameters_multiple_values_all = parameters_multiple_values\n",
    "        parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))\n",
    "\n",
    "        logger = set_logger (\"experiment_manager\", path_results_base)\n",
    "        if log_message != '':\n",
    "            other_parameters['log_message'] = log_message\n",
    "        insert_experiment_script_path (other_parameters, logger)\n",
    "\n",
    "        if random_search:\n",
    "            path_random_hp = '%s/random_hp.pk' %path_results_base\n",
    "            if load_previous and os.path.exists(path_random_hp):\n",
    "                parameters_multiple_values_all = pickle.load(open(path_random_hp,'rb'))\n",
    "            else:\n",
    "                parameters_multiple_values_all = list(np.random.permutation(parameters_multiple_values_all))\n",
    "                pickle.dump (parameters_multiple_values_all, open(path_random_hp,'wb'))\n",
    "        for (i_hp, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "            parameters = parameters_multiple_values.copy()\n",
    "            parameters.update(parameters_single_value)\n",
    "\n",
    "            for (i_run, run_number) in enumerate(run_numbers):\n",
    "                logger.info('processing hyper-parameter %d out of %d' %(i_hp, len(parameters_multiple_values_all)))\n",
    "                logger.info('doing run %d out of %d' %(i_run, len(run_numbers)))\n",
    "                logger.info('%s' %log_message)\n",
    "\n",
    "                self.create_experiment_and_run (parameters=parameters, other_parameters = other_parameters,\n",
    "                                           run_number=run_number, root_path=path_results_base)\n",
    "\n",
    "        # This solves an intermitent issue found in TensorFlow (reported as bug by community)\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    def run_multiple_repetitions (self, parameters={}, other_parameters = {},\n",
    "                     root_path=None, log_message='', nruns = None, run_numbers=[0]):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        logger = set_logger (\"experiment_manager\", root_path)\n",
    "        results = np.zeros((len(run_numbers),))\n",
    "        for (i_run, run_number) in enumerate(run_numbers):\n",
    "                logger.info('doing run %d out of %d' %(i_run, len(run_numbers)))\n",
    "                logger.info('%s' %log_message)\n",
    "\n",
    "                results[i_run], dict_results  = self.create_experiment_and_run (parameters=parameters, other_parameters = other_parameters,\n",
    "                                           run_number=run_number, root_path=root_path)\n",
    "                if dict_results.get('is_pruned', False):\n",
    "                    break\n",
    "\n",
    "        mu, std = results.mean(), results.std()\n",
    "        logger.info ('mean {}: {}, std: {}'.format(other_parameters.get('key_score',''), mu, std))\n",
    "\n",
    "        dict_results[other_parameters.get('key_score','cost')] = mu\n",
    "\n",
    "        return mu, std, dict_results\n",
    "\n",
    "\n",
    "    def hp_optimization (self, parameter_sampler = None, root_path=None, log_message=None, parameters={}, other_parameters={}, nruns=None):\n",
    "\n",
    "        import optuna\n",
    "        from optuna.pruners import SuccessiveHalvingPruner, MedianPruner\n",
    "        from optuna.samplers import RandomSampler, TPESampler\n",
    "        from optuna.integration.skopt import SkoptSampler\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        os.makedirs(root_path, exist_ok=True)\n",
    "        logger = set_logger (\"experiment_manager\", root_path)\n",
    "        if log_message != '':\n",
    "            other_parameters['log_message'] = log_message\n",
    "        insert_experiment_script_path (other_parameters, logger)\n",
    "\n",
    "        # n_warmup_steps: Disable pruner until the trial reaches the given number of step.\n",
    "        sampler_method = other_parameters.get('sampler_method', 'random')\n",
    "        pruner_method = other_parameters.get('pruner_method', 'halving')\n",
    "        n_evaluations = other_parameters.get('n_evaluations', 20)\n",
    "        seed = other_parameters.get('seed', 0)\n",
    "        if sampler_method == 'random':\n",
    "            sampler = RandomSampler(seed=seed)\n",
    "        elif sampler_method == 'tpe':\n",
    "            sampler = TPESampler(n_startup_trials=other_parameters.get('n_startup_trials', 5), seed=seed)\n",
    "        elif sampler_method == 'skopt':\n",
    "            # cf https://scikit-optimize.github.io/#skopt.Optimizer\n",
    "            # GP: gaussian process\n",
    "            # Gradient boosted regression: GBRT\n",
    "            sampler = SkoptSampler(skopt_kwargs={'base_estimator': \"GP\", 'acq_func': 'gp_hedge'})\n",
    "        else:\n",
    "            raise ValueError('Unknown sampler: {}'.format(sampler_method))\n",
    "\n",
    "        if pruner_method == 'halving':\n",
    "            pruner = SuccessiveHalvingPruner(min_resource=1, reduction_factor=4, min_early_stopping_rate=0)\n",
    "        elif pruner_method == 'median':\n",
    "            pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=n_evaluations // 3)\n",
    "        elif pruner_method == 'none':\n",
    "            # Do not prune\n",
    "            pruner = MedianPruner(n_startup_trials=other_parameters.get('n_trials', 10), n_warmup_steps=n_evaluations)\n",
    "        else:\n",
    "            raise ValueError('Unknown pruner: {}'.format(pruner_method))\n",
    "\n",
    "        logger.info (\"Sampler: {} - Pruner: {}\".format(sampler_method, pruner_method))\n",
    "\n",
    "        #study = optuna.create_study(sampler=sampler, pruner=pruner)\n",
    "        study_name = other_parameters.get('study_name', 'hp_study')  # Unique identifier of the study.\n",
    "        study = optuna.create_study(study_name=study_name, storage='sqlite:///%s/%s.db' %(root_path, study_name), sampler=sampler, pruner=pruner, load_if_exists=True)\n",
    "\n",
    "        def objective(trial):\n",
    "\n",
    "            hp_parameters = parameters.copy()\n",
    "            self.parameters_non_pickable = dict(trial=trial)\n",
    "\n",
    "            if parameter_sampler is not None:\n",
    "                hp_parameters.update(parameter_sampler(trial))\n",
    "\n",
    "            if nruns is None:\n",
    "                _, dict_results = self.create_experiment_and_run (parameters=hp_parameters, other_parameters = other_parameters, root_path=root_path, run_number=other_parameters.get('run_number'))\n",
    "            else:\n",
    "                mu_best, std_best, dict_results = self.run_multiple_repetitions (parameters=hp_parameters, other_parameters = other_parameters, root_path=root_path, nruns=nruns)\n",
    "\n",
    "            if dict_results.get('is_pruned', False):\n",
    "                raise optuna.structs.TrialPruned()\n",
    "\n",
    "            return dict_results[other_parameters.get('key_score', 'cost')]\n",
    "\n",
    "        optuna.logging.disable_propagation()\n",
    "        study.optimize(objective, n_trials=other_parameters.get('n_trials', 10), n_jobs=other_parameters.get('n_jobs', 1))\n",
    "\n",
    "        logger.info ('Number of finished trials: {}'.format(len(study.trials)))\n",
    "        logger.info ('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        logger.info ('Value: {}'.format(trial.value))\n",
    "        logger.info ('best params: {}'.format (study.best_params))\n",
    "        best_value = trial.value\n",
    "\n",
    "        nruns_best = other_parameters.get('nruns_best', 0)\n",
    "        if nruns_best > 0:\n",
    "            logger.info ('running best configuration %d times' %nruns_best)\n",
    "            parameters.update (study.best_params)\n",
    "            mu_best, std_best, _ = self.run_multiple_repetitions (parameters=parameters, other_parameters = other_parameters,\n",
    "                                            root_path=root_path, nruns=nruns_best)\n",
    "            best_value = mu_best\n",
    "\n",
    "        return best_value\n",
    "\n",
    "    def rerun_experiment (self, experiments=[], run_numbers=[0], nruns = None, root_path=None, root_folder = None,\n",
    "                          other_parameters={}, parameters = {}, parameter_sampler = None, parameters_multiple_values = None,\n",
    "                          log_message='', only_if_exists=False):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        if root_folder is not None:\n",
    "            other_parameters['root_folder'] = root_folder\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "\n",
    "        logger = set_logger (\"experiment_manager\", root_path)\n",
    "\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        parameters_original = parameters\n",
    "        other_parameters_original = other_parameters\n",
    "        for experiment_id in experiments:\n",
    "            check_experiment_matches = (parameters_multiple_values is None) and (parameter_sampler is None)\n",
    "            parameters, other_parameters = load_parameters (experiment=experiment_id, root_path=root_path, root_folder = root_folder,\n",
    "                                                            other_parameters=other_parameters_original, parameters = parameters_original,\n",
    "                                                            check_experiment_matches=check_experiment_matches)\n",
    "\n",
    "            # we need to set the following flag to False, since otherwise when we request to store the intermediate results\n",
    "            # and the experiment did not start, we do not run the experiment\n",
    "            if other_parameters.get('use_last_result', False) and not other_parameters_original.get('use_last_result', False):\n",
    "                logger.debug ('changing other_parameters[\"use_last_result\"] to False')\n",
    "                other_parameters['use_last_result'] = False\n",
    "            logger.info (f'running experiment {experiment_id} with parameters:\\n{parameters}\\nother_parameters:\\n{other_parameters}')\n",
    "\n",
    "            if parameter_sampler is not None:\n",
    "                logger.info ('running hp_optimization')\n",
    "                insert_experiment_script_path (other_parameters, logger)\n",
    "                self.hp_optimization (parameter_sampler = parameter_sampler, root_path=root_path, log_message=log_message,\n",
    "                                        parameters=parameters, other_parameters=other_parameters)\n",
    "            elif parameters_multiple_values is not None:\n",
    "                self.grid_search (parameters_multiple_values=parameters_multiple_values, parameters_single_value=parameters,\n",
    "                                    other_parameters = other_parameters, root_path=root_path, run_numbers=run_numbers, log_message=log_message)\n",
    "            else:\n",
    "                if only_if_exists:\n",
    "                    run_numbers = [run_number for run_number in run_numbers if os.path.exists('%s/%d' %(path_root_experiment, run_number))]\n",
    "\n",
    "                script_parameters = {}\n",
    "                insert_experiment_script_path (script_parameters, logger)\n",
    "                other_parameters['rerun_script'] = script_parameters\n",
    "                self.run_multiple_repetitions (parameters=parameters, other_parameters = other_parameters, root_path=root_path,\n",
    "                                                log_message=log_message, run_numbers=run_numbers)\n",
    "\n",
    "    def rerun_experiment_pipeline (self, experiments, run_numbers=None, root_path=None, root_folder=None, new_parameters={}, save_results=False):\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        for experiment_id in experiments:\n",
    "            path_root_experiment = self.get_path_experiment (experiment_id, root_path=root_path)\n",
    "\n",
    "            parameters, other_parameters=pickle.load(open('%s/parameters.pk' %path_root_experiment,'rb'))\n",
    "            parameters = parameters.copy()\n",
    "            parameters.update(other_parameters)\n",
    "            parameters.update(new_parameters)\n",
    "            for run_number in run_numbers:\n",
    "                path_experiment = '%s/%d/' %(path_root_experiment, run_number)\n",
    "                path_data = self.get_path_data (run_number, root_path, parameters)\n",
    "                score, _ = self.run_experiment_pipeline (run_number, path_experiment, parameters = parameters)\n",
    "\n",
    "                if save_results:\n",
    "                    experiment_number = experiment_id\n",
    "                    path_csv = '%s/experiments_data.csv' %root_path\n",
    "                    path_pickle = path_csv.replace('csv', 'pk')\n",
    "                    if os.path.exists(path_pickle):\n",
    "                        experiment_data = pd.read_pickle (path_pickle)\n",
    "                    else:\n",
    "                        experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "                    if type(score)==dict:\n",
    "                        for key in score.keys():\n",
    "                            if key != '':\n",
    "                                experiment_data.loc[experiment_number, '%d_%s' %(run_number, key)]=score[key]\n",
    "                            else:\n",
    "                                experiment_data.loc[experiment_number, '%d' %run_number]=score[key]\n",
    "                    else:\n",
    "                        experiment_data.loc[experiment_number, name_score]=score\n",
    "                    experiment_data.to_csv(path_csv)\n",
    "                    experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "    def rerun_experiment_par (self, experiments, run_numbers=None, root_path=None, root_folder=None, parameters={}):\n",
    "\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        for experiment_id in experiments:\n",
    "            path_root_experiment = self.get_path_experiment (experiment_id, root_path=root_path)\n",
    "\n",
    "            for run_number in run_numbers:\n",
    "                path_experiment = '%s/%d/' %(path_root_experiment, run_number)\n",
    "                self.run_experiment_pipeline (run_number, path_experiment, parameters = parameters)\n",
    "\n",
    "    def record_intermediate_results (self, experiments=range(100), run_numbers=range(100), root_path=None, root_folder=None, new_parameters={}, remove=False):\n",
    "\n",
    "        if remove:\n",
    "            new_parameters.update (remove_not_finished=True, only_remove_not_finished=True)\n",
    "        else:\n",
    "            new_parameters.update (use_last_result=True)\n",
    "\n",
    "        self.rerun_experiment_and_save(experiments=experiments, run_numbers=run_numbers,\n",
    "            root_path=root_path, root_folder=root_folder,\n",
    "            new_parameters=new_parameters)\n",
    "        \n",
    "    def find_closest_epoch (self, experiment_data, parameters, name_epoch=dflt.name_epoch):\n",
    "        '''Finds experiment with same parameters except for number of epochs, and takes the epochs that are closer but lower than the one in parameters.'''\n",
    "\n",
    "        experiment_numbers, _, _ = experiment_utils.find_rows_with_parameters_dict (experiment_data, parameters, ignore_keys=[name_epoch,'prev_epoch'])\n",
    "\n",
    "        defaults = self.get_default_parameters(parameters)\n",
    "        current_epoch = parameters.get(name_epoch, defaults.get(name_epoch))\n",
    "        if current_epoch is None:\n",
    "            current_epoch = -1\n",
    "        if len(experiment_numbers) > 1:\n",
    "            epochs = experiment_data.loc[experiment_numbers,name_epoch].copy()\n",
    "            epochs[epochs.isnull()]=defaults.get(name_epoch)\n",
    "            epochs = epochs.loc[epochs<=current_epoch]\n",
    "            if epochs.shape[0] == 0:\n",
    "                return None\n",
    "            else:\n",
    "                return epochs.astype(int).idxmax()\n",
    "        elif len(experiment_numbers) == 1:\n",
    "            return experiment_numbers[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_last_epoch (self, parameters, path_results, \n",
    "                             name_epoch=dflt.name_epoch, name_last_epoch=dflt.name_last_epoch):\n",
    "        \n",
    "        name_model_history = parameters.get('name_model_history', self.name_model_history)\n",
    "        path_model_history = f'{path_results}/{name_model_history}'\n",
    "\n",
    "        prev_epoch = -1\n",
    "        if os.path.exists(path_model_history):\n",
    "            summary = pickle.load(open(path_model_history, 'rb'))\n",
    "            prev_epoch = summary.get(name_last_epoch)\n",
    "            if prev_epoch is None:\n",
    "                key_score = self.get_key_score (parameters)\n",
    "                if key_score in summary and (isinstance(summary[key_score], list) \n",
    "                                             or isinstance(summary[key_score], np.array)):\n",
    "                    prev_epoch = (~np.isnan(summary[key_score])).sum()\n",
    "\n",
    "        return prev_epoch\n",
    "        \n",
    "    def finished_all_epochs (self, parameters, path_results, \n",
    "                             name_epoch=dflt.name_epoch, name_last_epoch=dflt.name_last_epoch):\n",
    "        defaults = self.get_default_parameters (parameters)\n",
    "        current_epoch = parameters.get(name_epoch, defaults.get(name_epoch))\n",
    "        prev_epoch = self.get_last_epoch (parameters, path_results, name_epoch=name_epoch, \n",
    "                                          name_last_epoch=name_last_epoch)\n",
    "        \n",
    "        if prev_epoch >= current_epoch:\n",
    "            finished = True\n",
    "        else:\n",
    "            finished = False\n",
    "\n",
    "        return finished\n",
    "    \n",
    "    def make_resume_from_checkpoint (self, parameters, prev_path_results, use_best=False):\n",
    "\n",
    "        if parameters.get('previous_model_file_name') is not None:\n",
    "            previous_model_file_name = parameters['previous_model_file_name']\n",
    "        else:\n",
    "            model_extension = parameters.get('model_extension', 'h5')\n",
    "            model_name = parameters.get('model_name', 'checkpoint_')\n",
    "            epoch_offset = parameters.get('epoch_offset', 0)\n",
    "            name_best_model = parameters.get('name_best_model', 'best_model')\n",
    "\n",
    "        found = False\n",
    "        name_model_history = parameters.get('name_model_history', 'model_history.pk')\n",
    "        name_last_epoch = parameters.get('name_last_epoch', dflt.name_last_epoch)\n",
    "        path_model_history = f'{prev_path_results}/{name_model_history}'\n",
    "        if os.path.exists(path_model_history):\n",
    "            parameters['resume_summary'] = path_model_history\n",
    "            found = True\n",
    "            parameters['prev_path_results'] = prev_path_results\n",
    "            if parameters.get('previous_model_file_name') is not None:\n",
    "                parameters['resume'] = f'{prev_path_results}/{previous_model_file_name}'\n",
    "            elif use_best:\n",
    "                parameters['resume'] = f'{prev_path_results}/{name_best_model}.{model_extension}'\n",
    "            else:\n",
    "                summary = pickle.load(open(path_model_history, 'rb'))\n",
    "                prev_epoch = summary.get(name_last_epoch)\n",
    "                key_score = self.get_key_score (parameters)\n",
    "                if prev_epoch is None:\n",
    "                    if key_score in summary and (isinstance(summary[key_score], list) \n",
    "                                                 or isinstance(summary[key_score], np.array)):\n",
    "                        prev_epoch = (~np.isnan(summary[key_score])).sum()\n",
    "                    else:\n",
    "                        prev_epoch = 0\n",
    "\n",
    "                if prev_epoch >= 0:\n",
    "                    parameters['resume'] = f'{prev_path_results}/{model_name}{prev_epoch+epoch_offset}.{model_extension}'\n",
    "            if not os.path.exists(parameters['resume']):\n",
    "                path_resume2 = f'{prev_path_results}/{self.model_file_name}'\n",
    "                if os.path.exists (path_resume2):\n",
    "                    parameters['resume'] = path_resume2\n",
    "                else:\n",
    "                    parameters['resume'] = ''\n",
    "                    parameters['prev_path_results'] = ''\n",
    "                    found = False\n",
    "\n",
    "        return found\n",
    "    \n",
    "    def exists_current_checkpoint (self, parameters, path_results):\n",
    "        model_file_name = self.get_parameter (parameters, 'model_file_name')\n",
    "        return os.path.exists (f'{path_results}/{model_file_name}')\n",
    "    \n",
    "    def get_parameter (self, parameters, key):\n",
    "        parameter = parameters.get(key)\n",
    "        return parameter if parameter is not None else getattr(self, key)\n",
    "    \n",
    "    def obtain_last_result (self, parameters, path_results):\n",
    "\n",
    "        if parameters.get('use_last_result_from_dict', False):\n",
    "            return self.obtain_last_result_from_dict (parameters, path_results)\n",
    "        name_result_file = self.get_parameter (parameters, 'name_model_history')\n",
    "        path_results_file = f'{path_results}/{name_result_file}'\n",
    "        dict_results = None\n",
    "        if os.path.exists (path_results_file):\n",
    "            history = pickle.load(open(path_results_file, 'rb'))\n",
    "            metrics = parameters.get('key_scores')\n",
    "            if metrics is None:\n",
    "                metrics = history.keys()\n",
    "            ops = parameters.get('ops')\n",
    "            if ops is None:\n",
    "                ops = ['max'] * len(metrics)\n",
    "            if type(ops) is str:\n",
    "                ops = [ops] * len(metrics)\n",
    "            if type(ops) is dict:\n",
    "                ops_dict = ops\n",
    "                ops = ['max'] * len(metrics)\n",
    "                i = 0\n",
    "                for k in metrics:\n",
    "                    if k in ops_dict.keys():\n",
    "                        ops[i] = ops_dict[k]\n",
    "                    i += 1\n",
    "            dict_results = {}\n",
    "            max_last_position = -1\n",
    "            for metric, op in zip(metrics, ops):\n",
    "                if metric in history.keys():\n",
    "                    history_array = history[metric]\n",
    "                    score = min(history_array) if op == 'min' else max(history_array)\n",
    "                    last_position = np.where(np.array(history_array).ravel()==0)[0]\n",
    "                    if len(last_position) > 0:\n",
    "                        last_position = last_position[0] - 1\n",
    "                    else:\n",
    "                        last_position = len(history_array)\n",
    "                    dict_results[metric] = score\n",
    "                else:\n",
    "                    last_position = -1\n",
    "                max_last_position = max(last_position, max_last_position)\n",
    "\n",
    "            dict_results['last'] = max_last_position\n",
    "            if max_last_position < parameters.get('min_iterations', dflt.min_iterations):\n",
    "                dict_results = None\n",
    "                print (f'not storing result from {path_results} with iterations {max_last_position}')\n",
    "            else:\n",
    "                print (f'storing result from {path_results} with iterations {max_last_position}')\n",
    "\n",
    "        return dict_results\n",
    "    \n",
    "    #export\n",
    "    def obtain_last_result_from_dict (self, parameters, path_results):\n",
    "        name_result_file = self.get_parameter(parameters, 'result_file')\n",
    "        path_results_file = '%s/%s' %(path_results, name_result_file)\n",
    "        dict_results = None\n",
    "        if os.path.exists (path_results_file):\n",
    "            dict_results = pickle.load(open(path_results_file, 'rb'))\n",
    "            if 'last' not in dict_results.keys() and 'epoch' in dict_results.keys():\n",
    "                dict_results['last'] = dict_results['epoch']\n",
    "            max_last_position = dict_results['last']\n",
    "            if max_last_position < parameters.get('min_iterations', dflt.min_iterations):\n",
    "                dict_results = None\n",
    "                print ('not storing result from {} with iterations {}'.format(path_results, max_last_position))\n",
    "            else:\n",
    "                print ('storing result from {} with iterations {}'.format(path_results, max_last_position))\n",
    "\n",
    "        return dict_results\n",
    "    \n",
    "    def register_and_store_subclassed_manager (self):\n",
    "        #self.logger.debug ('registering')\n",
    "        self.manager_factory.register_manager (self)\n",
    "        self.manager_factory.write_manager (self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### create_experiment_and_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the experiment manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def init_em (name_folder):\n",
    "    em = ComplexDummyExperimentManager (path_experiments=f'test_{name_folder}')\n",
    "\n",
    "    em.remove_previous_experiments()\n",
    "\n",
    "    with pytest.raises (FileNotFoundError):\n",
    "        os.listdir(em.get_path_experiments())\n",
    "    \n",
    "    return em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_experiment_and_run` is the main function of the `ExperimentManager`. All other functions make use of it adding additional functionalities.\n",
    "\n",
    "In order to call `create_experiment_and_run`, we pass a dictionary of parameters characterizing the experiment we want to run, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_basic_usage ():\n",
    "    em = init_em ('basic')\n",
    "    \n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    # The output is a tuple of two objects:\n",
    "    #1. The main result metric. In our case, we didn't indicate the name of this metric, \n",
    "    #and therefore we get None.\n",
    "    #1. A dictionary containing all the performance metrics for this experiment.\n",
    "    \n",
    "    assert result is None\n",
    "    assert dict_results == {'validation_accuracy': 0.6, 'test_accuracy': 0.5}\n",
    "\n",
    "    # Eight files  are stored in *path_experiments*, and the `experiments` folder is created:\n",
    "    \n",
    "    files_stored = ['current_experiment_number.pkl',\n",
    "             'experiments',\n",
    "             'experiments_data.csv',\n",
    "             'experiments_data.pk',\n",
    "             'git_hash.json',\n",
    "             'other_parameters.csv',\n",
    "             'parameters.pk',\n",
    "             'parameters.txt',\n",
    "             'summary.txt']\n",
    "    \n",
    "    display(files_stored)\n",
    "\n",
    "    path_experiments = em.get_path_experiments()\n",
    "\n",
    "    assert (sorted(os.listdir (path_experiments))==\n",
    "            files_stored)\n",
    "\n",
    "    # TODO TEST: test content of the above files\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_pickle (f'{path_experiments}/experiments_data.pk')\n",
    "\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    print (f'folder created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    print ('This folder has one sub-folder per run, since '\n",
    "            'multiple runs can be done with the same parameters.')\n",
    "    \n",
    "    list_run = os.listdir (f'{path_experiments}/experiments/00000')\n",
    "    \n",
    "    print (f'contents of current run at `{path_experiments}/experiments/00000`:'); print(list_run)\n",
    "    \n",
    "    # the same data frame can be obtained by doing:\n",
    "    df_bis = em.get_experiment_data ()\n",
    "    \n",
    "    pd.testing.assert_frame_equal(df,df_bis)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/725426095.py, line number: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_basic_usage\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['current_experiment_number.pkl',\n",
       " 'experiments',\n",
       " 'experiments_data.csv',\n",
       " 'experiments_data.pk',\n",
       " 'git_hash.json',\n",
       " 'other_parameters.csv',\n",
       " 'parameters.pk',\n",
       " 'parameters.txt',\n",
       " 'summary.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>23:04:59.067385</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001736   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:04:59.067385       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder created in `test_basic/experiments`:\n",
      "['00000']\n",
      "This folder has one sub-folder per run, since multiple runs can be done with the same parameters.\n",
      "contents of current run at `test_basic/experiments/00000`:\n",
      "['other_parameters.json', 'parameters.pk', 'parameters.txt', '0', 'parameters.json']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_basic_usage, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running second experiment with same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_same_values ():\n",
    "    em = init_em ('same_values')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "        \n",
    "    em.raise_error_if_run = True\n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used \n",
    "    # are already present in the first experiment.\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/314616621.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_same_values\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>23:04:59.182501</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.002022   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:04:59.182501       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folders created in `test_same_values/experiments`:\n",
      "['00000']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_same_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running second experiment with *almost* same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_almost_same_values ():\n",
    "    em = init_em ('almost_same_values')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "        \n",
    "    em.raise_error_if_run = True\n",
    "    # second experiment: the difference between the values of rate parameter is 1.e-16: \n",
    "    # too small to be considered different\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05+1e-16})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    # consider 1.e-17 difference big enough\n",
    "    em.raise_error_if_run = False\n",
    "    # second experiment: the difference between the values of rate parameter is 1.e-16: \n",
    "    # too small to be considered different\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05+1e-16},\n",
    "                                                        other_parameters={'precision': 1e-17})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    display (df)\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    assert list_exp == ['00000', '00001']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2249530880.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_almost_same_values\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2249530880.py, line number: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.1500000000000001\n",
      "epoch 1: accuracy: 0.2000000000000002\n",
      "epoch 2: accuracy: 0.25000000000000033\n",
      "epoch 3: accuracy: 0.30000000000000043\n",
      "epoch 4: accuracy: 0.35000000000000053\n",
      "epoch 5: accuracy: 0.40000000000000063\n",
      "epoch 6: accuracy: 0.45000000000000073\n",
      "epoch 7: accuracy: 0.5000000000000009\n",
      "epoch 8: accuracy: 0.5500000000000009\n",
      "epoch 9: accuracy: 0.600000000000001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>23:04:59.294735</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>23:04:59.331260</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.002137   \n",
       "1     0.1  0.05                    0.6              0.5  0.001731   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:04:59.294735       True  \n",
       "1  23:04:59.331260       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_almost_same_values, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding new runs on previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_runs ():\n",
    "    em = init_em ('new_runs')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "        \n",
    "    # second experiment: in order to run another experiment with same parametres, we increase\n",
    "    # the run number. The default run number used in the first experiment is 0, so we indicate\n",
    "    # run_number=1\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05}, \n",
    "                                                         run_number=1)\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished', '1_validation_accuracy',\n",
    "                                            '1_test_accuracy', 'time_1','1_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    # another adding a new run number is to indicate run_number=None. This will make the experiment\n",
    "    # manager find the next run number automatically. Since we have used run numbers 0 and 1, \n",
    "    # the next run number will be 2\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05}, \n",
    "                                                         run_number=None)\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished', '1_validation_accuracy',\n",
    "                                            '1_test_accuracy', 'time_1','1_finished',\n",
    "                                            '2_validation_accuracy', '2_test_accuracy', 'time_2', \n",
    "                                            '2_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used \n",
    "    # are already present in the first experiment.\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000']\n",
    "    \n",
    "    list_runs = os.listdir (f'{path_experiments}/experiments/00000')\n",
    "    assert sorted(list_runs) == ['0',\n",
    "                                 '1',\n",
    "                                 '2',\n",
    "                                 'other_parameters.json',\n",
    "                                 'parameters.json',\n",
    "                                 'parameters.pk',\n",
    "                                 'parameters.txt']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3459580111.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_new_runs\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3459580111.py, line number: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>1_validation_accuracy</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>time_1</th>\n",
       "      <th>1_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>23:04:59.491580</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001668   \n",
       "\n",
       "              date 0_finished  1_validation_accuracy  1_test_accuracy  \\\n",
       "0  23:04:59.491580       True                    0.6              0.5   \n",
       "\n",
       "     time_1 1_finished  \n",
       "0  0.002012       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3459580111.py, line number: 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>1_validation_accuracy</th>\n",
       "      <th>1_test_accuracy</th>\n",
       "      <th>time_1</th>\n",
       "      <th>1_finished</th>\n",
       "      <th>2_validation_accuracy</th>\n",
       "      <th>2_test_accuracy</th>\n",
       "      <th>time_2</th>\n",
       "      <th>2_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>23:04:59.536757</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>True</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001668   \n",
       "\n",
       "              date 0_finished  1_validation_accuracy  1_test_accuracy  \\\n",
       "0  23:04:59.536757       True                    0.6              0.5   \n",
       "\n",
       "     time_1 1_finished  2_validation_accuracy  2_test_accuracy    time_2  \\\n",
       "0  0.002012       True                    0.6              0.5  0.002238   \n",
       "\n",
       "  2_finished  \n",
       "0       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folders created in `test_new_runs/experiments`:\n",
      "['00000']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_new_runs, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding second experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_second_experiment ():\n",
    "    em = init_em ('second')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    md ('If we run a second experiment with new parameters, a new row is '\n",
    "        'added to the dataframe, and a new folder is created:')\n",
    "    \n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.7, 'rate': 0.2})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    \n",
    "    md (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "    \n",
    "    assert list_exp == ['00000','00001']\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3126725428.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_second_experiment\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "If we run a second experiment with new parameters, a new row is added to the dataframe, and a new folder is created:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3126725428.py, line number: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.8999999999999999\n",
      "epoch 1: accuracy: 1.0999999999999999\n",
      "epoch 2: accuracy: 1.2999999999999998\n",
      "epoch 3: accuracy: 1.4999999999999998\n",
      "epoch 4: accuracy: 1.6999999999999997\n",
      "epoch 5: accuracy: 1.8999999999999997\n",
      "epoch 6: accuracy: 2.0999999999999996\n",
      "epoch 7: accuracy: 2.3\n",
      "epoch 8: accuracy: 2.5\n",
      "epoch 9: accuracy: 2.7\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>23:04:59.682563</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>23:04:59.716228</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001703   \n",
       "1     0.7  0.20                    1.0              1.0  0.001819   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:04:59.682563       True  \n",
       "1  23:04:59.716228       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "folders created in `test_second/experiments`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000', '00001']\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_second_experiment, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding another parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b\n",
       "0  1  1.0\n",
       "1  2  NaN\n",
       "2  3  3.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'a':[1,2,3],'b':[1,None,3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().loc[1,'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_parameter ():\n",
    "    em = init_em ('another_parameter')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    # second experiment:\n",
    "    # same parameters as before plus new parameter 'epochs' not indicated in first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # a new experiment is added, and a new parameter `epochs` is added as additional column at the end\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished','epochs']).all()\n",
    "    \n",
    "    assert (df.index==[0,1]).all()\n",
    "    \n",
    "    # the new parameter has None value for all previous experiments that did not indicated its value\n",
    "    # In our case, the first experiment has None value for parameter `epochs`\n",
    "    # This means that the default value of epochs is used for that parameter.\n",
    "    # In our case, if we look at the implementation of DummyExperimentManager, we can see that \n",
    "    # the default value for epochs is 10.\n",
    "    assert df.isna().loc[0,'epochs']\n",
    "    \n",
    "    assert df.loc[1,'epochs'] == 5.0\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2342689092.py, line number: 7\n",
      "script: /tmp/ipykernel_130431/2342689092.py, line number: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_new_parameter\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>23:04:59.961083</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>23:04:59.991625</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                   0.60             0.50  0.001644   \n",
       "1     0.1  0.05                   0.35             0.45  0.001034   \n",
       "\n",
       "              date 0_finished epochs  \n",
       "0  23:04:59.961083       True   None  \n",
       "1  23:04:59.991625       True    5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_new_parameter, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding another parameter with default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_parameter_default ():\n",
    "    em = init_em ('another_parameter_default')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    # second experiment:\n",
    "    # same parameters as before plus new parameter 'epochs' not indicated in first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2374034115.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_new_parameter_default\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>23:05:00.101218</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001687   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:05:00.101218       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_new_parameter_default, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicating parameters that don't affect the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_other_parameters ():\n",
    "    em = init_em ('other_parameters')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment: \n",
    "    # we use the other_parameters argument to indicate a parameter that does not affect the outcome \n",
    "    # of the experiment\n",
    "    # in this example, we change the level of verbosity. This parameter should not affect how the \n",
    "    # experiment runs, and therefore we tell our experiment manager to not create a new experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         other_parameters={'verbose': False})\n",
    "    \n",
    "    # second experiment:\n",
    "    # same parameters as before except for the verbosity parameter. Our experiment manager considers\n",
    "    # this experiment the same as before, and therefore it does not run it, but outputs the same results \n",
    "    # obtained before\n",
    "    em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/1875725149.py, line number: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_other_parameters\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "experiment dataframe:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>23:05:00.207436</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.000246   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:05:00.207436       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_other_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove_not_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the parameter that specifies the number of epochs. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_epoch='epochs')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(name_epoch='epochs', ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_remove_not_finished ():\n",
    "    em = init_em ('remove_not_finished')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment: we simulate that a halt before finishing\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         other_parameters={'halt':True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    \n",
    "    # second experiment: remove unfinished\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.2})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    \n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.3},\n",
    "                                                         other_parameters={'remove_not_finished':True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2801524373.py, line number: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_remove_not_finished\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate\n",
       "0     0.1  0.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2801524373.py, line number: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.2\n",
      "epoch 1: accuracy: 1.4\n",
      "epoch 2: accuracy: 1.5999999999999999\n",
      "epoch 3: accuracy: 1.7999999999999998\n",
      "epoch 4: accuracy: 1.9999999999999998\n",
      "epoch 5: accuracy: 2.1999999999999997\n",
      "epoch 6: accuracy: 2.4\n",
      "epoch 7: accuracy: 2.6\n",
      "epoch 8: accuracy: 2.8000000000000003\n",
      "epoch 9: accuracy: 3.0000000000000004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>23:05:00.407541</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    NaN              NaN       NaN   \n",
       "1     1.0  0.20                    1.0              1.0  0.001805   \n",
       "\n",
       "              date 0_finished  \n",
       "0              NaN        NaN  \n",
       "1  23:05:00.407541       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2801524373.py, line number: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 1.3\n",
      "epoch 1: accuracy: 1.6\n",
      "epoch 2: accuracy: 1.9000000000000001\n",
      "epoch 3: accuracy: 2.2\n",
      "epoch 4: accuracy: 2.5\n",
      "epoch 5: accuracy: 2.8\n",
      "epoch 6: accuracy: 3.0999999999999996\n",
      "epoch 7: accuracy: 3.3999999999999995\n",
      "epoch 8: accuracy: 3.6999999999999993\n",
      "epoch 9: accuracy: 3.999999999999999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>23:05:00.407541</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>23:05:00.448265</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    NaN              NaN       NaN   \n",
       "1     1.0  0.20                    1.0              1.0  0.001805   \n",
       "2     1.0  0.30                    1.0              1.0  0.001835   \n",
       "\n",
       "              date 0_finished  \n",
       "0              NaN        NaN  \n",
       "1  23:05:00.407541       True  \n",
       "2  23:05:00.448265       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_remove_not_finished, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_repeat_experiment ():\n",
    "    em = init_em ('repeat_experiment')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    date = df.date.values[0]\n",
    "    \n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         other_parameters={'repeat_experiment': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert df.date.values[0] != date\n",
    "    \n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/1962307353.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_repeat_experiment\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>23:05:00.564363</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001832   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:05:00.564363       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/1962307353.py, line number: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>23:05:00.600932</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     0.1  0.05                    0.6              0.5  0.001832   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:05:00.600932       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_repeat_experiment, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the parameter that specifies the number of epochs. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_epoch='epochs')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(name_epoch='epochs', ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_check_finished ():\n",
    "    em = init_em ('check_finished')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment: we simulate that we only run for half the number of epochs\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},\n",
    "                                                         other_parameters={'actual_epochs': 5})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    date = df.date.values[0]\n",
    "    score = df['0_validation_accuracy'].values[0]\n",
    "    \n",
    "    # second experiment: same values in parameters dictionary, without other_parameters \n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    \n",
    "    assert (date==df.date.values[0]) and (score==df['0_validation_accuracy'].values[0])\n",
    "    \n",
    "    # third experiment: same values in parameters dictionary, with other_parameters indicating check_finished\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},\n",
    "                                                         other_parameters={'check_finished':True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==1 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    assert (df.index==[0]).all()\n",
    "    assert (date!=df.date.values[0]) and (score!=df['0_validation_accuracy'].values[0])\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2482338563.py, line number: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_check_finished\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2482338563.py, line number: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_check_finished, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recompute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_recompute_metrics ():\n",
    "    em = init_em ('recompute_metrics')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "    \n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    \n",
    "    df = em.get_experiment_data ()    \n",
    "    # second experiment: new values \n",
    "    em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02},\n",
    "                                                         other_parameters={'recompute_metrics': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()    \n",
    "    assert np.isnan(df['0_validation_accuracy'].values[1])\n",
    "    \n",
    "    # third experiment: new values \n",
    "    em.raise_error_if_run = False\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02, 'epochs': 10},\n",
    "                                                         other_parameters={'recompute_metrics':True,\n",
    "                                                                           'force_recompute_metrics': True})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape[0]==2 and (df.columns==['offset','rate','0_validation_accuracy','0_test_accuracy',\n",
    "                                           'time_0', 'date', '0_finished']).all()\n",
    "    assert (df.index==[0,1]).all()\n",
    "    assert df['0_validation_accuracy'].values[1]==0.3\n",
    "    \n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2340212191.py, line number: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_recompute_metrics\n",
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2340212191.py, line number: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.12000000000000001\n",
      "epoch 1: accuracy: 0.14\n",
      "epoch 2: accuracy: 0.16\n",
      "epoch 3: accuracy: 0.18\n",
      "epoch 4: accuracy: 0.19999999999999998\n",
      "epoch 5: accuracy: 0.21999999999999997\n",
      "epoch 6: accuracy: 0.23999999999999996\n",
      "epoch 7: accuracy: 0.25999999999999995\n",
      "epoch 8: accuracy: 0.27999999999999997\n",
      "epoch 9: accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_recompute_metrics, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prev_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate: \n",
    "1. The name of the parameter that specifies the number of epochs. \n",
    "1. The name of the file where the model is stored.\n",
    "This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_epoch='epochs', model_file_name='model_weights.pk')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(name_epoch='epochs', model_file_name='model_weights.pk')\n",
    "```\n",
    "\n",
    "Furthermore, in order to work, we need our experiment manager to make use of the parameter `resume` or the parameter `prev_path_results`. In particular, we need it to load the model file whose path is indicated in `parameters['resume']`, or whose path is indicated in `f'{parameters[\"prev_path_results\"]}/{self.model_file_name}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_prev_epoch ():\n",
    "    em = init_em ('prev_epoch')\n",
    "\n",
    "    # get reference result\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17})\n",
    "    reference_accuracy = em.model.accuracy\n",
    "    reference_weight = em.model.weight\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    em.remove_previous_experiments()\n",
    "\n",
    "    # first 3 experiments\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 20})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 15})\n",
    "\n",
    "    # more epochs\n",
    "    # in order to work, we need our experiment manager to make use of the \n",
    "    # parameter 'resume' or the parameter 'prev_path_results'. \n",
    "    # In particular, we need it to load the model file\n",
    "    # whose path is indicated in parameters['resume'], or whose path is \n",
    "    # indicated in f'{parameters[\"prev_path_results\"]}/{self.model_file_name}'\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},\n",
    "                                      other_parameters={'prev_epoch': True})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    assert em.model.epochs==2 and em.model.current_epoch==17\n",
    "\n",
    "    assert reference_accuracy==em.model.accuracy and reference_weight==em.model.weight\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},\n",
    "                                      other_parameters={'repeat_experiment': True})\n",
    "\n",
    "    assert em.model.epochs==17 and em.model.current_epoch==17\n",
    "\n",
    "    assert reference_accuracy==em.model.accuracy and reference_weight==em.model.weight\n",
    "\n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/1621658181.py, line number: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_prev_epoch\n",
      "fitting model with 17 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "epoch 10: accuracy: 0.65\n",
      "epoch 11: accuracy: 0.7000000000000001\n",
      "epoch 12: accuracy: 0.7500000000000001\n",
      "epoch 13: accuracy: 0.8000000000000002\n",
      "epoch 14: accuracy: 0.8500000000000002\n",
      "epoch 15: accuracy: 0.9000000000000002\n",
      "epoch 16: accuracy: 0.9500000000000003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>23:05:01.061801</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0    17.0     0.1  0.05                   0.95             0.85  0.002792   \n",
       "\n",
       "              date 0_finished  \n",
       "0  23:05:01.061801       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/1621658181.py, line number: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 10 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/1621658181.py, line number: 15\n",
      "script: /tmp/ipykernel_130431/1621658181.py, line number: 16\n",
      "script: /tmp/ipykernel_130431/1621658181.py, line number: 25\n",
      "script: /tmp/ipykernel_130431/1621658181.py, line number: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 20 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "epoch 10: accuracy: 0.65\n",
      "epoch 11: accuracy: 0.7000000000000001\n",
      "epoch 12: accuracy: 0.7500000000000001\n",
      "epoch 13: accuracy: 0.8000000000000002\n",
      "epoch 14: accuracy: 0.8500000000000002\n",
      "epoch 15: accuracy: 0.9000000000000002\n",
      "epoch 16: accuracy: 0.9500000000000003\n",
      "epoch 17: accuracy: 1.0000000000000002\n",
      "epoch 18: accuracy: 1.0500000000000003\n",
      "epoch 19: accuracy: 1.1000000000000003\n",
      "fitting model with 15 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "epoch 10: accuracy: 0.65\n",
      "epoch 11: accuracy: 0.7000000000000001\n",
      "epoch 12: accuracy: 0.7500000000000001\n",
      "epoch 13: accuracy: 0.8000000000000002\n",
      "epoch 14: accuracy: 0.8500000000000002\n",
      "reading model from test_prev_epoch/experiments/00002/0/model_weights.pk\n",
      "fitting model with 2 epochs\n",
      "epoch 0: accuracy: 0.9000000000000002\n",
      "epoch 1: accuracy: 0.9500000000000003\n",
      "fitting model with 17 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "epoch 5: accuracy: 0.39999999999999997\n",
      "epoch 6: accuracy: 0.44999999999999996\n",
      "epoch 7: accuracy: 0.49999999999999994\n",
      "epoch 8: accuracy: 0.5499999999999999\n",
      "epoch 9: accuracy: 0.6\n",
      "epoch 10: accuracy: 0.65\n",
      "epoch 11: accuracy: 0.7000000000000001\n",
      "epoch 12: accuracy: 0.7500000000000001\n",
      "epoch 13: accuracy: 0.8000000000000002\n",
      "epoch 14: accuracy: 0.8500000000000002\n",
      "epoch 15: accuracy: 0.9000000000000002\n",
      "epoch 16: accuracy: 0.9500000000000003\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_prev_epoch, tag='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_prev_epoch2 ():\n",
    "    em = init_em ('prev_epoch2')\n",
    "    \n",
    "    em.remove_previous_experiments()\n",
    "    score, results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'actual_epochs': 2})\n",
    "    \n",
    "    \n",
    "    assert score is None and results['validation_accuracy']==0.16\n",
    "    assert em.model.current_epoch==2 and em.model.epochs==2\n",
    "    \n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "\n",
    "        \n",
    "    # We use last result and have the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'prev_epoch': True, 'check_finished': True, 'use_previous_best': False}\n",
    "    )\n",
    "\n",
    "    assert score is None and results['validation_accuracy']==0.25\n",
    "    assert em.model.current_epoch==5 and em.model.epochs==3\n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df['0_validation_accuracy']==[0.25, 0.30]).all()\n",
    "    \n",
    "\n",
    "    em.remove_previous_experiments()\n",
    "\n",
    "    # **********************************\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'actual_epochs': 2, 'halt': True})\n",
    "    \n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "        \n",
    "    # We use last result and have the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'prev_epoch': True, 'check_finished_if_interrupted': True, \n",
    "                          'use_previous_best': False}\n",
    "    )\n",
    "    assert score is None and results['validation_accuracy']==0.25\n",
    "    assert em.model.current_epoch==5 and em.model.epochs==3\n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df['0_validation_accuracy']==[0.25, 0.30]).all()\n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2837899774.py, line number: 7\n",
      "script: /tmp/ipykernel_130431/2837899774.py, line number: 13\n",
      "script: /tmp/ipykernel_130431/2837899774.py, line number: 20\n",
      "script: /tmp/ipykernel_130431/2837899774.py, line number: 34\n",
      "script: /tmp/ipykernel_130431/2837899774.py, line number: 36\n",
      "script: /tmp/ipykernel_130431/2837899774.py, line number: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_prev_epoch2\n",
      "fitting model with 2 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.14\n",
      "epoch 1: accuracy: 0.18000000000000002\n",
      "epoch 2: accuracy: 0.22000000000000003\n",
      "epoch 3: accuracy: 0.26\n",
      "epoch 4: accuracy: 0.3\n",
      "reading model from test_prev_epoch2/experiments/00000/0/model_weights.pk\n",
      "fitting model with 3 epochs\n",
      "epoch 0: accuracy: 0.19\n",
      "epoch 1: accuracy: 0.22\n",
      "epoch 2: accuracy: 0.25\n",
      "fitting model with 2 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.14\n",
      "epoch 1: accuracy: 0.18000000000000002\n",
      "epoch 2: accuracy: 0.22000000000000003\n",
      "epoch 3: accuracy: 0.26\n",
      "epoch 4: accuracy: 0.3\n",
      "reading model from test_prev_epoch2/experiments/00000/0/model_weights.pk\n",
      "fitting model with 3 epochs\n",
      "epoch 0: accuracy: 0.19\n",
      "epoch 1: accuracy: 0.22\n",
      "epoch 2: accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_prev_epoch2, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work, we need our experiment manager to make use of the parameter `prev_path_results`. In particular, we need it to load the model file whose path is indicated in `f'{parameters[\"prev_path_results\"]}/{self.model_file_name}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_from_exp ():\n",
    "    em = init_em ('from_exp')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "\n",
    "    # get reference result\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5})\n",
    "    reference_accuracy = em.model.accuracy\n",
    "    reference_weight = em.model.weight\n",
    "    em.remove_previous_experiments()\n",
    "\n",
    "    # first 3 experiments\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 2})\n",
    "    \n",
    "    # the following resumes from experiment 0, and trains the model for 5 more epochs \n",
    "    # using now different `offset` and `rate` hyper-parameters\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},\n",
    "                                      other_parameters={'from_exp': 0})\n",
    "\n",
    "    assert em.model.epochs==5 and em.model.current_epoch==7\n",
    "    correct_accuracy = (0.1 + 0.03*2 # accuracy of model from experiment 0\n",
    "                        + 0.05*5)     # accuracy gained by training for 5 more epochs using \n",
    "                                    #  new hyper-parameters: rate=0.05 \n",
    "    assert (em.model.accuracy-correct_accuracy) < 1e-10\n",
    "    assert reference_accuracy!=em.model.accuracy\n",
    "\n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3213666507.py, line number: 7\n",
      "script: /tmp/ipykernel_130431/3213666507.py, line number: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_from_exp\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.15000000000000002\n",
      "epoch 1: accuracy: 0.2\n",
      "epoch 2: accuracy: 0.25\n",
      "epoch 3: accuracy: 0.3\n",
      "epoch 4: accuracy: 0.35\n",
      "fitting model with 2 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/3213666507.py, line number: 14\n",
      "script: /tmp/ipykernel_130431/3213666507.py, line number: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 2 epochs\n",
      "epoch 0: accuracy: 0.14\n",
      "epoch 1: accuracy: 0.18000000000000002\n",
      "reading model from test_from_exp/experiments/00000/0/model_weights.pk\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.21000000000000002\n",
      "epoch 1: accuracy: 0.26\n",
      "epoch 2: accuracy: 0.31\n",
      "epoch 3: accuracy: 0.36\n",
      "epoch 4: accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_from_exp, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### skip_interrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the file where the model is stored.\n",
    "This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (model_file_name='model_weights.pk')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(model_file_name='model_weights.pk')\n",
    "```\n",
    "\n",
    "Alternatively, we can indicate the name of the file where the model history exists. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_model_history='history.pk')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(model_file_name='history.pk')\n",
    "```\n",
    "If not indicated, the experiment manager tries to find the model history in a file named `model_history.pk`. In order to consider the history good enough, the experiment manager checks if the length of the arrays stored in the model_history dictionary is at least `parameters.get('min_iterations', dflt.min_iterations)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_skip_interrupted ():\n",
    "    em = init_em ('skip_interrupted')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "    \n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "    \n",
    "    em.raise_error_if_run = True\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'skip_interrupted': True}\n",
    "    )\n",
    "    assert score==None and len(results)==0\n",
    "    \n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'skip_interrupted': True,\n",
    "                          'model_file_name': 'wrong_file.pk',\n",
    "                          'min_iterations':1}\n",
    "    )\n",
    "    assert score==None and len(results)==0\n",
    "    \n",
    "    with pytest.raises (RuntimeError):\n",
    "        score, results = em.create_experiment_and_run (\n",
    "            parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "            other_parameters={'skip_interrupted': True,\n",
    "                              'model_file_name': 'wrong_file.pk'}\n",
    "        )\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "\n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_skip_interrupted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/4036359447.py, line number: 9\n",
      "script: /tmp/ipykernel_130431/4036359447.py, line number: 11\n",
      "script: /tmp/ipykernel_130431/4036359447.py, line number: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n",
      "epoch 2: accuracy: 0.19\n",
      "epoch 3: accuracy: 0.22\n",
      "epoch 4: accuracy: 0.25\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.14\n",
      "epoch 1: accuracy: 0.18000000000000002\n",
      "epoch 2: accuracy: 0.22000000000000003\n",
      "epoch 3: accuracy: 0.26\n",
      "epoch 4: accuracy: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/4036359447.py, line number: 24\n",
      "script: /tmp/ipykernel_130431/4036359447.py, line number: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing result from test_skip_interrupted/experiments/00000/0 with iterations 5\n",
      "not storing result from test_skip_interrupted/experiments/00000/0 with iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>23:05:09.113386</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     5.0     0.1  0.03                    NaN              NaN       NaN   \n",
       "1     5.0     0.1  0.04                    0.3              0.4  0.001057   \n",
       "\n",
       "              date 0_finished  \n",
       "0              NaN        NaN  \n",
       "1  23:05:09.113386       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_skip_interrupted, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### use_last_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the file where the model history exists. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_model_history='history.pk')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(model_file_name='history.pk')\n",
    "```\n",
    "If not indicated, the experiment manager tries to find the model history in a file named `model_history.pk`. In order to consider the history good enough, the experiment manager checks if the length of the arrays stored in the model_history dictionary is at least `parameters.get('min_iterations', dflt.min_iterations)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_use_last_result ():\n",
    "    em = init_em ('use_last_result')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "    \n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df.isna()['0_validation_accuracy'] == [True, False]).all()\n",
    "    \n",
    "    # We use last result but require that number of epochs is at least 50.\n",
    "    # Since this is not true, the last result is not used.\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'use_last_result': True}\n",
    "    )\n",
    "    assert score==None and results=={}\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert (df.isna()['0_validation_accuracy'] == [True, False]).all()\n",
    "    \n",
    "    # We use last result and lower the required number of epochs to 2\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'use_last_result': True, 'min_iterations': 2}\n",
    "    )\n",
    "    print (score, results)\n",
    "    assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert (df.isna()['0_validation_accuracy'] == [False, False]).all()\n",
    "    assert (df['0_validation_accuracy'] == [0.25, 0.30]).all()\n",
    "    \n",
    "    # We use last result and increase the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'use_last_result': True, 'run_if_not_interrumpted': True}\n",
    "    )\n",
    "    print (score, results)\n",
    "    #assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    #assert (df.isna()['0_validation_accuracy'] == [False, False]).all()\n",
    "    #assert (df['0_validation_accuracy'] == [0.25, 0.30]).all()\n",
    "\n",
    "    em.remove_previous_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_use_last_result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/906478241.py, line number: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n",
      "epoch 2: accuracy: 0.19\n",
      "epoch 3: accuracy: 0.22\n",
      "epoch 4: accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/906478241.py, line number: 11\n",
      "script: /tmp/ipykernel_130431/906478241.py, line number: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.14\n",
      "epoch 1: accuracy: 0.18000000000000002\n",
      "epoch 2: accuracy: 0.22000000000000003\n",
      "epoch 3: accuracy: 0.26\n",
      "epoch 4: accuracy: 0.3\n",
      "not storing result from test_use_last_result/experiments/00000/0 with iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>23:05:09.297575</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     5.0     0.1  0.03                    NaN              NaN       NaN   \n",
       "1     5.0     0.1  0.04                    0.3              0.4  0.001131   \n",
       "\n",
       "              date 0_finished  \n",
       "0              NaN        NaN  \n",
       "1  23:05:09.297575       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/906478241.py, line number: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing result from test_use_last_result/experiments/00000/0 with iterations 5\n",
      "None {'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>0_accuracy</th>\n",
       "      <th>0_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23:05:09.360072</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>23:05:09.297575</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     5.0     0.1  0.03                   0.25             0.35       NaN   \n",
       "1     5.0     0.1  0.04                   0.30             0.40  0.001131   \n",
       "\n",
       "              date 0_finished  0_accuracy  0_last  \n",
       "0  23:05:09.360072      False        0.25     5.0  \n",
       "1  23:05:09.297575       True         NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 {'validation_accuracy': 0.25}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>offset</th>\n",
       "      <th>rate</th>\n",
       "      <th>0_validation_accuracy</th>\n",
       "      <th>0_test_accuracy</th>\n",
       "      <th>time_0</th>\n",
       "      <th>date</th>\n",
       "      <th>0_finished</th>\n",
       "      <th>0_accuracy</th>\n",
       "      <th>0_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23:05:09.360072</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>23:05:09.297575</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs  offset  rate  0_validation_accuracy  0_test_accuracy    time_0  \\\n",
       "0     5.0     0.1  0.03                   0.25             0.35       NaN   \n",
       "1     5.0     0.1  0.04                   0.30             0.40  0.001131   \n",
       "\n",
       "              date 0_finished  0_accuracy  0_last  \n",
       "0  23:05:09.360072      False        0.25     5.0  \n",
       "1  23:05:09.297575       True         NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst.run (test_use_last_result, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_use_last_result_run_interrupted ():\n",
    "    em = init_em ('use_last_result_run_interrupted')\n",
    "    path_experiments = em.get_path_experiments()\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'actual_epochs': 2, 'halt': True})\n",
    "    \n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    #display (df)\n",
    "    assert (df.isna()['0_validation_accuracy'] == [True, False]).all()\n",
    "        \n",
    "    # We use last result and have the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        other_parameters={'use_last_result': True, 'run_if_not_interrumpted': True}\n",
    "    )\n",
    "    print (score, results)\n",
    "    #assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n",
    "    df = em.get_experiment_data ()\n",
    "    #display(df)\n",
    "    assert em.model.current_epoch==5 and em.model.epochs==5\n",
    "    assert (df.isna()['0_validation_accuracy'] == [False, False]).all()\n",
    "    assert (df['0_validation_accuracy'] == [0.25, 0.30]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_use_last_result_run_interrupted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2429739771.py, line number: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 2 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "script: /tmp/ipykernel_130431/2429739771.py, line number: 11\n",
      "script: /tmp/ipykernel_130431/2429739771.py, line number: 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.14\n",
      "epoch 1: accuracy: 0.18000000000000002\n",
      "epoch 2: accuracy: 0.22000000000000003\n",
      "epoch 3: accuracy: 0.26\n",
      "epoch 4: accuracy: 0.3\n",
      "not storing result from test_use_last_result_run_interrupted/experiments/00000/0 with iterations 2\n",
      "fitting model with 5 epochs\n",
      "epoch 0: accuracy: 0.13\n",
      "epoch 1: accuracy: 0.16\n",
      "epoch 2: accuracy: 0.19\n",
      "epoch 3: accuracy: 0.22\n",
      "epoch 4: accuracy: 0.25\n",
      "None {'validation_accuracy': 0.25, 'test_accuracy': 0.35}\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_use_last_result_run_interrupted, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_git_revision_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_git_revision_hash (root_path=None):\n",
    "    try:\n",
    "        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "        git_hash = str(git_hash)\n",
    "        json.dump(git_hash, open('%s/git_hash.json' %root_path, 'wt'))\n",
    "    except:\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "        if root_path is not None:\n",
    "            logger.info ('could not get git hash, retrieving it from disk...')\n",
    "            git_hash = json.load(open('%s/git_hash.json' %root_path, 'rt'))\n",
    "        else:\n",
    "            logger.info ('could not get git hash, using empty string...')\n",
    "            git_hash = ''\n",
    "\n",
    "    return str(git_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## record_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def record_parameters (path_save, parameters, other_parameters=None):\n",
    "    with open('%s/parameters.txt' %path_save, 'wt') as f:\n",
    "        f.write('%s\\n' %mypprint(parameters, dict_name='parameters'))\n",
    "        if other_parameters is not None:\n",
    "            f.write('\\n\\n%s\\n' %mypprint(other_parameters, dict_name='other_parameters'))\n",
    "    if other_parameters is not None:\n",
    "        pickle.dump ([parameters,other_parameters],open('%s/parameters.pk' %path_save, 'wb'))\n",
    "    else:\n",
    "        pickle.dump (parameters,open('%s/parameters.pk' %path_save, 'wb'))\n",
    "    try:\n",
    "        json.dump(parameters,open('%s/parameters.json' %path_save, 'wt'))\n",
    "    except:\n",
    "        pass\n",
    "    if other_parameters is not None:\n",
    "        try:\n",
    "            json.dump(parameters,open('%s/other_parameters.json' %path_save, 'wt'))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mypprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mypprint(parameters, dict_name=None):\n",
    "    if dict_name is not None:\n",
    "        text = '%s=dict(' %dict_name\n",
    "        tpad = ' ' * len(text)\n",
    "    else:\n",
    "        text = '\\t'\n",
    "        tpad = '\\t'\n",
    "    for idx, (key, value) in enumerate(sorted(parameters.items(), key=lambda x: x[0])):\n",
    "        if type(value) is str:\n",
    "            value = '%s%s%s' %(\"'\",value,\"'\")\n",
    "        text += '{}={}'.format(key, value)\n",
    "        if idx < (len(parameters)-1):\n",
    "            text += ',\\n{}'.format(tpad)\n",
    "\n",
    "    if dict_name is not None:\n",
    "        text += ')\\n'\n",
    "    else:\n",
    "        text += '\\n'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mymakedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mymakedirs (path, exist_ok=False):\n",
    "    '''work around for python 2.7'''\n",
    "    if exist_ok:\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_or_create_experiment_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_or_create_experiment_values (path_csv, parameters, precision=1e-15):\n",
    "\n",
    "    logger = logging.getLogger(\"experiment_manager\")\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    experiment_numbers = []\n",
    "    changed_dataframe = False\n",
    "\n",
    "    if os.path.exists (path_pickle) or os.path.exists (path_csv):\n",
    "        if os.path.exists (path_pickle):\n",
    "            experiment_data = pd.read_pickle (path_pickle)\n",
    "        else:\n",
    "            experiment_data = pd.read_csv (path_csv, index_col=0)\n",
    "            experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "        experiment_data, removed_defaults = remove_defaults_from_experiment_data (experiment_data)\n",
    "\n",
    "        # Finds rows that match parameters. If the dataframe doesn't have any parameter with that name, a new column is created and changed_dataframe is set to True\n",
    "        experiment_numbers, changed_dataframe, _ = experiment_utils.find_rows_with_parameters_dict (\n",
    "            experiment_data, parameters, precision = precision\n",
    "        )\n",
    "\n",
    "        changed_dataframe = changed_dataframe or removed_defaults\n",
    "\n",
    "        if len(experiment_numbers) > 1:\n",
    "            logger.info ('more than one matching experiment: ', experiment_numbers)\n",
    "    else:\n",
    "        experiment_data = pd.DataFrame()\n",
    "\n",
    "    if len(experiment_numbers) == 0:\n",
    "        experiment_data = experiment_data.append (parameters, ignore_index=True)\n",
    "        changed_dataframe = True\n",
    "        experiment_number = experiment_data.shape[0]-1\n",
    "    else:\n",
    "        experiment_number = experiment_numbers[0]\n",
    "\n",
    "    if changed_dataframe:\n",
    "        experiment_data.to_csv(path_csv)\n",
    "        experiment_data.to_pickle(path_pickle)\n",
    "\n",
    "    return experiment_number, experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_parameters (root_path, experiment_number, parameters):\n",
    "    \"\"\" Keeps track of dictionary to map experiment number and parameters values for the different experiments.\"\"\"\n",
    "    path_hp_dictionary = '%s/parameters.pk' %root_path\n",
    "    if os.path.exists(path_hp_dictionary):\n",
    "        all_parameters = pickle.load (open(path_hp_dictionary,'rb'))\n",
    "    else:\n",
    "        all_parameters = {}\n",
    "    if experiment_number not in all_parameters.keys():\n",
    "        str_par = '\\n\\nExperiment %d => parameters: \\n%s\\n' %(experiment_number,mypprint(parameters))\n",
    "        f = open('%s/parameters.txt' %root_path, 'at')\n",
    "        f.write(str_par)\n",
    "        f.close()\n",
    "        all_parameters[experiment_number] = parameters\n",
    "        pickle.dump (all_parameters, open(path_hp_dictionary,'wb'))\n",
    "\n",
    "    # pickle number of current experiment, for visualization\n",
    "    pickle.dump(experiment_number, open('%s/current_experiment_number.pkl' %root_path,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isnull (experiment_data, experiment_number, name_column):\n",
    "    return (name_column not in experiment_data.columns) or (experiment_data.loc[experiment_number, name_column] is None) or np.isnan(experiment_data.loc[experiment_number, name_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_number (root_path, parameters = {}):\n",
    "\n",
    "    path_csv = '%s/experiments_data.csv' %root_path\n",
    "    path_pickle = path_csv.replace('csv', 'pk')\n",
    "    experiment_number, _ = load_or_create_experiment_values (path_csv, parameters)\n",
    "\n",
    "    return experiment_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_numbers (path_results_base, parameters_single_value, parameters_multiple_values_all):\n",
    "\n",
    "    experiment_numbers = []\n",
    "\n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))\n",
    "\n",
    "    for (i_hp, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_single_value)\n",
    "        parameters = remove_defaults (parameters)\n",
    "\n",
    "        experiment_number = get_experiment_number (path_results_base, parameters=parameters)\n",
    "        experiment_numbers.append(experiment_number)\n",
    "\n",
    "    return experiment_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_logger (name, path_results, stdout=True, mode='a', just_message = False, filename='logs.txt'):\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    for hdlr in logger.handlers[:]:  # remove all old handlers\n",
    "        logger.removeHandler(hdlr)\n",
    "\n",
    "    #if not logger.hasHandlers():\n",
    "\n",
    "    # Create handlers\n",
    "    if stdout:\n",
    "        c_handler = logging.StreamHandler()\n",
    "        c_handler.setLevel(logging.DEBUG)\n",
    "        c_format = logging.Formatter('%(message)s')\n",
    "        c_handler.setFormatter(c_format)\n",
    "        logger.addHandler(c_handler)\n",
    "\n",
    "    f_handler = logging.FileHandler('%s/%s' %(path_results, filename), mode = mode)\n",
    "    f_handler.setLevel(logging.DEBUG)\n",
    "    if just_message:\n",
    "        f_format = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    else:\n",
    "        f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    f_handler.setFormatter(f_format)\n",
    "    logger.addHandler(f_handler)\n",
    "    logger.propagate = 0\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert_experiment_script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def insert_experiment_script_path (other_parameters, logger):\n",
    "    if other_parameters.get('script_path') is None:\n",
    "        stack_level = other_parameters.get('stack_level', -3)\n",
    "        stack = traceback.extract_stack()[stack_level]\n",
    "        other_parameters['script_path'] = stack.filename\n",
    "        other_parameters['lineno'] = stack.lineno\n",
    "        logger.info ('experiment script: {}, line: {}'.format(stack.filename, stack.lineno))\n",
    "        if 'stack_level' in other_parameters:\n",
    "            del other_parameters['stack_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_parameters (experiment=None, root_path=None, root_folder = None, other_parameters={}, parameters = {}, check_experiment_matches=True):\n",
    "\n",
    "    from hpsearch.config.hpconfig import get_path_experiments, get_path_experiment\n",
    "    if root_folder is not None:\n",
    "        other_parameters['root_folder'] = root_folder\n",
    "\n",
    "    if root_path is None:\n",
    "        root_path = get_path_experiments(folder  = other_parameters.get('root_folder'))\n",
    "\n",
    "    path_root_experiment = get_path_experiment (experiment, root_path=root_path)\n",
    "\n",
    "    logger = set_logger (\"experiment_manager\", root_path)\n",
    "\n",
    "    if os.path.exists('%s/parameters.pk' %path_root_experiment):\n",
    "        parameters2, other_parameters2=pickle.load(open('%s/parameters.pk' %path_root_experiment,'rb'))\n",
    "\n",
    "        other_parameters2.update(other_parameters)\n",
    "        other_parameters = other_parameters2\n",
    "\n",
    "        # if we don't add or modify parameters, we require that the old experiment number matches the new one\n",
    "        if (len(parameters) == 0) and check_experiment_matches:\n",
    "            logger.info ('requiring experiment number to be {}'.format(experiment))\n",
    "            other_parameters['experiment_number'] = experiment\n",
    "        elif 'experiment_number' in other_parameters:\n",
    "            del other_parameters['experiment_number']\n",
    "\n",
    "        parameters2.update(parameters)\n",
    "        parameters = parameters2\n",
    "    else:\n",
    "        raise FileNotFoundError ('file {} not found'.format ('%s/parameters.pk' %path_root_experiment))\n",
    "\n",
    "    return parameters, other_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_other_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_other_parameters (experiment_number, other_parameters, root_path):\n",
    "    parameters_to_save = {}\n",
    "    for k in other_parameters.keys():\n",
    "        if type(other_parameters[k]) is str:\n",
    "            parameters_to_save[k] = other_parameters[k]\n",
    "        elif np.isscalar(other_parameters[k]) and np.isreal(other_parameters[k]):\n",
    "            parameters_to_save[k] = other_parameters[k]\n",
    "\n",
    "    path_csv = '%s/other_parameters.csv' %root_path\n",
    "    df = pd.DataFrame (index = [experiment_number], data=parameters_to_save)\n",
    "\n",
    "    if os.path.exists (path_csv):\n",
    "        df_all = pd.read_csv (path_csv, index_col=0)\n",
    "        df_all = pd.concat([df_all, df], sort=True)\n",
    "        df_all = df_all.loc[~df_all.index.duplicated(keep='last')]\n",
    "    else:\n",
    "        df_all = df\n",
    "    df_all.to_csv (path_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena_old)",
   "language": "python",
   "name": "athena_old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
