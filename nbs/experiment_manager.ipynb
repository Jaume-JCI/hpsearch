{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp experiment_manager\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "#tst = TestRunner (targets=['dummy'])\n",
    "tst = TestRunner (targets=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Manager\n",
    "\n",
    "> Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (experiment_manager.py, line 820)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/anaconda/envs/hpsearch/lib/python3.9/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3457\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipykernel_59613/613363490.py\"\u001b[0m, line \u001b[1;32m24\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from hpsearch.config.manager_factory import ManagerFactory\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jcidatascience/jaume/workspace/remote/temp/hpsearch/hpsearch/__init__.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from hpsearch.experiment_manager import ExperimentManager\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jcidatascience/jaume/workspace/remote/temp/hpsearch/hpsearch/experiment_manager.py\"\u001b[0;36m, line \u001b[0;32m820\u001b[0m\n\u001b[0;31m    query_args, info=Bunch(), n_iterations=1, **kwargs):\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# coding: utf-8\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.utils import Bunch\n",
    "import platform\n",
    "import pprint\n",
    "import subprocess\n",
    "from multiprocessing import Process\n",
    "import logging\n",
    "import traceback\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from dsblocks.utils.utils import set_logger, set_verbosity, store_attr, json_load, json_dump\n",
    "\n",
    "# hpsearch core API\n",
    "from hpsearch.config.manager_factory import ManagerFactory\n",
    "from hpsearch.utils import experiment_utils\n",
    "from hpsearch.utils.experiment_utils import (remove_defaults, read_df, write_df, \n",
    "                                             write_binary_df_if_not_exists)\n",
    "from hpsearch.utils.convert_legacy import update_data_format\n",
    "from hpsearch.utils.organize_experiments import remove_defaults_from_experiment_data\n",
    "import hpsearch.config.hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import os\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from dsblocks.utils.nbdev_utils import md\n",
    "from dsblocks.utils.utils import check_last_part, remove_previous_results\n",
    "\n",
    "from hpsearch.examples.complex_dummy_experiment_manager import init_em\n",
    "import hpsearch.config.hp_defaults as dflt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperimentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dflt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59691/1723295557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mExperimentManager\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def __init__ (self,\n\u001b[1;32m      4\u001b[0m                   \u001b[0mallow_base_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdflt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_base_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mpath_experiments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hpsearch/results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_59691/1723295557.py\u001b[0m in \u001b[0;36mExperimentManager\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mExperimentManager\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__ (self,\n\u001b[0;32m----> 4\u001b[0;31m                   \u001b[0mallow_base_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdflt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_base_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                   \u001b[0mpath_experiments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hpsearch/results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dflt' is not defined"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class ExperimentManager (object):\n",
    "    def __init__ (self,\n",
    "                  allow_base_class=dflt.allow_base_class,\n",
    "                  path_experiments='hpsearch/results',\n",
    "                  folder=None,\n",
    "                  parent_path=None,\n",
    "                  alternative_path=None,\n",
    "                  alternative_parent_path=None,\n",
    "                  defaults=dflt.defaults,\n",
    "                  metric=dflt.metric,\n",
    "                  op=dflt.op,\n",
    "                  backend=dflt.backend,\n",
    "                  path_data=None,\n",
    "                  name_model_history=dflt.name_model_history,\n",
    "                  model_file_name=dflt.model_file_name,\n",
    "                  name_epoch=dflt.name_epoch,\n",
    "                  result_file=dflt.result_file,\n",
    "                  target_model_file=None,\n",
    "                  destination_model_file=None,\n",
    "                  manager_path=dflt.manager_path,\n",
    "                  non_pickable_fields=[],\n",
    "                  avoid_saving_fields=[],\n",
    "                  logger=None,\n",
    "                  verbose: int = dflt.verbose,\n",
    "                  name_logger:str = dflt.name_logger\n",
    "                 ):\n",
    "\n",
    "        # ********************\n",
    "        # store_attr ()\n",
    "        # ********************\n",
    "        self.allow_base_class = allow_base_class\n",
    "        self._path_experiments = path_experiments\n",
    "        self.defaults = defaults\n",
    "        self.key_score = metric\n",
    "        self.op = op\n",
    "        self.backend = backend\n",
    "        self._alternative_path = alternative_path\n",
    "        self.path_data = path_data\n",
    "        self.name_model_history = name_model_history\n",
    "        self.model_file_name = model_file_name\n",
    "        self.name_epoch = name_epoch\n",
    "        self.result_file = result_file\n",
    "        self.target_model_file = target_model_file\n",
    "        self.destination_model_file = destination_model_file\n",
    "        self.name_logger = name_logger\n",
    "        self.logger = logger\n",
    "        self.verbose = verbose\n",
    "        self.manager_path = manager_path\n",
    "        # ********************\n",
    "\n",
    "        class_name = self.__class__.__name__\n",
    "\n",
    "        self._path_experiments = Path (self._path_experiments).resolve ()\n",
    "        if folder is not None or parent_path is not None:\n",
    "            self.set_path_experiments (folder=folder, parent_path=parent_path)\n",
    "        self._alternative_path = Path(self._alternative_path) if self._alternative_path is not None else None\n",
    "        if alternative_path is None and alternative_parent_path is not None:\n",
    "            self.set_alternative_path (alternative_parent_path=alternative_parent_path)\n",
    "        self.path_data = Path(self.path_data) if self.path_data is not None else None\n",
    "\n",
    "        if self.logger is None:\n",
    "            self.logger = set_logger (self.name_logger, path_results=self.path_experiments, verbose=self.verbose)\n",
    "\n",
    "        self.key_score = metric\n",
    "        self.registered_name = f'{class_name}-{self.folder}'\n",
    "\n",
    "        self.parameters_non_pickable = {}\n",
    "        self.manager_factory = ManagerFactory(allow_base_class=allow_base_class, manager_path=self.manager_path,\n",
    "                                              logger=self.logger)\n",
    "        self.manager_factory.register_manager (self)\n",
    "        non_pickable_fields = (non_pickable_fields if isinstance(non_pickable_fields, list)\n",
    "                               else [non_pickable_fields])\n",
    "        avoid_saving_fields = (avoid_saving_fields if isinstance(avoid_saving_fields, list)\n",
    "                               else [avoid_saving_fields])\n",
    "        self.non_pickable_fields = (non_pickable_fields + avoid_saving_fields +\n",
    "                                    ['manager_factory', 'parameters_non_pickable', 'logger'])\n",
    "        self.avoid_saving_fields = avoid_saving_fields\n",
    "\n",
    "    @property\n",
    "    def folder (self):\n",
    "        return self._path_experiments.name\n",
    "\n",
    "    @property\n",
    "    def parent_path (self):\n",
    "        return self._path_experiments.parent\n",
    "\n",
    "    @property\n",
    "    def path_experiments (self):\n",
    "        return self._path_experiments\n",
    "    \n",
    "    @property\n",
    "    def alternative_parent_path (self):\n",
    "        return self._alternative_path.parent\n",
    "\n",
    "    @property\n",
    "    def alternative_path (self):\n",
    "        return self._alternative_path\n",
    "\n",
    "    def set_path_experiments (self, path_experiments=None, folder=None, parent_path=None):\n",
    "        if path_experiments is not None: self._path_experiments = Path(path_experiments).resolve()\n",
    "        else:\n",
    "            parent_path = Path(parent_path).resolve() if parent_path is not None else self.parent_path\n",
    "            folder = folder if folder is not None else self.folder\n",
    "            self._path_experiments = parent_path/folder\n",
    "\n",
    "    def set_alternative_path (self, alternative_path=None, alternative_parent_path=None):\n",
    "        if alternative_path is not None: self._alternative_path = Path(alternative_path).resolve()\n",
    "        else:\n",
    "            alternative_parent_path = (Path(alternative_parent_path).resolve() \n",
    "                                       if alternative_parent_path is not None else \n",
    "                                       self.alternative_parent_path)\n",
    "            folder = self.folder\n",
    "            self._alternative_path = alternative_parent_path/folder\n",
    "            \n",
    "    def set_verbose (self, verbose):\n",
    "        self.verbose = verbose\n",
    "        set_verbosity (logger=self.logger, verbose=verbose)\n",
    "\n",
    "    def get_default_parameters (self, parameters):\n",
    "        if not self.allow_base_class:\n",
    "            raise ImportError ('call get_default_parameters from base class is not allowed')\n",
    "        return self.defaults\n",
    "\n",
    "    def get_default_operations (self):\n",
    "        return {'folder': self.folder, 'op': self.op, 'metric': self.key_score}\n",
    "\n",
    "    def get_path_experiment (self, experiment_id):\n",
    "        path_experiment = self.path_experiments/f'experiments/{experiment_id:05d}'\n",
    "        return path_experiment\n",
    "\n",
    "    def get_path_results (self, experiment_id=None, run_number=0, path_experiment=None):\n",
    "        assert experiment_id is not None or path_experiment is not None\n",
    "        if path_experiment is None:\n",
    "            path_experiment = self.get_path_experiment (experiment_id)\n",
    "        path_results = path_experiment/f'{run_number}'\n",
    "        return path_results\n",
    "\n",
    "    def get_path_alternative (self, path_results):\n",
    "        if self.alternative_path is None:\n",
    "            return path_results\n",
    "        path_alternative = str(path_results).replace (str(self.path_experiments), str(self.alternative_path))\n",
    "\n",
    "        return path_alternative\n",
    "\n",
    "    def get_path_data (self, run_number, parameters={}):\n",
    "        if self.path_data is None:\n",
    "            return self.path_experiments/'data'\n",
    "        else:\n",
    "            return self.path_data\n",
    "\n",
    "    def get_experiment_data (self, experiments=None):\n",
    "        experiment_data = read_df (self.path_experiments)\n",
    "        if experiment_data is not None:\n",
    "            if experiment_data.columns.nlevels==1: experiment_data = update_data_format (experiment_data)\n",
    "            if experiments is not None:\n",
    "                experiment_data = experiment_data.loc[experiments,:]\n",
    "        return experiment_data\n",
    "\n",
    "    def remove_previous_experiments (self, parent=False, only_test=True, include_alternative=True, \n",
    "                                     use_alternative_path=False):\n",
    "        if use_alternative_path:\n",
    "            path_to_remove = self.alternative_path.parent if parent else self.alternative_path\n",
    "        else:\n",
    "            path_to_remove = self.path_experiments.parent if parent else self.path_experiments\n",
    "        if not str(path_to_remove.name).startswith ('test_') and only_test:\n",
    "            raise ValueError (f'path to remove does not start with test_: {path_to_remove}')\n",
    "        if path_to_remove.exists():\n",
    "            shutil.rmtree (path_to_remove)\n",
    "        if include_alternative and self.alternative_path is not None:\n",
    "            self.remove_previous_experiments (parent=parent, \n",
    "                                              only_test=only_test, \n",
    "                                              use_alternative_path=True,\n",
    "                                              include_alternative=False)\n",
    "\n",
    "    def experiment_visualization (self, **kwargs):\n",
    "        raise ValueError ('this type of experiment visualization is not recognized')\n",
    "\n",
    "    def run_experiment_pipeline (self, run_number=0, path_results='./results', parameters = {}, \n",
    "                                 use_process=False):\n",
    "        \"\"\" Runs complete learning pipeline: loading / generating data, building and learning model, applying it to data,\n",
    "        and evaluating it.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # record all parameters except for non-pickable ones\n",
    "        record_parameters (path_results, parameters)\n",
    "\n",
    "        # integrate non-pickable parameters into global dictionary\n",
    "        parameters.update (self.parameters_non_pickable)\n",
    "        self.parameters_non_pickable = {}\n",
    "\n",
    "        # #####################################\n",
    "        # Evaluation\n",
    "        # #####################################\n",
    "        time_before = time.time()\n",
    "        score_dict = self._run_experiment (parameters=parameters, path_results=path_results, \n",
    "                                           run_number=run_number, use_process=use_process)\n",
    "        self.logger.info ('time spent on this experiment: {}'.format(time.time()-time_before))\n",
    "\n",
    "        # #####################################\n",
    "        # Final scores\n",
    "        # #####################################\n",
    "        score_name = self.key_score\n",
    "        if len(score_name) > 0:\n",
    "            if score_name[0] == '_':\n",
    "                score_name = score_name[1:]\n",
    "            if score_dict.get(score_name) is not None:\n",
    "                self.logger.info (f'score: {score_dict.get(score_name)}')\n",
    "\n",
    "        spent_time = time.time() - time_before\n",
    "\n",
    "        return score_dict, spent_time\n",
    "\n",
    "    # *************************************************************************\n",
    "    #   run_experiment methods\n",
    "    # *************************************************************************\n",
    "    def _run_experiment (self, parameters={}, path_results='./results', run_number=None, use_process=False):\n",
    "\n",
    "        parameters['run_number'] = run_number\n",
    "\n",
    "        # wrap parameters\n",
    "        parameters = Bunch(**parameters)\n",
    "\n",
    "        if use_process:\n",
    "            return self.run_experiment_in_separate_process (parameters, path_results)\n",
    "        else:\n",
    "            return self.run_experiment (parameters=parameters, path_results=path_results)\n",
    "\n",
    "    def run_experiment_in_separate_process (self, parameters={}, path_results='./results'):\n",
    "\n",
    "        parameters['return_results']=False\n",
    "        p = Process(target=self.run_experiment_saving_results, args=(parameters, path_results))\n",
    "        p.start()\n",
    "        p.join()\n",
    "        if p.exitcode != 0:\n",
    "            self.logger.warning ('process exited with non-zero code: there might be an error '\n",
    "                                 'in run_pipeline function')\n",
    "\n",
    "        path_dict_results = f'{path_results}/dict_results.pk'\n",
    "        try:\n",
    "            dict_results = joblib.load (path_dict_results)\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError (f'{path_dict_results} not found: probably there is an error in run_pipeline'\n",
    "                                'function. Please run in debug mode, without multi-processing')\n",
    "\n",
    "        return dict_results\n",
    "\n",
    "    def run_experiment_saving_results (self, parameters={}, path_results='./results'):\n",
    "        dict_results = self.run_experiment (parameters=parameters, path_results=path_results)\n",
    "        joblib.dump (dict_results, '%s/dict_results.pk' %path_results)\n",
    "\n",
    "    def run_experiment (self, parameters={}, path_results='./results'):\n",
    "        raise NotImplementedError ('This method needs to be defined in subclass')\n",
    "\n",
    "\n",
    "    # *************************************************************************\n",
    "    # *************************************************************************\n",
    "    def create_experiment_and_run (self, parameters={}, other_parameters={},\n",
    "                                   info=Bunch(), em_args=Bunch(),\n",
    "                                   run_number=0, log_message=None, stack_level=-3,\n",
    "                                   precision=1e-15, experiment_number=None,\n",
    "                                   repeat_experiment=False, remove_not_finished=False,\n",
    "                                   only_remove_not_finished=False, check_finished=False,\n",
    "                                   recompute_metrics=False, force_recompute_metrics=False,\n",
    "                                   check_finished_if_interrupted=False, prev_epoch=False,\n",
    "                                   use_previous_best=dflt.use_previous_best, from_exp=None,\n",
    "                                   skip_interrupted=False, use_last_result=False,\n",
    "                                   run_if_not_interrumpted=False, use_last_result_from_dict=False,\n",
    "                                   previous_model_file_name=None, model_extension='h5', \n",
    "                                   model_name='checkpoint_', epoch_offset=0, name_best_model='best_model',\n",
    "                                   name_last_epoch=dflt.name_last_epoch, min_iterations=dflt.min_iterations,\n",
    "                                   use_process=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Log experiment parameters, run experiment if not run before, and log results.\n",
    "        \n",
    "        Can resume previously run experiment.\n",
    "        \"\"\"\n",
    "        current_em_args = Bunch ()\n",
    "        store_attr (store_args=False, self=current_em_args, but='parameters, other_parameters, info, em_args')\n",
    "        em_args.update (current_em_args)\n",
    "        requested_experiment_number = experiment_number\n",
    "        # ****************************************************\n",
    "        #  preliminary set-up: logger and path_experiments\n",
    "        # ****************************************************\n",
    "        if log_message is not None:\n",
    "            self.logger.info ('**************************************************')\n",
    "            self.logger.info (log_message)\n",
    "            self.logger.info ('**************************************************')\n",
    "\n",
    "        # insert path to experiment script file that called the experiment manager\n",
    "        insert_experiment_script_path (info, self.logger, stack_level=stack_level)\n",
    "\n",
    "        # create directories\n",
    "        path_experiments = self.path_experiments\n",
    "        path_experiments.mkdir (parents=True, exist_ok=True)\n",
    "\n",
    "        # ****************************************************\n",
    "        # register (subclassed) manager so that it can be used by decoupled modules\n",
    "        # ****************************************************\n",
    "        self.register_and_store_subclassed_manager ()\n",
    "\n",
    "        # ****************************************************\n",
    "        #   get experiment number given parameters\n",
    "        # ****************************************************\n",
    "        parameters = remove_defaults (parameters)\n",
    "\n",
    "        experiment_number, experiment_data = load_or_create_experiment_values (\n",
    "            path_experiments, parameters, precision=precision)\n",
    "\n",
    "        # if old experiment, we can require that given parameters match with experiment number\n",
    "        if (requested_experiment_number is not None\n",
    "            and experiment_number != requested_experiment_number):\n",
    "            raise ValueError (f'expected number: {requested_experiment_number}, '\n",
    "                              f'found: {experiment_number}')\n",
    "        other_parameters['experiment_number'] = experiment_number\n",
    "\n",
    "        # ****************************************************\n",
    "        # get key_score\n",
    "        # ****************************************************\n",
    "        key_score = self.key_score\n",
    "\n",
    "        # ****************************************************\n",
    "        #   get run_id, if not given\n",
    "        # ****************************************************\n",
    "        if run_number is None:\n",
    "            run_number = 0\n",
    "            mi_score = (dflt.scores_col, key_score, run_number)\n",
    "            while not isnull(experiment_data, experiment_number, mi_score):\n",
    "                score = experiment_data.loc[experiment_number, mi_score]\n",
    "                self.logger.info (f'found previous run for experiment number {experiment_number}, '\n",
    "                                  f'run {run_number}, with score {key_score} = {score}')\n",
    "                run_number += 1\n",
    "                mi_score = (dflt.scores_col, key_score, run_number)\n",
    "            self.logger.info (f'starting experiment {experiment_number} with run number {run_number}')\n",
    "\n",
    "        else:\n",
    "            mi_score = (dflt.scores_col, key_score, run_number)\n",
    "            if not isnull(experiment_data, experiment_number, mi_score):\n",
    "                previous_result = experiment_data.loc[experiment_number, mi_score]\n",
    "                self.logger.info (f'found completed: experiment number: {experiment_number}, '\n",
    "                                  f'run number: {run_number} - score: {previous_result}')\n",
    "                self.logger.info (parameters)\n",
    "                if repeat_experiment:\n",
    "                    self.logger.info ('redoing experiment')\n",
    "\n",
    "        # ****************************************************\n",
    "        #   remove unfinished experiments\n",
    "        # ****************************************************\n",
    "        if remove_not_finished:\n",
    "            name_finished = (dflt.run_info_col, 'finished', run_number)\n",
    "            if not isnull(experiment_data, experiment_number, name_finished):\n",
    "                finished = experiment_data.loc[experiment_number, name_finished]\n",
    "                self.logger.info (f'experiment {experiment_number}, run number {run_number}, finished {finished}')\n",
    "                if not finished:\n",
    "                    experiment_data.loc[experiment_number, mi_score] = None\n",
    "                    write_df (experiment_data, path_experiments)\n",
    "                    self.logger.info (f'removed experiment {experiment_number}, '\n",
    "                                 f'run number {run_number}, finished {finished}')\n",
    "            if only_remove_not_finished:\n",
    "                return None, {}\n",
    "\n",
    "        unfinished_flag = False\n",
    "        name_epoch = self.name_epoch\n",
    "        current_path_results = self.get_path_results (experiment_number, run_number=run_number)\n",
    "\n",
    "        # ****************************************************\n",
    "        #   check conditions for skipping experiment\n",
    "        # ****************************************************\n",
    "        if not isnull(experiment_data, experiment_number, mi_score) and not repeat_experiment:\n",
    "            if (check_finished\n",
    "                and not self.finished_all_epochs (parameters, current_path_results)):\n",
    "                unfinished_flag = True\n",
    "            else:\n",
    "                self.logger.info ('skipping...')\n",
    "                return previous_result, {key_score: previous_result}\n",
    "        elif (isnull(experiment_data, experiment_number, mi_score)\n",
    "              and recompute_metrics\n",
    "              and not force_recompute_metrics):\n",
    "            self.logger.info (f'experiment not found, skipping {run_number} due to only recompute_metrics')\n",
    "            return None, {}\n",
    "\n",
    "        # ****************************************************\n",
    "        # log info\n",
    "        # ****************************************************\n",
    "        self.logger.info ('running experiment %d' %experiment_number)\n",
    "        self.logger.info ('run number: %d' %run_number)\n",
    "        self.logger.info ('\\nparameters:\\n%s' %print_parameters(parameters))\n",
    "\n",
    "        # ****************************************************\n",
    "        #  get paths\n",
    "        # ****************************************************\n",
    "        # path_experiment folder\n",
    "        path_experiment = self.get_path_experiment (experiment_number)\n",
    "        path_experiment.mkdir (parents=True, exist_ok=True)\n",
    "\n",
    "        # path_results folder (where results are)\n",
    "        path_results = self.get_path_results (run_number=run_number, path_experiment=path_experiment)\n",
    "        os.makedirs (path_results, exist_ok=True)\n",
    "\n",
    "        # path to save big files\n",
    "        path_results_big_size = self.get_path_alternative (path_results)\n",
    "        os.makedirs (path_results_big_size, exist_ok = True)\n",
    "        other_parameters['path_results_big'] = path_results_big_size\n",
    "\n",
    "        # ****************************************************\n",
    "        # get git and record parameters\n",
    "        # ****************************************************\n",
    "        # get git revision number\n",
    "        info['git_hash'] = get_git_revision_hash(path_experiments)\n",
    "\n",
    "        # write parameters in experiment folder\n",
    "        record_parameters (path_experiment, parameters, other_parameters, em_args, info, self.__dict__)\n",
    "\n",
    "        # store hyper_parameters in dictionary that maps experiment_number with hyper_parameter values\n",
    "        store_parameters (path_experiments, experiment_number, parameters)\n",
    "\n",
    "        # ****************************************************************\n",
    "        # loggers\n",
    "        # ****************************************************************\n",
    "        logger_experiment = set_logger (\"experiment\", path_results, verbose=self.verbose)\n",
    "        logger_experiment.info (f'script: {info[\"script_path\"]}, line number: {info[\"lineno\"]}')\n",
    "        if os.path.exists(info['script_path']):\n",
    "            shutil.copy (info['script_path'], path_results)\n",
    "            shutil.copy (info['script_path'], path_experiment)\n",
    "\n",
    "        # summary logger\n",
    "        logger_summary = set_logger (\"summary\", path_experiments, mode='w', stdout=False, just_message=True,\n",
    "                                     filename='summary.txt', verbose=self.verbose,\n",
    "                                     verbose_out=self.verbose)\n",
    "        logger_summary.info (f'\\n\\n{\"*\"*100}\\nexperiment: {experiment_number}, run: {run_number}\\n'\n",
    "                             f'script: {info[\"script_path\"]}, line number: {info[\"lineno\"]}\\n'\n",
    "                             f'parameters:\\n{print_parameters(parameters)}{\"*\"*100}')\n",
    "        if info.get('rerun_script') is not None:\n",
    "            logger_summary.info ('\\nre-run:\\n{}'.format(info['rerun_script']))\n",
    "        # same file in path_results\n",
    "        logger_summary2 = set_logger (\"summary\", path_results, mode='w', stdout=False,\n",
    "                                      just_message=True, filename='summary.txt', verbose=self.verbose,\n",
    "                                      verbose_out=self.verbose)\n",
    "        logger_summary2.info (f'\\n\\n{\"*\"*100}\\nexperiment: {experiment_number}, run: {run_number}\\nscript: '\n",
    "                              f'{info[\"script_path\"]}, line number: {info[\"lineno\"]}\\n'\n",
    "                              f'parameters:\\n{print_parameters(parameters)}{\"*\"*100}')\n",
    "\n",
    "        # ****************************************************************\n",
    "        # Do final adjustments to parameters\n",
    "        # ****************************************************************\n",
    "        parameters = parameters.copy()\n",
    "        original_parameters = parameters.copy()\n",
    "        parameters.update(other_parameters)\n",
    "\n",
    "        # add default parameters - their values are overwritten by input values, if given\n",
    "        defaults = self.get_default_parameters(parameters)\n",
    "        parameters_with_defaults = defaults.copy()\n",
    "        parameters_with_defaults.update(parameters)\n",
    "        parameters = parameters_with_defaults\n",
    "\n",
    "        # ***********************************************************\n",
    "        # resume from previous experiment\n",
    "        # ***********************************************************\n",
    "        if (isnull(experiment_data, experiment_number, mi_score)\n",
    "            and check_finished_if_interrupted\n",
    "            and not self.finished_all_epochs (parameters, current_path_results)):\n",
    "            unfinished_flag = True\n",
    "\n",
    "        resuming_from_prev_epoch_flag = False\n",
    "        if prev_epoch:\n",
    "            self.logger.info('trying prev_epoch')\n",
    "            experiment_data2 = experiment_data.copy()\n",
    "            if (not unfinished_flag\n",
    "                and (repeat_experiment or isnull(experiment_data, experiment_number, mi_score))):\n",
    "                    experiment_data2 = experiment_data2.drop(experiment_number,axis=0)\n",
    "            prev_experiment_number = self.find_closest_epoch (experiment_data2, original_parameters)\n",
    "            if prev_experiment_number is not None:\n",
    "                self.logger.info(f'using prev_epoch: {prev_experiment_number}')\n",
    "                prev_path_results = self.get_path_results (prev_experiment_number,\n",
    "                                                           run_number=run_number)\n",
    "                found = self.make_resume_from_checkpoint (parameters, prev_path_results)\n",
    "                if found:\n",
    "                    self.logger.info (f'found previous exp: {prev_experiment_number}')\n",
    "                    if prev_experiment_number == experiment_number:\n",
    "                        other_parameters['use_previous_best'] = use_previous_best\n",
    "                        if not use_previous_best and unfinished_flag:\n",
    "                            prev_epoch = self.get_last_epoch (parameters,\n",
    "                                                              current_path_results)\n",
    "                            prev_epoch = max (int(prev_epoch), 0)\n",
    "                            parameters[name_epoch] = parameters[name_epoch] - prev_epoch\n",
    "                        self.logger.info ('using previous best')\n",
    "                    else:\n",
    "                        mi_epoch = (dflt.parameters_col, name_epoch, '')\n",
    "                        prev_epoch = experiment_data.loc[prev_experiment_number,mi_epoch]\n",
    "                        prev_epoch = (int(prev_epoch) if prev_epoch is not None\n",
    "                                      else defaults.get(name_epoch))\n",
    "                        parameters[name_epoch] = parameters[name_epoch] - prev_epoch\n",
    "\n",
    "                resuming_from_prev_epoch_flag = found\n",
    "\n",
    "\n",
    "        if not resuming_from_prev_epoch_flag and from_exp is not None:\n",
    "            prev_experiment_number = from_exp\n",
    "            self.logger.info('using previous experiment %d' %prev_experiment_number)\n",
    "            prev_path_results = self.get_path_results (prev_experiment_number, run_number=run_number)\n",
    "            self.make_resume_from_checkpoint (parameters, prev_path_results, use_best=True,\n",
    "                                              previous_model_file_name=previous_model_file_name,\n",
    "                                              model_extension=model_extension, model_name=model_name,\n",
    "                                              epoch_offset=epoch_offset, name_best_model=name_best_model,\n",
    "                                              name_last_epoch=name_last_epoch)\n",
    "\n",
    "        # ****************************************************************\n",
    "        #   Analyze if experiment was interrupted\n",
    "        # ****************************************************************\n",
    "        if skip_interrupted:\n",
    "            was_interrumpted = self.exists_current_checkpoint (parameters, path_results)\n",
    "            was_interrumpted = (was_interrumpted or\n",
    "                                self.obtain_last_result (\n",
    "                                    parameters, path_results,\n",
    "                                    use_last_result_from_dict=use_last_result_from_dict,\n",
    "                                    min_iterations=min_iterations) is not None)\n",
    "            if was_interrumpted:\n",
    "                self.logger.info ('found intermediate results, skipping...')\n",
    "                return None, {}\n",
    "\n",
    "        # ****************************************************************\n",
    "        # retrieve last results in interrupted experiments\n",
    "        # ****************************************************************\n",
    "        run_pipeline = True\n",
    "        if use_last_result:\n",
    "            dict_results = self.obtain_last_result (\n",
    "                parameters, path_results, use_last_result_from_dict=use_last_result_from_dict,\n",
    "                min_iterations=min_iterations)\n",
    "            if dict_results is None and run_if_not_interrumpted:\n",
    "                run_pipeline = True\n",
    "            elif dict_results is None:\n",
    "                return None, {}\n",
    "            else:\n",
    "                run_pipeline = False\n",
    "\n",
    "        # ****************************************************************\n",
    "        # run experiment\n",
    "        # ****************************************************************\n",
    "        if run_pipeline:\n",
    "            dict_results, time_spent = self.run_experiment_pipeline (run_number, path_results,\n",
    "                                                                          parameters=parameters,\n",
    "                                                                          use_process=use_process)\n",
    "            finished = True\n",
    "        else:\n",
    "            finished = False\n",
    "            time_spent = None\n",
    "\n",
    "        # ****************************************************************\n",
    "        #  Retrieve and store results\n",
    "        # ****************************************************************\n",
    "        self.log_results (dict_results, experiment_data, experiment_number, run_number, time_spent, \n",
    "                          finished=finished)\n",
    "\n",
    "        try:\n",
    "            save_other_parameters (experiment_number, {**other_parameters, **em_args, **info}, path_experiments)\n",
    "        except Exception as e:\n",
    "            print (f'error saving other parameters: {e}')\n",
    "\n",
    "        logger_summary2.info ('\\nresults:\\n{}'.format(dict_results))\n",
    "        self.logger.info ('finished experiment %d' %experiment_number)\n",
    "\n",
    "        # return final score\n",
    "        result = dict_results.get(key_score)\n",
    "        return result, dict_results\n",
    "    \n",
    "    def log_results (self, dict_results, experiment_data, experiment_number, run_number,\n",
    "                time_spent, finished=True, str_run_number=False):\n",
    "        if not isinstance(dict_results, dict): dict_results = {self.key_score: dict_results}\n",
    "        if str_run_number:\n",
    "            experiment_data.columns = pd.MultiIndex.from_tuples(\n",
    "                [(c[0],c[1],str(c[2])) for c in experiment_data.columns])\n",
    "            run_number = str(run_number)\n",
    "        columns = pd.MultiIndex.from_product ([[dflt.scores_col], \n",
    "                                               list(dict_results.keys()), \n",
    "                                               [run_number]])\n",
    "        experiment_data[[x for x in columns if x not in experiment_data]] = None\n",
    "        experiment_data.loc[experiment_number, columns]=dict_results.values()\n",
    "\n",
    "        mi_col = (dflt.run_info_col, 'time', run_number)\n",
    "        if isnull(experiment_data, experiment_number,  mi_col) and finished:\n",
    "            experiment_data.loc[experiment_number, mi_col]=time_spent\n",
    "        mi_col = (dflt.run_info_col, 'date', run_number)\n",
    "        experiment_data.loc[experiment_number, mi_col]=datetime.datetime.time(datetime.datetime.now())\n",
    "        mi_col = (dflt.run_info_col, 'finished', run_number)\n",
    "        experiment_data.loc[experiment_number, mi_col]=finished\n",
    "\n",
    "        try:\n",
    "            experiment_data = experiment_data[experiment_data.columns.sort_values()]\n",
    "        except TypeError as e:\n",
    "            self.logger.warning (f'received exception {e}, we cannot sort the columns')\n",
    "        write_df (experiment_data, self.path_experiments)\n",
    "\n",
    "    def grid_search (self, parameters_multiple_values={}, parameters_single_value={}, other_parameters={},\n",
    "                     info=Bunch(), run_numbers=[0], random_search=False, load_previous=False,\n",
    "                     log_message='', nruns=None, keep='multiple', **kwargs):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        path_experiments = self.path_experiments\n",
    "        path_results_base = path_experiments\n",
    "\n",
    "        os.makedirs (path_results_base,exist_ok=True)\n",
    "\n",
    "        if keep=='multiple':\n",
    "            parameters_single_value = {k:parameters_single_value[k]\n",
    "                                       for k in parameters_single_value.keys()\n",
    "                                       if k not in parameters_multiple_values}\n",
    "        elif keep=='single':\n",
    "            parameters_multiple_values = {k:parameters_multiple_values[k]\n",
    "                                          for k in parameters_multiple_values.keys()\n",
    "                                          if k not in parameters_single_value}\n",
    "        else:\n",
    "            raise ValueError (f'parameter keep {keep} not recognized: it must be either multiple or single')\n",
    "\n",
    "        parameters_multiple_values_all = parameters_multiple_values\n",
    "        parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))\n",
    "\n",
    "        if log_message != '':\n",
    "            info['log_message'] = log_message\n",
    "        insert_experiment_script_path (info, self.logger)\n",
    "\n",
    "        if random_search:\n",
    "            path_random_hp = '%s/random_hp.pk' %path_results_base\n",
    "            if load_previous and os.path.exists(path_random_hp):\n",
    "                parameters_multiple_values_all = joblib.load (path_random_hp)\n",
    "            else:\n",
    "                parameters_multiple_values_all = list (np.random.permutation(parameters_multiple_values_all))\n",
    "                joblib.dump (parameters_multiple_values_all, path_random_hp)\n",
    "        for (i_hp, parameters_multiple_values) in enumerate (parameters_multiple_values_all):\n",
    "            parameters = parameters_multiple_values.copy()\n",
    "            parameters.update(parameters_single_value)\n",
    "\n",
    "            for (i_run, run_number) in enumerate (run_numbers):\n",
    "                self.logger.info (f'processing hyper-parameter {i_hp+1} '\n",
    "                                 f'out of {len(parameters_multiple_values_all)}')\n",
    "                self.logger.info (f'doing run {i_run+1} out of {len(run_numbers)}')\n",
    "                self.logger.info (log_message)\n",
    "\n",
    "                self.create_experiment_and_run (parameters=parameters, other_parameters=other_parameters,\n",
    "                                                info=info, run_number=run_number, **kwargs)\n",
    "\n",
    "        # This solves an intermitent issue found in TensorFlow (reported as bug by community)\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    def run_multiple_repetitions (self, parameters={}, other_parameters={},\n",
    "                     log_message='', nruns=None, run_numbers=[0], **kwargs):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        if nruns is not None: run_numbers = range (nruns)\n",
    "\n",
    "        path_experiments = self.path_experiments\n",
    "        path_experiments.mkdir (parents=True, exist_ok = True)\n",
    "\n",
    "        results = np.zeros((len(run_numbers),))\n",
    "        for (i_run, run_number) in enumerate(run_numbers):\n",
    "                self.logger.info('doing run %d out of %d' %(i_run+1, len(run_numbers)))\n",
    "                self.logger.info('%s' %log_message)\n",
    "\n",
    "                results[i_run], dict_results  = self.create_experiment_and_run (\n",
    "                    parameters=parameters, other_parameters=other_parameters,\n",
    "                    run_number=run_number, **kwargs)\n",
    "                if dict_results.get('is_pruned', False):\n",
    "                    break\n",
    "\n",
    "        mu, std = results.mean(), results.std()\n",
    "        self.logger.info (f'mean {self.key_score}: {mu}, std: {std}')\n",
    "\n",
    "        dict_results[self.key_score] = mu\n",
    "\n",
    "        return mu, std, dict_results\n",
    "\n",
    "    def hp_optimization (self, parameter_sampler=None, log_message=None,\n",
    "                         parameters={}, other_parameters={}, info=Bunch(),\n",
    "                         nruns=None, stack_level=-3, sampler_method='random', \n",
    "                         pruner_method='none', n_evaluations=20, seed=0, \n",
    "                         n_startup_trials=5, n_startup_trials_pruner=5,\n",
    "                         n_trials=10, study_name='hp_study', \n",
    "                         run_number=None, n_jobs=1, nruns_best=0, \n",
    "                         n_warmup_steps=None, **kwargs):\n",
    "\n",
    "        import optuna\n",
    "        from optuna.pruners import SuccessiveHalvingPruner, MedianPruner\n",
    "        from optuna.samplers import RandomSampler, TPESampler\n",
    "        from optuna.integration.skopt import SkoptSampler\n",
    "\n",
    "        optuna.logging.disable_propagation()\n",
    "\n",
    "        em_args = Bunch ()\n",
    "        store_attr (store_args=False, self=em_args, but='parameters, other_parameters')\n",
    "\n",
    "        path_experiments = self.path_experiments\n",
    "        path_experiments.mkdir (parents=True, exist_ok=True)\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "\n",
    "        if log_message != '':\n",
    "            info['log_message'] = log_message\n",
    "        insert_experiment_script_path (info, self.logger, stack_level=stack_level)\n",
    "\n",
    "        if sampler_method == 'random':\n",
    "            sampler = RandomSampler(seed=seed)\n",
    "        elif sampler_method == 'tpe':\n",
    "            sampler = TPESampler(n_startup_trials=n_startup_trials, seed=seed)\n",
    "        elif sampler_method == 'skopt':\n",
    "            # cf https://scikit-optimize.github.io/#skopt.Optimizer\n",
    "            # GP: gaussian process\n",
    "            # Gradient boosted regression: GBRT\n",
    "            sampler = SkoptSampler(skopt_kwargs={'base_estimator': \"GP\", 'acq_func': 'gp_hedge'})\n",
    "        else:\n",
    "            raise ValueError('Unknown sampler: {}'.format(sampler_method))\n",
    "\n",
    "        if pruner_method == 'halving':\n",
    "            pruner = SuccessiveHalvingPruner(min_resource=1, reduction_factor=4, min_early_stopping_rate=0)\n",
    "        elif pruner_method == 'median':\n",
    "            # n_warmup_steps: Disable pruner until the trial reaches the given number of step.\n",
    "            if n_warmup_steps is None: n_warmup_steps = n_evaluations // 3\n",
    "            pruner = MedianPruner(n_startup_trials=n_startup_trials_pruner, n_warmup_steps=n_warmup_steps)\n",
    "        elif pruner_method == 'none': # Do not prune\n",
    "            pruner = optuna.pruners.NopPruner ()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown pruner: {pruner_method}')\n",
    "\n",
    "        self.logger.info (f'Sampler: {sampler_method} - Pruner: {pruner_method}')\n",
    "\n",
    "        #study = optuna.create_study(sampler=sampler, pruner=pruner)\n",
    "        direction = 'maximize' if self.op=='max' else 'minimize'\n",
    "        study = optuna.create_study(direction=direction,\n",
    "                                    study_name=study_name,\n",
    "                                    storage=f'sqlite:///{path_experiments}/{study_name}.db',\n",
    "                                    sampler=sampler, pruner=pruner, load_if_exists=True)\n",
    "\n",
    "        key_score = self.key_score\n",
    "\n",
    "        def objective(trial):\n",
    "\n",
    "            hp_parameters = parameters.copy()\n",
    "            self.parameters_non_pickable = dict(trial=trial)\n",
    "\n",
    "            if parameter_sampler is not None:\n",
    "                hp_parameters.update(parameter_sampler(trial))\n",
    "\n",
    "            if nruns is None or nruns < 2:\n",
    "                _, dict_results = self.create_experiment_and_run (\n",
    "                    parameters=hp_parameters, other_parameters=other_parameters,\n",
    "                    run_number=run_number, info=info, em_args=em_args, **kwargs\n",
    "                )\n",
    "            else:\n",
    "                mu_best, std_best, dict_results = self.run_multiple_repetitions (\n",
    "                    parameters=hp_parameters, other_parameters=other_parameters,\n",
    "                    info=info, em_args=em_args, nruns=nruns, **kwargs\n",
    "                )\n",
    "\n",
    "            if dict_results.get('is_pruned', False):\n",
    "                raise optuna.structs.TrialPruned()\n",
    "\n",
    "            assert key_score in dict_results, f'metric {key_score} not found in results'\n",
    "\n",
    "            return dict_results[key_score]\n",
    "\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)\n",
    "\n",
    "        self.logger.info ('Number of finished trials: {}'.format(len(study.trials)))\n",
    "        self.logger.info ('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        self.logger.info ('Value: {}'.format(trial.value))\n",
    "        self.logger.info ('best params: {}'.format (study.best_params))\n",
    "        best_value = trial.value\n",
    "\n",
    "        if nruns_best > 0:\n",
    "            self.logger.info ('running best configuration %d times' %nruns_best)\n",
    "            parameters.update (study.best_params)\n",
    "            mu_best, std_best, _ = self.run_multiple_repetitions (\n",
    "                parameters=parameters, other_parameters=other_parameters,\n",
    "                nruns=nruns_best, info=info, em_args=em_args, **kwargs)\n",
    "            best_value = mu_best\n",
    "\n",
    "        return best_value\n",
    "    \n",
    "    def greedy_search (self, run_numbers=[0], nruns=None, other_parameters={}, \n",
    "                       em_args={}, parameters_greedy={},\n",
    "                       parameters_single_value={}, \n",
    "                       parameters_multiple_values={}, log_message='', \n",
    "                       only_if_exists=False, check_experiment_matches=True,\n",
    "                       query_args={}, info=Bunch(), n_iterations=1, **kwargs):\n",
    "        \n",
    "        from hpsearch.tools.query import query\n",
    "        \n",
    "        em_args=kwargs\n",
    "        parameters_multiple_values_original = parameters_multiple_values.copy()\n",
    "        script_parameters = {}\n",
    "        insert_experiment_script_path (script_parameters, self.logger)\n",
    "        info['rerun_script'] = script_parameters\n",
    "        em_args['info'] = info\n",
    "        parameters_greedy_list = (parameters_greedy if isinstance (parameters_greedy, list)\n",
    "                                  else [{k:parameters_greedy[k]} for k in parameters_greedy])\n",
    "        for iteration in range(n_iterations):\n",
    "            for parameters_greedy in parameters_greedy_list:\n",
    "                parameters_multiple_values = parameters_multiple_values_original.copy()\n",
    "                parameters_multiple_values.update (parameters_greedy)\n",
    "\n",
    "                df = query (folder=self.folder, metric=self.key_score, op=self.op, stats=['mean'], **query_args)\n",
    "                if df is None or df.empty:\n",
    "                    message = 'starting set of experiments with grid-search'\n",
    "                    print (message); self.logger.info (message)\n",
    "                    self.grid_search (parameters_multiple_values=parameters_multiple_values, \n",
    "                                      parameters_single_value=parameters_single_value, \n",
    "                                      other_parameters=other_parameters, \n",
    "                                      run_numbers=run_numbers, log_message=log_message, \n",
    "                                      nruns=nruns, **em_args)\n",
    "                else:\n",
    "                    experiment_number = df.index[0]\n",
    "                    dict_par = df.loc[experiment_number, dflt.parameters_col].to_dict()\n",
    "                    dict_par = {k[0]:dict_par[k] for k in dict_par}\n",
    "                    message = '- Continuing search starting from previous best\\n'\n",
    "                    message += (f'    Exp#={experiment_number}, '\n",
    "                                f'Parameters={dict_par}\\n')\n",
    "                    message += (f'    Best metric {self.key_score}: '\n",
    "                                f'{df.loc[experiment_number, (dflt.stats_col, self.key_score)].values}\\n')\n",
    "                    message += (f'- Starting new round with variable parameters: {parameters_multiple_values}\\n'\n",
    "                                f'And fixed parameters: {parameters_single_value}')\n",
    "                    print (message); self.logger.info (message)\n",
    "                    self.rerun_experiment (experiments=[experiment_number], nruns=nruns,\n",
    "                                           other_parameters=other_parameters, \n",
    "                                           parameters=parameters_single_value, \n",
    "                                           parameters_multiple_values=parameters_multiple_values,\n",
    "                                           log_message=log_message, only_if_exists=only_if_exists, \n",
    "                                           check_experiment_matches=check_experiment_matches, **em_args)\n",
    "\n",
    "    def rerun_experiment (self, experiments=[], run_numbers=[0], nruns=None,\n",
    "                          other_parameters={}, em_args={}, parameters={},\n",
    "                          parameter_sampler=None, parameters_multiple_values=None,\n",
    "                          log_message='', only_if_exists=False, check_experiment_matches=True,\n",
    "                          info=Bunch (), **kwargs):\n",
    "\n",
    "        other_parameters = other_parameters.copy()\n",
    "        em_args = kwargs\n",
    "        path_experiments = self.path_experiments\n",
    "\n",
    "        if nruns is not None:\n",
    "            run_numbers = range (nruns)\n",
    "\n",
    "        parameters_original = parameters\n",
    "        other_parameters_original = other_parameters\n",
    "        em_args_original = em_args\n",
    "        for experiment_id in experiments:\n",
    "            path_experiment = self.get_path_experiment (experiment_id)\n",
    "            check_experiment_matches = (check_experiment_matches and\n",
    "                                        parameters_multiple_values is None\n",
    "                                        and parameter_sampler is None)\n",
    "            parameters, other_parameters, em_args = load_parameters (em=self,\n",
    "                experiment=experiment_id,\n",
    "                other_parameters=other_parameters_original, em_args=em_args_original,\n",
    "                parameters=parameters_original, check_experiment_matches=check_experiment_matches\n",
    "            )\n",
    "\n",
    "            if 'log_message' in em_args:\n",
    "                info['old_log_message'] = em_args['log_message']\n",
    "                del em_args['log_message']\n",
    "            if 'run_number' in em_args:\n",
    "                info['old_run_number'] = em_args['run_number']\n",
    "                del em_args['run_number']\n",
    "            \n",
    "            # we need to set the following flag to False, since otherwise when we request to store the intermediate results\n",
    "            # and the experiment did not start, we do not run the experiment\n",
    "            if (em_args.get('use_last_result', False)\n",
    "                and not em_args_original.get('use_last_result', False)):\n",
    "                self.logger.debug ('changing other_parameters[\"use_last_result\"] to False')\n",
    "                em_args['use_last_result'] = False\n",
    "            self.logger.info (f'running experiment {experiment_id} with parameters:\\n{parameters}\\n'\n",
    "                         f'other_parameters:\\n{other_parameters}')\n",
    "\n",
    "            if parameter_sampler is not None:\n",
    "                self.logger.info ('running hp_optimization')\n",
    "                if 'parameter_sampler' in em_args:\n",
    "                    info['old_parameter_sampler'] = em_args['parameter_sampler']\n",
    "                    del em_args['parameter_sampler']\n",
    "                insert_experiment_script_path (info, self.logger)\n",
    "                em_args['info'] = info\n",
    "                self.hp_optimization (parameter_sampler=parameter_sampler,\n",
    "                                      log_message=log_message, parameters=parameters,\n",
    "                                      other_parameters=other_parameters, **em_args)\n",
    "            elif parameters_multiple_values is not None:\n",
    "                script_parameters = {}\n",
    "                insert_experiment_script_path (script_parameters, self.logger)\n",
    "                info['rerun_script'] = script_parameters\n",
    "                em_args['info'] = info\n",
    "                self.grid_search (\n",
    "                    parameters_multiple_values=parameters_multiple_values,\n",
    "                    parameters_single_value=parameters, other_parameters=other_parameters,\n",
    "                    run_numbers=run_numbers, log_message=log_message, **em_args)\n",
    "            else:\n",
    "                if only_if_exists:\n",
    "                    run_numbers = [run_number for run_number in run_numbers\n",
    "                                   if (path_experiment/run_number).exists()]\n",
    "\n",
    "                script_parameters = {}\n",
    "                insert_experiment_script_path (script_parameters, self.logger)\n",
    "                info['rerun_script'] = script_parameters\n",
    "                em_args['info'] = info\n",
    "                self.run_multiple_repetitions (\n",
    "                    parameters=parameters, other_parameters=other_parameters,\n",
    "                    log_message=log_message, run_numbers=run_numbers, **em_args\n",
    "                )\n",
    "\n",
    "    def rerun_experiment_pipeline (self, experiments, run_numbers=None,\n",
    "                                   new_parameters={}, save_results=False):\n",
    "\n",
    "        path_experiments = self.path_experiments\n",
    "        for experiment_id in experiments:\n",
    "            path_experiment = self.get_path_experiment (experiment_id)\n",
    "\n",
    "            parameters, other_parameters, em_args, info, em_attrs =joblib.load (\n",
    "                f'{path_experiment}/parameters.pk'\n",
    "            )\n",
    "            parameters = parameters.copy()\n",
    "            parameters.update(other_parameters)\n",
    "            parameters.update(new_parameters)\n",
    "            for run_number in run_numbers:\n",
    "                path_results = path_experiment/f'{run_number}'\n",
    "                path_data = self.get_path_data (run_number, parameters)\n",
    "                dict_results, time_spent = self.run_experiment_pipeline (run_number, path_results,\n",
    "                                                         parameters=parameters)\n",
    "\n",
    "                if save_results:\n",
    "                    experiment_number = experiment_id\n",
    "                    experiment_data = read_df (path_experiments)\n",
    "                    self.log_results (dict_results, experiment_data, experiment_number, run_number, time_spent)\n",
    "                    \n",
    "    def rerun_experiment_par (self, experiments, run_numbers=None, parameters={}):\n",
    "\n",
    "        path_experiments = self.path_experiments\n",
    "        for experiment_id in experiments:\n",
    "            path_experiment = self.get_path_experiment (experiment_id)\n",
    "\n",
    "            for run_number in run_numbers:\n",
    "                path_results = path_experiment/f'{run_number}'\n",
    "                self.run_experiment_pipeline (run_number, path_results, parameters=parameters)\n",
    "\n",
    "    def find_closest_epoch (self, experiment_data, parameters):\n",
    "        \"\"\"Finds experiment with same parameters except for number of epochs.\n",
    "\n",
    "        Takes the epochs that are closer but lower than the one in parameters.\"\"\"\n",
    "        name_epoch = self.name_epoch\n",
    "        experiment_numbers, _, _ = experiment_utils.find_rows_with_parameters_dict (\n",
    "            experiment_data, parameters, ignore_keys=[name_epoch,'prev_epoch'])\n",
    "\n",
    "        defaults = self.get_default_parameters(parameters)\n",
    "        current_epoch = parameters.get(name_epoch, defaults.get(name_epoch))\n",
    "        mi_epoch = (dflt.parameters_col, name_epoch, '')\n",
    "        if current_epoch is None:\n",
    "            current_epoch = -1\n",
    "        if len(experiment_numbers) > 1:\n",
    "            epochs = experiment_data.loc[experiment_numbers, mi_epoch].copy()\n",
    "            epochs[epochs.isnull()]=defaults.get(name_epoch)\n",
    "            epochs = epochs.loc[epochs<=current_epoch]\n",
    "            if epochs.shape[0] == 0:\n",
    "                return None\n",
    "            else:\n",
    "                return epochs.astype(int).idxmax()\n",
    "        elif len(experiment_numbers) == 1:\n",
    "            return experiment_numbers[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_last_epoch (self, parameters, path_results, name_last_epoch=dflt.name_last_epoch):\n",
    "\n",
    "        name_epoch = self.name_epoch\n",
    "        name_model_history = self.name_model_history\n",
    "        path_model_history = f'{path_results}/{name_model_history}'\n",
    "\n",
    "        prev_epoch = -1\n",
    "        if os.path.exists(path_model_history):\n",
    "            summary = joblib.load (path_model_history)\n",
    "            prev_epoch = summary.get(name_last_epoch)\n",
    "            if prev_epoch is None:\n",
    "                key_score = self.key_score\n",
    "                if key_score in summary and (isinstance(summary[key_score], list)\n",
    "                                             or isinstance(summary[key_score], np.array)):\n",
    "                    prev_epoch = (~np.isnan(summary[key_score])).sum()\n",
    "\n",
    "        return prev_epoch\n",
    "\n",
    "    def finished_all_epochs (self, parameters, path_results,\n",
    "                             name_last_epoch=dflt.name_last_epoch):\n",
    "        defaults = self.get_default_parameters (parameters)\n",
    "        current_epoch = parameters.get(self.name_epoch, defaults.get(self.name_epoch))\n",
    "        prev_epoch = self.get_last_epoch (parameters, path_results,\n",
    "                                          name_last_epoch=name_last_epoch)\n",
    "\n",
    "        if prev_epoch >= current_epoch:\n",
    "            finished = True\n",
    "        else:\n",
    "            finished = False\n",
    "\n",
    "        return finished\n",
    "\n",
    "    def make_resume_from_checkpoint (self, parameters, prev_path_results, use_best=False, previous_model_file_name=None,\n",
    "                                    model_extension='h5', model_name='checkpoint_', epoch_offset=0, name_best_model='best_model',\n",
    "                                    name_last_epoch=dflt.name_last_epoch):\n",
    "\n",
    "        found = False\n",
    "        path_model_history = f'{prev_path_results}/{self.name_model_history}'\n",
    "        if os.path.exists(path_model_history):\n",
    "            parameters['resume_summary'] = path_model_history\n",
    "            found = True\n",
    "            parameters['prev_path_results'] = prev_path_results\n",
    "            if parameters.get('previous_model_file_name') is not None:\n",
    "                parameters['resume'] = f'{prev_path_results}/{previous_model_file_name}'\n",
    "            elif use_best:\n",
    "                parameters['resume'] = f'{prev_path_results}/{name_best_model}.{model_extension}'\n",
    "            else:\n",
    "                summary = joblib.load (path_model_history)\n",
    "                prev_epoch = summary.get(name_last_epoch)\n",
    "                key_score = self.key_score\n",
    "                if prev_epoch is None:\n",
    "                    if key_score in summary and (isinstance(summary[key_score], list)\n",
    "                                                 or isinstance(summary[key_score], np.array)):\n",
    "                        prev_epoch = (~np.isnan(summary[key_score])).sum()\n",
    "                    else:\n",
    "                        prev_epoch = 0\n",
    "\n",
    "                if prev_epoch >= 0:\n",
    "                    parameters['resume'] = (f'{prev_path_results}/'\n",
    "                                            f'{model_name}{prev_epoch+epoch_offset}.{model_extension}')\n",
    "            if not os.path.exists(parameters['resume']):\n",
    "                path_resume2 = f'{prev_path_results}/{self.model_file_name}'\n",
    "                if os.path.exists (path_resume2):\n",
    "                    parameters['resume'] = path_resume2\n",
    "                else:\n",
    "                    parameters['resume'] = ''\n",
    "                    parameters['prev_path_results'] = ''\n",
    "                    found = False\n",
    "\n",
    "        return found\n",
    "\n",
    "    def exists_current_checkpoint (self, parameters, path_results):\n",
    "        model_file_name = self.model_file_name\n",
    "        return os.path.exists (f'{path_results}/{model_file_name}')\n",
    "\n",
    "    def obtain_last_result (self, parameters, path_results, use_last_result_from_dict=False,\n",
    "                            min_iterations=dflt.min_iterations):\n",
    "\n",
    "        if use_last_result_from_dict:\n",
    "            return self.obtain_last_result_from_dict (parameters, path_results,\n",
    "                                                      use_last_result_from_dict=use_last_result_from_dict,\n",
    "                                                      min_iterations=min_iterations)\n",
    "        name_result_file = self.name_model_history\n",
    "        path_results_file = f'{path_results}/{name_result_file}'\n",
    "        dict_results = None\n",
    "        if os.path.exists (path_results_file):\n",
    "            history = joblib.load (path_results_file)\n",
    "            metrics = parameters.get('key_scores')\n",
    "            if metrics is None:\n",
    "                metrics = history.keys()\n",
    "            ops = parameters.get('ops')\n",
    "            if ops is None:\n",
    "                ops = ['max'] * len(metrics)\n",
    "            if type(ops) is str:\n",
    "                ops = [ops] * len(metrics)\n",
    "            if type(ops) is dict:\n",
    "                ops_dict = ops\n",
    "                ops = ['max'] * len(metrics)\n",
    "                i = 0\n",
    "                for k in metrics:\n",
    "                    if k in ops_dict.keys():\n",
    "                        ops[i] = ops_dict[k]\n",
    "                    i += 1\n",
    "            dict_results = {}\n",
    "            max_last_position = -1\n",
    "            for metric, op in zip(metrics, ops):\n",
    "                if metric in history.keys():\n",
    "                    history_array = history[metric]\n",
    "                    score = min(history_array) if op == 'min' else max(history_array)\n",
    "                    last_position = np.where(np.array(history_array).ravel()==0)[0]\n",
    "                    if len(last_position) > 0:\n",
    "                        last_position = last_position[0] - 1\n",
    "                    else:\n",
    "                        last_position = len(history_array)\n",
    "                    dict_results[metric] = score\n",
    "                else:\n",
    "                    last_position = -1\n",
    "                max_last_position = max(last_position, max_last_position)\n",
    "\n",
    "            dict_results['last'] = max_last_position\n",
    "            if max_last_position < min_iterations:\n",
    "                dict_results = None\n",
    "                print (f'not storing result from {path_results} with iterations {max_last_position}')\n",
    "            else:\n",
    "                print (f'storing result from {path_results} with iterations {max_last_position}')\n",
    "\n",
    "        return dict_results\n",
    "\n",
    "    #export\n",
    "    def obtain_last_result_from_dict (self, parameters, path_results, use_last_result_from_dict=False,\n",
    "                            min_iterations=dflt.min_iterations):\n",
    "        name_result_file = self.result_file\n",
    "        path_results_file = f'{path_results}/{name_result_file}'\n",
    "        dict_results = None\n",
    "        if os.path.exists (path_results_file):\n",
    "            dict_results = joblib.load (path_results_file)\n",
    "            if 'last' not in dict_results.keys() and 'epoch' in dict_results.keys():\n",
    "                dict_results['last'] = dict_results['epoch']\n",
    "            if 'last' not in dict_results:\n",
    "                dict_results_from_history = self.obtain_last_result (\n",
    "                    parameters, path_results, use_last_result_from_dict=False,\n",
    "                    min_iterations=min_iterations)\n",
    "                if dict_results_from_history is not None:\n",
    "                    dict_results['last'] = dict_results_from_history['last']\n",
    "            if 'last' not in dict_results:\n",
    "                raise RuntimeError ('dict_results has no entry named \"last\", and '\n",
    "                                    'the value of last could not be retrieved from '\n",
    "                                    'a model history file')\n",
    "            max_last_position = dict_results['last']\n",
    "            if max_last_position < min_iterations:\n",
    "                dict_results = None\n",
    "                print (f'not storing result from {path_results} with iterations {max_last_position}')\n",
    "            else:\n",
    "                print (f'storing result from {path_results} with iterations {max_last_position}')\n",
    "\n",
    "        return dict_results\n",
    "\n",
    "    def register_and_store_subclassed_manager (self):\n",
    "        #self.logger.debug ('registering')\n",
    "        self.manager_factory.register_manager (self)\n",
    "        self.manager_factory.write_manager (self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set_path_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_set_path_experiments ():\n",
    "    em = init_em ('basic')\n",
    "    check_last_part (em.path_experiments, 'test_basic/default')\n",
    "    em.set_path_experiments (parent_path='another_path')\n",
    "    check_last_part (em.path_experiments, 'another_path/default')\n",
    "    em.set_path_experiments (folder='another_folder')\n",
    "    check_last_part (em.path_experiments, 'another_path/another_folder')\n",
    "\n",
    "    em = init_em ('basic', parent_path='test_parent')\n",
    "    check_last_part (em.path_experiments, 'test_parent/default')\n",
    "\n",
    "    em = init_em ('basic', folder='other_folder')\n",
    "    check_last_part (em.path_experiments, 'test_basic/other_folder')\n",
    "\n",
    "    em = init_em ('basic', parent_path='test_parent', folder='other_folder')\n",
    "    check_last_part (em.path_experiments, 'test_parent/other_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_set_path_experiments, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set_alternative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_set_alternative_path ():\n",
    "    em = init_em ('basic')\n",
    "    assert em.alternative_path is None\n",
    "    em.set_alternative_path (alternative_parent_path='test_another_path')\n",
    "    check_last_part (em.alternative_path, 'test_another_path/default')\n",
    "    em.set_alternative_path (alternative_path='test_alternative_path/new_folder')\n",
    "    check_last_part (em.alternative_path, 'test_alternative_path/new_folder')\n",
    "\n",
    "    em = init_em ('basic', alternative_parent_path='test_alternative_parent')\n",
    "    check_last_part (em.alternative_path, 'test_alternative_parent/default')\n",
    "\n",
    "    em = init_em ('basic', alternative_path='test_alternative_path/new_folder')\n",
    "    check_last_part (em.alternative_path, 'test_alternative_path/new_folder')\n",
    "\n",
    "    em = init_em ('basic', alternative_path='test_alternative_path/new_folder', \n",
    "                  alternative_parent_path='test_alternative_parent')\n",
    "    check_last_part (em.alternative_path, 'test_alternative_path/new_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_set_alternative_path, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_path_alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_get_path_alternative ():\n",
    "    em = init_em ('basic')\n",
    "\n",
    "    em.set_alternative_path (alternative_parent_path='test_other_path')\n",
    "    path_results = em.get_path_results (experiment_id=1, run_number=2)\n",
    "    check_last_part (path_results,'test_basic/default/experiments/00001/2')\n",
    "    path_alternative = em.get_path_alternative (path_results)\n",
    "    check_last_part (path_alternative, 'test_other_path/default/experiments/00001/2')\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_get_path_alternative, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_previous_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_remove_previous_experiments ():\n",
    "    em = init_em ('basic')\n",
    "\n",
    "    em.set_alternative_path (alternative_parent_path='test_other_path_basic')\n",
    "    em.path_experiments.mkdir (parents=True)\n",
    "    joblib.dump ([1,2,3], em.path_experiments / 'myfile.pk')\n",
    "    em.alternative_path.mkdir (parents=True)\n",
    "    joblib.dump ([10,20,30], em.alternative_path / 'other_file.pk')\n",
    "    print (em.path_experiments)\n",
    "    print (os.listdir (em.path_experiments))\n",
    "    print (em.alternative_path)\n",
    "    print (os.listdir (em.alternative_path))\n",
    "\n",
    "    # run function\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    \n",
    "    # check results\n",
    "    assert not em.path_experiments.parent.exists() and not em.alternative_path.parent.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_remove_previous_experiments, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "### create_experiment_and_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_experiment_and_run` is the main function of the `ExperimentManager`. All other functions make use of it adding additional functionalities.\n",
    "\n",
    "In order to call `create_experiment_and_run`, we pass a dictionary of parameters characterizing the experiment we want to run, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_basic_usage ():\n",
    "    em = init_em ('basic')\n",
    "\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    # The output is a tuple of two objects:\n",
    "    #1. The main result metric. In our case, we didn't indicate the name of this metric,\n",
    "    #and therefore we get None.\n",
    "    #1. A dictionary containing all the performance metrics for this experiment.\n",
    "\n",
    "    assert result==0.6\n",
    "    assert dict_results == {'validation_accuracy': 0.6, 'test_accuracy': 0.5}\n",
    "\n",
    "    # Eight files  are stored in *path_experiments*, and the `experiments` folder is created:\n",
    "\n",
    "    files_stored = ['current_experiment_number.pkl', 'experiments', 'experiments_data.csv',\n",
    "                    'experiments_data.pk', 'experiments_data_columns.pk', 'git_hash.json', 'managers', \n",
    "                    'other_parameters.csv', 'parameters.pk', 'parameters.txt', 'summary.txt']\n",
    "    display (files_stored)\n",
    "\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    assert (sorted(os.listdir (path_experiments))==\n",
    "            files_stored)\n",
    "\n",
    "    # TODO TEST: test content of the above files\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = read_df (path_experiments)\n",
    "    #df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "\n",
    "    print (f'folder created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "\n",
    "    assert list_exp == ['00000']\n",
    "\n",
    "    print ('This folder has one sub-folder per run, since '\n",
    "            'multiple runs can be done with the same parameters.')\n",
    "\n",
    "    list_run = os.listdir (f'{path_experiments}/experiments/00000')\n",
    "\n",
    "    print (f'contents of current run at `{path_experiments}/experiments/00000`:'); print(list_run)\n",
    "\n",
    "    # the same data frame can be obtained by doing:\n",
    "    df_bis = em.get_experiment_data ()\n",
    "\n",
    "    pd.testing.assert_frame_equal(df,df_bis)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_basic_usage, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "#### Running second experiment with same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_same_values ():\n",
    "    em = init_em ('same_values')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    em.raise_error_if_run = True\n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used\n",
    "    # are already present in the first experiment.\n",
    "\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "\n",
    "    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "\n",
    "    assert list_exp == ['00000']\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_same_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running second experiment with *almost* same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_almost_same_values ():\n",
    "    em = init_em ('almost_same_values')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    em.raise_error_if_run = True\n",
    "    # second experiment: the difference between the values of rate parameter is 1.e-16:\n",
    "    # too small to be considered different\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05+1e-16})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "    \n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    assert list_exp == ['00000']\n",
    "\n",
    "    # consider 1.e-17 difference big enough\n",
    "    em.raise_error_if_run = False\n",
    "    # second experiment: the difference between the values of rate parameter is 1.e-16:\n",
    "    # too small to be considered different\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05+1e-16},\n",
    "                                                         precision=1e-17)\n",
    "\n",
    "    df = em.get_experiment_data ()   \n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==2\n",
    "    \n",
    "    display (df)\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "    assert sorted(list_exp) == sorted(['00000', '00001'])\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_almost_same_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding new runs on previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_runs ():\n",
    "    em = init_em ('new_runs')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    # second experiment: in order to run another experiment with same parametres, we increase\n",
    "    # the run number. The default run number used in the first experiment is 0, so we indicate\n",
    "    # run_number=1\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         run_number=1)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0, 1]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0, 1]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    # another adding a new run number is to indicate run_number=None. This will make the experiment\n",
    "    # manager find the next run number automatically. Since we have used run numbers 0 and 1,\n",
    "    # the next run number will be 2\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         run_number=None)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0, 1, 2]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0, 1, 2]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    # As we can see, no new experiment is added to the DataFrame, since the values of the parameters used\n",
    "    # are already present in the first experiment.\n",
    "\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "\n",
    "    print (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "\n",
    "    assert list_exp == ['00000']\n",
    "\n",
    "    list_runs = os.listdir (f'{path_experiments}/experiments/00000')\n",
    "    if False:\n",
    "        assert sorted(list_runs) == ['0',\n",
    "                                     '1',\n",
    "                                     '2',\n",
    "                                     'other_parameters.json',\n",
    "                                     'parameters.json',\n",
    "                                     'parameters.pk',\n",
    "                                     'parameters.txt']\n",
    "    else:\n",
    "        print (sorted(list_runs))\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_new_runs, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding second experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_second_experiment ():\n",
    "    em = init_em ('second')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    md ('If we run a second experiment with new parameters, a new row is '\n",
    "        'added to the dataframe, and a new folder is created:')\n",
    "\n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.7, 'rate': 0.2})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==2\n",
    "    \n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    list_exp = os.listdir (f'{path_experiments}/experiments')\n",
    "\n",
    "    md (f'folders created in `{path_experiments}/experiments`:'); print(list_exp)\n",
    "\n",
    "    assert sorted(list_exp) == sorted(['00000','00001'])\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_second_experiment, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding another parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_parameter ():\n",
    "    em = init_em ('another_parameter')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    # second experiment:\n",
    "    # same parameters as before plus new parameter 'epochs' not indicated in first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # a new experiment is added, and a new parameter `epochs` is added as additional column at the end\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['epochs', 'offset', 'rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date', 'finished', 'time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy', 'validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==2\n",
    "    \n",
    "    assert (df.index==[0,1]).all()\n",
    "\n",
    "    # the new parameter has None value for all previous experiments that did not indicated its value\n",
    "    # In our case, the first experiment has None value for parameter `epochs`\n",
    "    # This means that the default value of epochs is used for that parameter.\n",
    "    # In our case, if we look at the implementation of DummyExperimentManager, we can see that\n",
    "    # the default value for epochs is 10.\n",
    "    mi_epochs = (dflt.parameters_col, 'epochs', '')\n",
    "    assert df.isna().loc[0, mi_epochs]\n",
    "    assert df.loc[1, mi_epochs] == 5.0\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_new_parameter, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding another parameter with default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_new_parameter_default ():\n",
    "    em = init_em ('another_parameter_default')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    # second experiment:\n",
    "    # same parameters as before plus new parameter 'epochs' not indicated in first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "\n",
    "    assert (df.index==[0]).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_new_parameter_default, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicating parameters that don't affect the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_other_parameters ():\n",
    "    em = init_em ('other_parameters')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment:\n",
    "    # we use the other_parameters argument to indicate a parameter that does not affect the outcome\n",
    "    # of the experiment\n",
    "    # in this example, we change the level of verbosity. This parameter should not affect how the\n",
    "    # experiment runs, and therefore we tell our experiment manager to not create a new experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         other_parameters={'verbose': False})\n",
    "\n",
    "    # second experiment:\n",
    "    # same parameters as before except for the verbosity parameter. Our experiment manager considers\n",
    "    # this experiment the same as before, and therefore it does not run it, but outputs the same results\n",
    "    # obtained before\n",
    "    em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "\n",
    "    assert (df.index==[0]).all()\n",
    "\n",
    "    md ('experiment dataframe:'); display(df)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_other_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove_not_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the parameter that specifies the number of epochs. This can be done by passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_epoch='epochs')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_remove_not_finished ():\n",
    "    em = init_em ('remove_not_finished')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment: we simulate that a halt before finishing\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         other_parameters={'halt':True})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "\n",
    "    # second experiment: remove unfinished\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.2})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':1.0, 'rate': 0.3},\n",
    "                                                         remove_not_finished=True)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_remove_not_finished, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_repeat_experiment ():\n",
    "    em = init_em ('repeat_experiment')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    mi_date = (dflt.run_info_col, 'date', 0)\n",
    "    date = df[mi_date].values[0]\n",
    "\n",
    "    # second experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05},\n",
    "                                                         repeat_experiment = True)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert df[mi_date].values[0] != date\n",
    "\n",
    "\n",
    "    # in this case, no new experiment is added, since the new parameter has the same value as the default value\n",
    "    # implicitly used in the first experiment.\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "    \n",
    "\n",
    "    assert (df.index==[0]).all()\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_repeat_experiment, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the parameter that specifies the number of epochs. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_epoch='epochs')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_check_finished ():\n",
    "    em = init_em ('check_finished')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment: we simulate that we only run for half the number of epochs\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},\n",
    "                                                         other_parameters={'actual_epochs': 5})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    mi_date = (dflt.run_info_col, 'date', 0)\n",
    "    date = df[mi_date].values[0]\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    score = df[mi_validation].values[0]\n",
    "\n",
    "    # second experiment: same values in parameters dictionary, without other_parameters\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "\n",
    "    assert (date==df[mi_date].values[0]) and (score==df[mi_validation].values[0])\n",
    "\n",
    "    # third experiment: same values in parameters dictionary, with other_parameters indicating check_finished\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10},\n",
    "                                                         check_finished=True)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==1 \n",
    "    \n",
    "    assert (df.index==[0]).all()\n",
    "    assert (date!=df[mi_date].values[0]) and (score!=df[mi_validation].values[0])\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_check_finished, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recompute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_recompute_metrics ():\n",
    "    em = init_em ('recompute_metrics')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first experiment\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    # second experiment: new values\n",
    "    em.raise_error_if_run = True\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02},\n",
    "                                                         recompute_metrics=True)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==2\n",
    "    \n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    assert np.isnan(df[mi_validation].values[1])\n",
    "\n",
    "    # third experiment: new values\n",
    "    em.raise_error_if_run = False\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.02,\n",
    "                                                                     'epochs': 10},\n",
    "                                                         recompute_metrics=True,\n",
    "                                                         force_recompute_metrics=True)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    expected_col = list(pd.MultiIndex.from_product([['parameters'],['offset','rate'],['']]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['run_info'],['date','finished','time'],[0]]))\n",
    "    expected_col += list(pd.MultiIndex.from_product([['scores'],['test_accuracy','validation_accuracy',],[0]]))\n",
    "    assert df.columns.tolist()==expected_col\n",
    "    assert df.shape[0]==2\n",
    "    \n",
    "    assert (df.index==[0,1]).all()\n",
    "    assert df[mi_validation].values[1]==0.3\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_recompute_metrics, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prev_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate: \n",
    "1. The name of the parameter that specifies the number of epochs. \n",
    "1. The name of the file where the model is stored.\n",
    "This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_epoch='epochs', model_file_name='model_weights.pk')\n",
    "```\n",
    "\n",
    "Furthermore, in order to work, we need our experiment manager to make use of the parameter `resume` or the parameter `prev_path_results`. In particular, we need it to load the model file whose path is indicated in `parameters['resume']`, or whose path is indicated in `f'{parameters[\"prev_path_results\"]}/{self.model_file_name}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_prev_epoch ():\n",
    "    em = init_em ('prev_epoch')\n",
    "\n",
    "    # get reference result\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17})\n",
    "    reference_accuracy = em.model.accuracy\n",
    "    reference_weight = em.model.weight\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "\n",
    "    # first 3 experiments\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 10})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 20})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 15})\n",
    "\n",
    "    # more epochs\n",
    "    # in order to work, we need our experiment manager to make use of the\n",
    "    # parameter 'resume' or the parameter 'prev_path_results'.\n",
    "    # In particular, we need it to load the model file\n",
    "    # whose path is indicated in parameters['resume'], or whose path is\n",
    "    # indicated in f'{parameters[\"prev_path_results\"]}/{self.model_file_name}'\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},\n",
    "                                      prev_epoch=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    assert em.model.epochs==2 and em.model.current_epoch==17\n",
    "\n",
    "    assert reference_accuracy==em.model.accuracy and reference_weight==em.model.weight\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 17},\n",
    "                                      repeat_experiment = True)\n",
    "\n",
    "    assert em.model.epochs==17 and em.model.current_epoch==17\n",
    "\n",
    "    assert reference_accuracy==em.model.accuracy and reference_weight==em.model.weight\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_prev_epoch, tag='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_prev_epoch2 ():\n",
    "    em = init_em ('prev_epoch2')\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    score, results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'actual_epochs': 2})\n",
    "\n",
    "\n",
    "    assert score==0.16 and results['validation_accuracy']==0.16\n",
    "    assert em.model.current_epoch==2 and em.model.epochs==2\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "\n",
    "\n",
    "    # We use last result and have the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        prev_epoch=True, check_finished=True, use_previous_best=False\n",
    "    )\n",
    "\n",
    "    assert score==0.25 and results['validation_accuracy']==0.25\n",
    "    assert em.model.current_epoch==5 and em.model.epochs==3\n",
    "    df = em.get_experiment_data ()\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    assert (df[mi_validation]==[0.25, 0.30]).all()\n",
    "\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "\n",
    "    # **********************************\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'actual_epochs': 2, 'halt': True})\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "\n",
    "    # We use last result and have the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        prev_epoch=True, check_finished_if_interrupted=True,\n",
    "        use_previous_best=False)\n",
    "    assert score==0.25 and results['validation_accuracy']==0.25\n",
    "    assert em.model.current_epoch==5 and em.model.epochs==3\n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df[mi_validation]==[0.25, 0.30]).all()\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_prev_epoch2, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work, we need our experiment manager to make use of the parameter `prev_path_results`. In particular, we need it to load the model file whose path is indicated in `f'{parameters[\"prev_path_results\"]}/{self.model_file_name}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_from_exp ():\n",
    "    em = init_em ('from_exp')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # get reference result\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5})\n",
    "    reference_accuracy = em.model.accuracy\n",
    "    reference_weight = em.model.weight\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "\n",
    "    # first 3 experiments\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 2})\n",
    "\n",
    "    # the following resumes from experiment 0, and trains the model for 5 more epochs\n",
    "    # using now different `offset` and `rate` hyper-parameters\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},\n",
    "                                      from_exp=0)\n",
    "\n",
    "    assert em.model.epochs==5 and em.model.current_epoch==7\n",
    "    correct_accuracy = (0.1 + 0.03*2 # accuracy of model from experiment 0\n",
    "                        + 0.05*5)     # accuracy gained by training for 5 more epochs using\n",
    "                                    #  new hyper-parameters: rate=0.05\n",
    "    assert (em.model.accuracy-correct_accuracy) < 1e-10\n",
    "    assert reference_accuracy!=em.model.accuracy\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_from_exp, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### skip_interrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the file where the model is stored.\n",
    "This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (model_file_name='model_weights.pk')\n",
    "```\n",
    "or indicating it in the `other_parameters` dictionary:\n",
    "```python\n",
    "other_parameters = dict(model_file_name='model_weights.pk')\n",
    "```\n",
    "\n",
    "Alternatively, we can indicate the name of the file where the model history exists. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_model_history='history.pk')\n",
    "```\n",
    "\n",
    "If not indicated, the experiment manager tries to find the model history in a file named `model_history.pk`. In order to consider the history good enough, the experiment manager checks if the length of the arrays stored in the model_history dictionary is at least `parameters.get('min_iterations', dflt.min_iterations)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_skip_interrupted ():\n",
    "    em = init_em ('skip_interrupted')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "\n",
    "    em.raise_error_if_run = True\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        skip_interrupted=True)\n",
    "    assert score is None and len(results)==0\n",
    "\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        skip_interrupted=True, min_iterations=1)\n",
    "    assert score is None and len(results)==0\n",
    "\n",
    "    em.model_file_name='wrong_file.pk'\n",
    "    with pytest.raises (RuntimeError):\n",
    "        score, results = em.create_experiment_and_run (\n",
    "            parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "            skip_interrupted=True)\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_skip_interrupted, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### use_last_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this functionality, we need to indicate the name of the file where the model history exists. This can be done either passing this when constructing the object:\n",
    "```python\n",
    "em = MyExperimentManager (name_model_history='history.pk')\n",
    "```\n",
    "\n",
    "If not indicated, the experiment manager tries to find the model history in a file named `model_history.pk`. In order to consider the history good enough, the experiment manager checks if the length of the arrays stored in the model_history dictionary is at least `parameters.get('min_iterations', dflt.min_iterations)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_use_last_result ():\n",
    "    em = init_em ('use_last_result')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    assert (df.isna()[mi_validation] == [True, False]).all()\n",
    "\n",
    "    # We use last result but require that number of epochs is at least 50.\n",
    "    # Since this is not true, the last result is not used.\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        use_last_result=True)\n",
    "    assert score is None and results=={}\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert (df.isna()[mi_validation] == [True, False]).all()\n",
    "\n",
    "    # We use last result and lower the required number of epochs to 2\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        use_last_result=True, min_iterations=2)\n",
    "    print (score, results)\n",
    "    assert score==0.25 and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    assert (df.isna()[mi_validation] == [False, False]).all()\n",
    "    assert (df[mi_validation] == [0.25, 0.30]).all()\n",
    "\n",
    "    # We use last result and increase the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        use_last_result=True, run_if_not_interrumpted=True)\n",
    "    print (score, results)\n",
    "    #assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_use_last_result, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### second case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_use_last_result_run_interrupted ():\n",
    "    em = init_em ('use_last_result_run_interrupted')\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "                                          other_parameters={'actual_epochs': 2, 'halt': True})\n",
    "\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.04, 'epochs': 5})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    #display (df)\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    assert (df.isna()[mi_validation] == [True, False]).all()\n",
    "\n",
    "    # We use last result and have the required number of epochs to default number (50)\n",
    "    # But we request to run the experiment until the end\n",
    "    score, results = em.create_experiment_and_run (\n",
    "        parameters={'offset':0.1, 'rate': 0.03, 'epochs': 5},\n",
    "        use_last_result=True, run_if_not_interrumpted=True)\n",
    "    print (score, results)\n",
    "    #assert score==None and results=={'validation_accuracy': 0.25, 'test_accuracy': 0.35, 'accuracy': 0.25, 'last': 5}\n",
    "    df = em.get_experiment_data ()\n",
    "    #display(df)\n",
    "    assert em.model.current_epoch==5 and em.model.epochs==5\n",
    "    assert (df.isna()[mi_validation] == [False, False]).all()\n",
    "    assert (df[mi_validation] == [0.25, 0.30]).all()\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_use_last_result_run_interrupted, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Storing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_storing_em_args_and_parameters ():\n",
    "    em = init_em ('storing_em_args_and_parameters')\n",
    "\n",
    "    path_experiment = em.get_path_experiment (0)\n",
    "    result, dict_results = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05})\n",
    "    check_last_part (path_experiment, 'test_storing_em_args_and_parameters/default/experiments/00000')\n",
    "    ref_list = ['0', 'em_args.json', 'em_attrs.json',  'info.json', 'other_parameters.json', \n",
    "     'parameters.json', 'parameters.pk', 'parameters.txt', 'test_experiment_manager.py',]\n",
    "    ref_list2 = ref_list.copy()\n",
    "    del ref_list2[3]\n",
    "    if __name__ == '__main__':\n",
    "        del ref_list[-1]\n",
    "        del ref_list2[-1]\n",
    "    result_list = sorted (os.listdir(path_experiment))\n",
    "    assert result_list==ref_list or result_list==ref_list2\n",
    "    par, other, em_args, info, em_attrs = joblib.load (path_experiment/'parameters.pk')\n",
    "    print (em_args)\n",
    "    #assert em_args ==  {'run_number': 0, 'log_message': None, 'stack_level': -3}\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_storing_em_args_and_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_grid_search ():\n",
    "    em = init_em ('grid_search')\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    em.grid_search (parameters_multiple_values={'rate': [0.03,0.01], 'epochs': [5, 7]},\n",
    "                    parameters_single_value={'offset':0.1},\n",
    "                    other_parameters={'verbose':False})\n",
    "    df = em.get_experiment_data ()\n",
    "    mi_epochs = (dflt.parameters_col, 'epochs', '')\n",
    "    mi_rate = (dflt.parameters_col, 'rate', '')\n",
    "    mi_offset = (dflt.parameters_col, 'offset', '')\n",
    "    assert (df[mi_epochs]==[5.0, 5.0, 7.0, 7.0]).all()\n",
    "    assert (df[mi_rate].values[[0,2]]==[0.03, 0.03]).all()\n",
    "    assert (df.isna()[mi_rate]==[False, True, False, True]).all()\n",
    "    assert (df[mi_offset]==0.1).all()\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    mi_validation_1 = ('scores', 'validation_accuracy', 1)\n",
    "    assert (np.abs(df[mi_validation]-[0.25, 0.15, 0.31, 0.17])<1.0e-15).all()\n",
    "\n",
    "    #assert (df.isna()[mi_validation] == [True, False]).all()\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    em.raise_error_if_run = True\n",
    "    em.grid_search (parameters_multiple_values={'rate': [0.01,0.03], 'epochs': [7, 5]},\n",
    "                    parameters_single_value={'offset':0.1})\n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df[mi_epochs]==[5.0, 5.0, 7.0, 7.0]).all()\n",
    "    assert (df[mi_rate].values[[0,2]]==[0.03, 0.03]).all()\n",
    "    assert (df.isna()[mi_rate]==[False, True, False, True]).all()\n",
    "    assert (df[mi_offset]==0.1).all()\n",
    "    assert (np.abs(df[mi_validation]-[0.25, 0.15, 0.31, 0.17])<1.0e-15).all()\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    em.raise_error_if_run = False\n",
    "    em.grid_search (parameters_multiple_values={'rate': [0.01,0.03], 'epochs': [7, 5]},\n",
    "                    parameters_single_value={'offset':0.1, 'noise':0.0001}, nruns=2)\n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df[mi_epochs]==[7.0, 7.0, 5.0, 5.0]).all()\n",
    "    assert (df[mi_rate].values[[1,3]]==[0.03, 0.03]).all()\n",
    "    assert (df.isna()[mi_rate]==[True, False, True, False]).all()\n",
    "    assert (df[mi_offset]==0.1).all()\n",
    "    assert (np.abs(df[mi_validation]-[0.17, 0.31, 0.15, 0.25])<0.1).all()\n",
    "    assert (np.abs(df[mi_validation_1]-[0.17, 0.31, 0.15, 0.25])<0.1).all()\n",
    "    assert (df[mi_validation]!=df[mi_validation_1]).all()\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    np.random.seed (42)\n",
    "    em.grid_search (parameters_multiple_values={'rate': [0.01,0.03], 'epochs': [7, 5]},\n",
    "                    parameters_single_value={'offset':0.1}, random_search=True,\n",
    "                    other_parameters={'verbose':False})\n",
    "\n",
    "    df = em.get_experiment_data ()\n",
    "    assert (df[mi_epochs]==[7., 5., 7., 5.]).all()\n",
    "    assert (df[mi_rate].values[[0,1]]==[0.03, 0.03]).all()\n",
    "    assert (df.isna()[mi_rate]==[False, False, True, True]).all()\n",
    "    assert (df[mi_offset]==0.1).all()\n",
    "    assert (np.abs(df[mi_validation]-[0.31, 0.25, 0.17, 0.15])<1e-15).all()\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_grid_search, tag='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run_multiple_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_run_multiple_repetitions ():\n",
    "    em = init_em ('run_multiple_repetitions')\n",
    "    np.random.seed (42)\n",
    "\n",
    "    mu, std, dict_results = em.run_multiple_repetitions (\n",
    "        parameters={'rate': 0.03, 'epochs': 5, 'offset': 0.1},\n",
    "        other_parameters = {'verbose': False, 'noise': 0.001}, nruns=5\n",
    "    )\n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape==(1,28)\n",
    "    x=[(dflt.scores_col, 'validation_accuracy', i) for i in range(5)]; assert df.columns.isin(x).sum()==5\n",
    "    assert (0 < np.abs(mu-0.25) < 1e-3) and (0 < std < 1e-3)\n",
    "\n",
    "    # *********************************\n",
    "    # *********************************\n",
    "    em.remove_previous_experiments (parent=True)\n",
    "    mu, std, dict_results = em.run_multiple_repetitions (\n",
    "        parameters={'rate': 0.03, 'epochs': 5, 'offset': 0.1},\n",
    "        other_parameters = {'verbose': False}\n",
    "    )\n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape==(1,8)\n",
    "    assert mu==0.25 and std==0\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_run_multiple_repetitions, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hp_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def parameter_sampler1 (trial):\n",
    "    rate = trial.suggest_uniform('rate', 0.001, 0.01)\n",
    "    offset = trial.suggest_categorical('offset', [0.01, 0.05, 0.1])\n",
    "\n",
    "    parameters = dict(rate=rate,\n",
    "                      offset=offset)\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def test_hp_optimization ():\n",
    "    em = init_em ('hp_optimization')\n",
    "    np.random.seed (42)\n",
    "\n",
    "    parameters = {'epochs': 12}\n",
    "\n",
    "    em.hp_optimization (parameter_sampler=parameter_sampler1, parameters=parameters,\n",
    "                        study_name='test_hp_optimization_study',\n",
    "                        n_trials=5)\n",
    "    # show\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    assert df.shape == (5,8)\n",
    "\n",
    "    # check\n",
    "    offset_col = (dflt.parameters_col, 'offset', '')\n",
    "    assert (df[offset_col]==[0.01,0.10,0.05,0.01,0.10]).all()\n",
    "    rate_col = (dflt.parameters_col, 'rate', '')\n",
    "    assert np.max(np.abs(df[rate_col]-[0.005939, 0.004813, 0.009673, 0.006112, 0.001182])) < 1e-5\n",
    "    epochs_col = (dflt.parameters_col, 'epochs', '')\n",
    "    assert (df[epochs_col]==12).all()\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_hp_optimization, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicating `sampler_method`, `pruner_method`, and `nruns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_hp_optimization_2 ():\n",
    "    em = init_em ('hp_optimization_2')\n",
    "    np.random.seed (42)\n",
    "    parameters = {'epochs': 21}\n",
    "    em.hp_optimization (sampler_method='skopt', pruner_method='halving',\n",
    "                        parameter_sampler=parameter_sampler1, parameters=parameters,\n",
    "                        study_name='test_hp_optimization_study_2', \n",
    "                        n_trials=6, nruns=2)\n",
    "    # show\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "\n",
    "    # check\n",
    "    assert df.shape == (6,13)\n",
    "    assert sorted(df[(dflt.scores_col, 'test_accuracy')].columns)==[0,1]\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_hp_optimization_2, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicating `nruns_best`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_hp_optimization_3 ():\n",
    "    em = init_em ('hp_optimization_3')\n",
    "    np.random.seed (42)\n",
    "\n",
    "    parameters = {'epochs': 21}\n",
    "\n",
    "    em.hp_optimization (sampler_method='tpe', pruner_method='median',\n",
    "                        parameter_sampler=parameter_sampler1, parameters=parameters,\n",
    "                        study_name='test_hp_optimization_study_3', \n",
    "                        n_trials=6, nruns_best=5)\n",
    "    # show\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    \n",
    "    # check\n",
    "    assert df.shape==(6,28)\n",
    "    assert sorted(df[(dflt.scores_col, 'test_accuracy')].columns)==list(range(5))\n",
    "    assert all(df[(dflt.scores_col, 'test_accuracy')].loc[:,1:4].isna().all(axis=1)==[True]*5+[False])\n",
    "    assert all(df[(dflt.scores_col, 'test_accuracy')].loc[:,1:4].isna().any(axis=1)==[True]*5+[False])\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_hp_optimization_3, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_greedy_search ():\n",
    "    em = init_em ('greedy_search')\n",
    "    \n",
    "    em.greedy_search (parameters_multiple_values={'rate': [0.03, 0.01]},\n",
    "                      parameters_greedy={'epochs': [5, 7], 'offset': [0.1, 0.2, 0.3]},\n",
    "                      other_parameters={'verbose':False})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    \n",
    "    # checks\n",
    "    assert (df[('parameters', 'rate')][0::2]==0.03).all() and df[('parameters', 'rate')][1::2].isna().all()\n",
    "    assert (df[('parameters', 'epochs')].loc[:1]==5).all() and (df[('parameters', 'epochs')].loc[2:]==7).all()\n",
    "    assert df[('parameters', 'offset')].loc[0:3].isna().all() and (df[('parameters', 'offset')].loc[4:]==[0.1,0.1,0.2,0.2,0.3,0.3]).all()\n",
    "    \n",
    "    em.greedy_search (parameters_multiple_values={'noise': [1.0, 0.0]},\n",
    "                      parameters_greedy=[{'epochs': [10, 20]},\n",
    "                                         {'offset': [0.3, 0.6, 0.5]},\n",
    "                                         {'rate': [0.1, 0.2, 0.3]}],\n",
    "                      other_parameters={'verbose':False})\n",
    "    \n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    \n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_greedy_search, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rerun_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def parameter_sampler2 (trial):\n",
    "    epochs = trial.suggest_categorical('epochs', [2, 4])\n",
    "    offset = trial.suggest_categorical('offset', [0.02, 0.06])\n",
    "\n",
    "    parameters = dict(epochs=epochs, offset=offset)\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def test_rerun_experiment ():\n",
    "    em = init_em ('rerun_experiment')\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})\n",
    "    _ = em.create_experiment_and_run (parameters={'rate': 0.04})\n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape==(3,8)\n",
    "\n",
    "    # ****************************************\n",
    "    # case 1: re-running finished experiment\n",
    "    # ****************************************\n",
    "    em.raise_error_if_run = True\n",
    "    em.rerun_experiment (experiments=[1])\n",
    "\n",
    "    # ****************************************\n",
    "    # case 2: re-running interrupted experiment\n",
    "    # ****************************************\n",
    "    em.raise_error_if_run = False\n",
    "    em.rerun_experiment (experiments=[0], other_parameters={'halt':False, 'verbose':False})\n",
    "    df = em.get_experiment_data ()\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    mi_validation_1 = ('scores', 'validation_accuracy', 1)\n",
    "    assert df.loc[0,mi_validation]==0.35\n",
    "\n",
    "    # ****************************************\n",
    "    # case 3: adding more runs to previous experiment\n",
    "    # ****************************************\n",
    "    em.rerun_experiment (experiments=[1], nruns=5, other_parameters={'noise': 0.001, 'verbose':False})\n",
    "    df = em.get_experiment_data ()\n",
    "    x=[(dflt.scores_col, 'validation_accuracy', i) for i in range(5)]; assert df.columns.isin(x).sum()==5\n",
    "    assert df.shape==(3,28)\n",
    "\n",
    "    # ****************************************\n",
    "    # case 4: using previous experiment parameters as fixed, and using grid search with other\n",
    "    # parameters\n",
    "    # ****************************************\n",
    "    em.rerun_experiment (experiments=[2],\n",
    "                         parameters_multiple_values={'offset': [0.01,0.05], 'epochs': [3,5]},\n",
    "                         other_parameters={'verbose':False},\n",
    "                         nruns=2)\n",
    "    df = em.get_experiment_data ()\n",
    "    assert df.shape==(7,28)\n",
    "    assert np.max(np.abs(df[mi_validation].values- [0.35, 0.16, 0.9,  0.13, 0.17, 0.21, 0.25])) < 1e-10\n",
    "    assert df.isna()[mi_validation_1].sum()==2\n",
    "    n1 = (~df.isna())[mi_validation_1].sum()\n",
    "    n0 = (~df.isna())[mi_validation].sum()\n",
    "\n",
    "    # ****************************************\n",
    "    # case 5: using previous experiment parameters as fixed, and using BO with other\n",
    "    # parameters\n",
    "    # ****************************************\n",
    "    em.rerun_experiment (experiments=[2],\n",
    "                         parameter_sampler=parameter_sampler2,\n",
    "                         other_parameters={'verbose':False},\n",
    "                         n_trials=4, sampler_method='skopt')\n",
    "    df2 = em.get_experiment_data ()\n",
    "    display (df2)\n",
    "    print (df2.shape)\n",
    "    assert df2.shape[0]>7\n",
    "    n1 = (~df2.isna())[mi_validation_1].sum()\n",
    "    n0 = (~df2.isna())[mi_validation].sum()\n",
    "    #assert (n0+n1)==16\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_rerun_experiment, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rerun_experiment_pipeline\n",
    "\n",
    "Allows to update some of the parameters on previous experiments and re-run them with those updated parameters, keeping the experiment number unchanged. Optionally, it can save the result to the csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_rerun_experiment_pipeline ():\n",
    "    em = init_em ('rerun_experiment_pipeline')\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})\n",
    "    _ = em.run_multiple_repetitions (parameters={'rate': 0.04}, nruns=5)\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    mi_test = ('scores', 'test_accuracy', 0)\n",
    "    assert np.abs(df.loc[1,mi_validation]-0.16)<1e-5 and (df.loc[1, mi_test]-0.26)<1e-5\n",
    "    #print (df.shape)\n",
    "    assert df.shape==(3,28)\n",
    "\n",
    "    # ****************************************\n",
    "    # case 1: re-running finished experiment\n",
    "    # ****************************************\n",
    "    # the following produces an error since run_numbers must be indicated\n",
    "    with pytest.raises (TypeError):\n",
    "        em.rerun_experiment_pipeline (experiments=[1])\n",
    "\n",
    "    em.raise_error_if_run = True\n",
    "    with pytest.raises (RuntimeError):\n",
    "        em.rerun_experiment_pipeline (experiments=[1], run_numbers=[0])\n",
    "    em.raise_error_if_run = False\n",
    "    # ****************************************\n",
    "    # case 2: changing parameters of prev experiment number\n",
    "    # ****************************************\n",
    "    em.rerun_experiment_pipeline (experiments=[1], run_numbers=[0],\n",
    "                                  new_parameters={'rate': 0.04}, save_results=True)\n",
    "    df = em.get_experiment_data ()\n",
    "    assert np.abs(df.loc[1,mi_validation]-0.18)<1e-5 and np.abs(df.loc[1, mi_test]-0.28)<1e-5\n",
    "\n",
    "    # ****************************************\n",
    "    # case 2: re-running interrupted experiment\n",
    "    # ****************************************\n",
    "    # the following produces an error since halt is True in loaded parameters\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        em.rerun_experiment_pipeline (experiments=[0], run_numbers=[0])\n",
    "\n",
    "    # ****************************************\n",
    "    # case 3: adding more runs to previous experiment\n",
    "    # ****************************************\n",
    "    # the following produces an error since run_numbers must be a subset of those already run\n",
    "    with pytest.raises (FileNotFoundError):\n",
    "        em.rerun_experiment_pipeline (experiments=[1], run_numbers=list(range(5)))\n",
    "\n",
    "    em.rerun_experiment_pipeline (experiments=[2], run_numbers=list(range(5)))\n",
    "    df2 = em.get_experiment_data ()\n",
    "    pd.testing.assert_frame_equal(df,df2)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_rerun_experiment_pipeline, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### rerun_experiment_par\n",
    "\n",
    "The only difference with `rerun_experiment_pipeline` is that now we need to introduce all the parameters to be used. Therefore, `rerun_experiment_par` is not about updating *some* of the parameters but about using entirely new parameters. There is no saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_rerun_experiment_par ():\n",
    "    em = init_em ('rerun_experiment_par')\n",
    "\n",
    "    # first 3 experiments\n",
    "    with pytest.raises (KeyboardInterrupt):\n",
    "        _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.05, 'epochs': 5},\n",
    "                                          other_parameters={'halt': True})\n",
    "    _ = em.create_experiment_and_run (parameters={'offset':0.1, 'rate': 0.03, 'epochs': 2})\n",
    "    _ = em.run_multiple_repetitions (parameters={'rate': 0.04}, nruns=5)\n",
    "    df = em.get_experiment_data ()\n",
    "    display (df)\n",
    "    mi_validation = ('scores', 'validation_accuracy', 0)\n",
    "    mi_test = ('scores', 'test_accuracy', 0)\n",
    "    assert np.abs(df.loc[1,mi_validation]-0.16)<1e-5 and (df.loc[1, mi_test]-0.26)<1e-5\n",
    "    #print (df.shape)\n",
    "    assert df.shape==(3,28)\n",
    "\n",
    "    # ****************************************\n",
    "    # case 1: re-running finished experiment\n",
    "    # ****************************************\n",
    "    # the following produces an error since run_numbers must be indicated\n",
    "    with pytest.raises (TypeError):\n",
    "        em.rerun_experiment_par (experiments=[1])\n",
    "\n",
    "    em.raise_error_if_run = True\n",
    "    with pytest.raises (RuntimeError):\n",
    "        em.rerun_experiment_par (experiments=[1], run_numbers=[0])\n",
    "    em.raise_error_if_run = False\n",
    "    # ****************************************\n",
    "    # case 2: changing parameters of prev experiment number\n",
    "    # ****************************************\n",
    "    em.rerun_experiment_par (experiments=[1], run_numbers=[0],\n",
    "                                  parameters={'rate': 0.04})\n",
    "    df2 = em.get_experiment_data ()\n",
    "\n",
    "    pd.testing.assert_frame_equal(df,df2)\n",
    "\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_rerun_experiment_par, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get_git_revision_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_git_revision_hash (path_experiments=None):\n",
    "    path_experiments = Path(path_experiments).resolve() if path_experiments is not None else None\n",
    "    try:\n",
    "        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "        git_hash = str(git_hash)\n",
    "        if path_experiments is not None:\n",
    "            json_dump(git_hash, path_experiments/'git_hash.json')\n",
    "    except:\n",
    "        logger = logging.getLogger(\"experiment_manager\")\n",
    "        if path_experiments is not None and os.path.exists(path_experiments):\n",
    "            logger.info ('could not get git hash, retrieving it from disk...')\n",
    "            git_hash = json_load (path_experiments/'git_hash.json')\n",
    "        else:\n",
    "            logger.info ('could not get git hash, using empty string...')\n",
    "            git_hash = ''\n",
    "\n",
    "    return str(git_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_get_git_revision_hash ():\n",
    "    global git_hash\n",
    "    path_results = 'test_get_git_revision_hash'\n",
    "    os.makedirs (path_results, exist_ok=True)\n",
    "\n",
    "    # first option: git hash returned but not saved to disk\n",
    "    git_hash = get_git_revision_hash ()\n",
    "    assert git_hash != ''\n",
    "\n",
    "    # second option: git hash saved to disk\n",
    "    git_hash = get_git_revision_hash (path_results)\n",
    "    assert os.listdir (path_results)==['git_hash.json']\n",
    "\n",
    "    # third option: no git repo\n",
    "    curdir = os.path.abspath('.')\n",
    "    os.chdir ('..')\n",
    "    git_hash = get_git_revision_hash (path_results)\n",
    "    assert git_hash == ''\n",
    "\n",
    "    os.chdir (curdir)\n",
    "    remove_previous_results (path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_get_git_revision_hash, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## record_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def record_parameters (path_save, parameters, other_parameters=None, em_args=None, info=None,\n",
    "                      em_attrs=None):\n",
    "    if em_attrs is not None:\n",
    "        em_attrs = get_scalar_fields (em_attrs)\n",
    "    with open(f'{path_save}/parameters.txt', 'wt') as f:\n",
    "        f.write(f'{print_parameters(parameters, dict_name=\"parameters\")}\\n')\n",
    "        if other_parameters is not None:\n",
    "            f.write(f'\\n\\n{print_parameters(other_parameters, dict_name=\"other_parameters\")}\\n')\n",
    "        if em_args is not None:\n",
    "            f.write(f'\\n\\n{print_parameters(em_args, dict_name=\"em_args\")}\\n')\n",
    "        if info is not None:\n",
    "            f.write(f'\\n\\n{print_parameters(info, dict_name=\"info\")}\\n')\n",
    "        if em_attrs is not None:\n",
    "            f.write(f'\\n\\n{print_parameters(em_attrs, dict_name=\"em_attrs\")}\\n')\n",
    "\n",
    "    to_pickle = [parameters]\n",
    "    if other_parameters is not None:\n",
    "        to_pickle.append (other_parameters)\n",
    "    if em_args is not None:\n",
    "        to_pickle.append(em_args)\n",
    "    if info is not None:\n",
    "        to_pickle.append(info)\n",
    "    if em_attrs is not None:\n",
    "        to_pickle.append(em_attrs)\n",
    "    if len(to_pickle) == 1: to_pickle = to_pickle[0]\n",
    "    joblib.dump (to_pickle,f'{path_save}/parameters.pk')\n",
    "\n",
    "    try:\n",
    "        json_dump(parameters, f'{path_save}/parameters.json')\n",
    "    except:\n",
    "        pass\n",
    "    if other_parameters is not None:\n",
    "        try:\n",
    "            json_dump(other_parameters, f'{path_save}/other_parameters.json')\n",
    "        except:\n",
    "            pass\n",
    "    if em_args is not None:\n",
    "        try:\n",
    "            json_dump(em_args, f'{path_save}/em_args.json')\n",
    "        except:\n",
    "            pass\n",
    "    if info is not None:\n",
    "        try:\n",
    "            json_dump(info, f'{path_save}/info.json')\n",
    "        except:\n",
    "            pass\n",
    "    if em_attrs is not None:\n",
    "        try:\n",
    "            json_dump(em_attrs, f'{path_save}/em_attrs.json')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## print_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def print_parameters(parameters, dict_name=None):\n",
    "    if dict_name is not None:\n",
    "        text = '%s=dict(' %dict_name\n",
    "        tpad = ' ' * len(text)\n",
    "    else:\n",
    "        text = '\\t'\n",
    "        tpad = '\\t'\n",
    "    for idx, (key, value) in enumerate(sorted(parameters.items(), key=lambda x: x[0])):\n",
    "        if type(value) is str:\n",
    "            value = '%s%s%s' %(\"'\",value,\"'\")\n",
    "        text += '{}={}'.format(key, value)\n",
    "        if idx < (len(parameters)-1):\n",
    "            text += ',\\n{}'.format(tpad)\n",
    "\n",
    "    if dict_name is not None:\n",
    "        text += ')\\n'\n",
    "    else:\n",
    "        text += '\\n'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_or_create_experiment_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_or_create_experiment_values (path_experiments, parameters, precision=1e-15, logger=None):\n",
    "    if logger is None: logger = logging.getLogger(\"experiment_manager\")\n",
    "    experiment_numbers = []\n",
    "    changed_dataframe = False\n",
    "\n",
    "    experiment_data = read_df (path_experiments)\n",
    "    if experiment_data is not None:\n",
    "        write_binary_df_if_not_exists (experiment_data, path_experiments)\n",
    "        experiment_data = experiment_data.copy()\n",
    "        experiment_data, removed_defaults = remove_defaults_from_experiment_data (experiment_data)\n",
    "\n",
    "        # Finds rows that match parameters. If the dataframe doesn't have any parameter with that name, \n",
    "        # a new column is created and changed_dataframe is set to True\n",
    "        experiment_numbers, changed_dataframe, _ = experiment_utils.find_rows_with_parameters_dict (\n",
    "            experiment_data, parameters, precision=precision\n",
    "        )\n",
    "\n",
    "        changed_dataframe = changed_dataframe or removed_defaults\n",
    "\n",
    "        if len(experiment_numbers) > 1:\n",
    "            logger.info ('more than one matching experiment: ', experiment_numbers)\n",
    "    else:\n",
    "        experiment_data = pd.DataFrame()\n",
    "\n",
    "    if len(experiment_numbers) == 0:\n",
    "        columns = pd.MultiIndex.from_product (\n",
    "            [[dflt.parameters_col], list(parameters.keys()), ['']])\n",
    "        experiment_number = experiment_data.shape[0]\n",
    "        if experiment_data.empty:\n",
    "            experiment_data = experiment_data.append (parameters, ignore_index=True)\n",
    "            experiment_data.columns = columns\n",
    "        else:\n",
    "            experiment_data = pd.concat(\n",
    "                [experiment_data, pd.DataFrame(columns=experiment_data.columns, index=[experiment_number])],\n",
    "                axis=0)\n",
    "            experiment_data[[c for c in columns if c not in experiment_data]] = None\n",
    "            experiment_data.loc [experiment_number, columns] = parameters.values()\n",
    "            experiment_data = experiment_data[experiment_data.columns.sort_values()]\n",
    "        changed_dataframe = True\n",
    "    else:\n",
    "        experiment_number = experiment_numbers[0]\n",
    "\n",
    "    if changed_dataframe:\n",
    "        write_df (experiment_data, path_experiments)\n",
    "\n",
    "    return experiment_number, experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export tests.test_experiment_manager\n",
    "def test_load_or_create_experiment_values ():\n",
    "    path_csv_folder = 'test_load_or_create_experiment_values'\n",
    "    os.makedirs (path_csv_folder, exist_ok=True)\n",
    "    parameters = dict (a='yes', b=1.2, c=True)\n",
    "    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)\n",
    "    display(experiment_data)\n",
    "    assert experiment_data.shape==(1, 3)\n",
    "    assert (experiment_data.columns==pd.MultiIndex.from_product (\n",
    "                [[dflt.parameters_col], list(parameters.keys()), ['']])).all()\n",
    "    assert experiment_data.values[0].tolist() == list(parameters.values())\n",
    "    assert experiment_number==0\n",
    "\n",
    "    parameters = dict (a='no', b=1.2, c=True)\n",
    "    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)\n",
    "    display(experiment_data)\n",
    "    assert experiment_data.shape==(2, 3)\n",
    "    assert (experiment_data.columns==pd.MultiIndex.from_product (\n",
    "                [[dflt.parameters_col], list(parameters.keys()), ['']])).all()\n",
    "    assert experiment_data.values[1].tolist() == list(parameters.values())\n",
    "    assert experiment_number==1\n",
    "\n",
    "    parameters = dict (a='no', d=12, c=True)\n",
    "    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)\n",
    "    assert experiment_data.shape==(3, 4)\n",
    "    assert (experiment_data.columns==pd.MultiIndex.from_product (\n",
    "                [[dflt.parameters_col], ['a','b','c','d'], ['']])).all()\n",
    "    c = pd.MultiIndex.from_product (\n",
    "                [[dflt.parameters_col], ['a','d','c'], ['']])\n",
    "    assert experiment_data[c].values[2].tolist() == list(parameters.values())\n",
    "    assert np.isnan(experiment_data.loc[2, (dflt.parameters_col, 'b', '')])\n",
    "    experiment_data_before = experiment_data.copy()\n",
    "    display(experiment_data)\n",
    "\n",
    "    parameters = dict (a='no', b=1.2, c=True)\n",
    "    experiment_number, experiment_data = load_or_create_experiment_values (path_csv_folder, parameters)\n",
    "    assert experiment_number==1\n",
    "    pd.testing.assert_frame_equal (experiment_data_before,experiment_data)\n",
    "\n",
    "    remove_previous_results (path_csv_folder)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_load_or_create_experiment_values, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## store_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_parameters (path_experiments, experiment_number, parameters):\n",
    "    \"\"\" Keeps track of dictionary to map experiment number and parameters values for the different experiments.\"\"\"\n",
    "    path_experiments = Path(path_experiments).resolve() if path_experiments is not None else None\n",
    "    path_hp_dictionary = path_experiments/'parameters.pk'\n",
    "    if os.path.exists(path_hp_dictionary):\n",
    "        all_parameters = joblib.load (path_hp_dictionary)\n",
    "    else:\n",
    "        all_parameters = {}\n",
    "    if experiment_number not in all_parameters.keys():\n",
    "        str_par = '\\n\\nExperiment %d => parameters: \\n%s\\n' %(experiment_number,print_parameters(parameters))\n",
    "        f = open(path_experiments/'parameters.txt', 'at')\n",
    "        f.write(str_par)\n",
    "        f.close()\n",
    "        all_parameters[experiment_number] = parameters\n",
    "        joblib.dump (all_parameters, path_hp_dictionary)\n",
    "\n",
    "    # pickle number of current experiment, for visualization\n",
    "    joblib.dump (experiment_number, path_experiments/'current_experiment_number.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isnull (experiment_data, experiment_number, name_column):\n",
    "    return ((name_column not in experiment_data.columns) or \n",
    "            (experiment_data.loc[experiment_number, name_column] is None) or \n",
    "            np.isnan(float(experiment_data.loc[experiment_number, name_column])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_number (path_experiments, parameters = {}):\n",
    "    experiment_number, _ = load_or_create_experiment_values (path_experiments, parameters)\n",
    "\n",
    "    return experiment_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_experiment_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_experiment_numbers (path_results_base, parameters_single_value, parameters_multiple_values_all):\n",
    "\n",
    "    experiment_numbers = []\n",
    "\n",
    "    parameters_multiple_values_all = list(ParameterGrid(parameters_multiple_values_all))\n",
    "\n",
    "    for (i_hp, parameters_multiple_values) in enumerate(parameters_multiple_values_all):\n",
    "        parameters = parameters_multiple_values.copy()\n",
    "        parameters.update(parameters_single_value)\n",
    "        parameters = remove_defaults (parameters)\n",
    "\n",
    "        experiment_number = get_experiment_number (path_results_base, parameters=parameters)\n",
    "        experiment_numbers.append(experiment_number)\n",
    "\n",
    "    return experiment_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports tests.test_experiment_manager\n",
    "def test_get_experiment_numbers ():\n",
    "    # get input data\n",
    "    em = init_em ('get_experiment_numbers')\n",
    "    parameters_single_value={'offset':0.1}\n",
    "    parameters_multiple_values={'rate': [0.03,0.01], 'epochs': [5, 7]}\n",
    "    em.grid_search (parameters_multiple_values=parameters_multiple_values,\n",
    "                    parameters_single_value=parameters_single_value,\n",
    "                    other_parameters={'verbose':False})\n",
    "    df = em.get_experiment_data ()\n",
    "    display(df)\n",
    "    \n",
    "    # run `get_experiment_numbers`\n",
    "    experiment_numbers = get_experiment_numbers (em.path_experiments, parameters_single_value, \n",
    "                                                 parameters_multiple_values)\n",
    "\n",
    "    # check results\n",
    "    assert experiment_numbers==[0, 1, 2, 3]\n",
    "    \n",
    "    # delete generated experiment data\n",
    "    em.remove_previous_experiments (parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tst.run (test_get_experiment_numbers, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert_experiment_script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def insert_experiment_script_path (info, logger, stack_level=-3):\n",
    "    if info.get('script_path') is None:\n",
    "        stack_level = info.get('stack_level', stack_level)\n",
    "        stack = traceback.extract_stack()[stack_level]\n",
    "        info['script_path'] = stack.filename\n",
    "        info['lineno'] = stack.lineno\n",
    "        logger.info ('experiment script: {}, line: {}'.format(stack.filename, stack.lineno))\n",
    "        if 'stack_level' in info:\n",
    "            del info['stack_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_parameters (experiment=None,\n",
    "                     other_parameters={}, em_args={}, parameters = {},\n",
    "                     check_experiment_matches=True, em=None):\n",
    "\n",
    "    if em is None:\n",
    "        from hpsearch.config.hpconfig import get_experiment_manager\n",
    "        em = get_experiment_manager ()\n",
    "\n",
    "    path_experiments = em.path_experiments\n",
    "\n",
    "    path_experiment = em.get_path_experiment (experiment)\n",
    "\n",
    "    if (path_experiment/'parameters.pk').exists():\n",
    "        try:\n",
    "            parameters2, other_parameters2, em_args2, *_ = joblib.load (path_experiment/'parameters.pk')\n",
    "        except ValueError:\n",
    "            parameters2, other_parameters2 = joblib.load (path_experiment/'parameters.pk')\n",
    "            em_args2 = {}\n",
    "\n",
    "        other_parameters2.update(other_parameters)\n",
    "        other_parameters = other_parameters2\n",
    "        em_args2.update(em_args)\n",
    "        em_args = em_args2\n",
    "\n",
    "        # if we don't add or modify parameters, we require that the old experiment number matches the new one\n",
    "        if (len(parameters) == 0) and check_experiment_matches:\n",
    "            em.logger.info (f'requiring experiment number to be {experiment}')\n",
    "            em_args['experiment_number'] = experiment\n",
    "        elif 'experiment_number' in em_args:\n",
    "            del em_args['experiment_number']\n",
    "\n",
    "        parameters2.update(parameters)\n",
    "        parameters = parameters2\n",
    "    else:\n",
    "        raise FileNotFoundError (f'file {path_experiment/\"parameters.pk\"} not found')\n",
    "\n",
    "    return parameters, other_parameters, em_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_other_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_scalar_fields (other_parameters):\n",
    "    parameters_to_save = {}\n",
    "    for k in other_parameters.keys():\n",
    "        if type(other_parameters[k]) is str:\n",
    "            parameters_to_save[k] = other_parameters[k]\n",
    "        elif np.isscalar(other_parameters[k]) and np.isreal(other_parameters[k]):\n",
    "            parameters_to_save[k] = other_parameters[k]\n",
    "    return parameters_to_save\n",
    "\n",
    "def save_other_parameters (experiment_number, other_parameters, path_experiments):\n",
    "\n",
    "    parameters_to_save = get_scalar_fields (other_parameters)\n",
    "\n",
    "    path_csv = f'{str(path_experiments)}/other_parameters.csv'\n",
    "    df = pd.DataFrame (index = [experiment_number], data=parameters_to_save)\n",
    "\n",
    "    if os.path.exists (path_csv):\n",
    "        df_all = pd.read_csv (path_csv, index_col=0)\n",
    "        df_all = pd.concat([df_all, df], sort=True)\n",
    "        df_all = df_all.loc[~df_all.index.duplicated(keep='last')]\n",
    "    else:\n",
    "        df_all = df\n",
    "    df_all.to_csv (path_csv)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python (hpsearch)",
   "language": "python",
   "name": "hpsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
