{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#default_exp examples.dummy_experiment_manager\n",
    "from nbdev.showdoc import show_doc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Experiment Manager\n",
    "\n",
    "> Dummy experiment manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class FakeModel (object):\n",
    "    \n",
    "    overfitting_epochs = 20\n",
    "    \n",
    "    def __init__ (self, offset=0.5, rate=0.01, epochs=10, noise=0.0, verbose=True):\n",
    "        # hyper-parameters\n",
    "        self.offset = offset\n",
    "        self.rate = rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # fake internal weight\n",
    "        self.weight = 0\n",
    "        \n",
    "        # fake accuracy\n",
    "        self.accuracy = 0\n",
    "        \n",
    "        # noise\n",
    "        self.noise = noise\n",
    "        \n",
    "        # other parameters\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.history = {}\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def fit (self):\n",
    "        number_epochs = int(self.epochs)\n",
    "        if self.verbose:\n",
    "            print (f'fitting model with {number_epochs} epochs')\n",
    "        \n",
    "        if self.current_epoch==0:\n",
    "            self.accuracy = self.offset\n",
    "        \n",
    "        for epoch in range(number_epochs):\n",
    "            self.weight += self.rate\n",
    "            if self.current_epoch < self.overfitting_epochs:\n",
    "                self.accuracy += self.rate\n",
    "            else:\n",
    "                self.accuracy -= self.rate\n",
    "            if self.verbose:\n",
    "                print (f'epoch {epoch}: accuracy: {self.accuracy}')\n",
    "            \n",
    "            # we keep track of the evolution of different metrics to later be able to visualize it\n",
    "            self.store_intermediate_metrics ()\n",
    "            \n",
    "            # increase current epoch by 1\n",
    "            self.current_epoch += 1\n",
    "    \n",
    "    def store_intermediate_metrics (self):\n",
    "        validation_accuracy, test_accuracy = self.score()\n",
    "        if 'validation_accuracy' not in self.history:\n",
    "            self.history['validation_accuracy'] = []\n",
    "        self.history['validation_accuracy'].append(validation_accuracy)\n",
    "        \n",
    "        if 'test_accuracy' not in self.history:\n",
    "            self.history['test_accuracy'] = []\n",
    "        self.history['test_accuracy'].append(test_accuracy)\n",
    "        \n",
    "        if 'accuracy' not in self.history:\n",
    "            self.history['accuracy'] = []\n",
    "        self.history['accuracy'].append(self.accuracy)\n",
    "        \n",
    "    def save_model_and_history (self, path_results):\n",
    "        pickle.dump (self.weight, open(f'{path_results}/model_weights.pk','wb'))\n",
    "        pickle.dump (self.history, open(f'{path_results}/model_history.pk','wb'))\n",
    "        \n",
    "    def load_model_and_history (self, path_results):\n",
    "        if os.path.exists(f'{path_results}/model_weights.pk'):\n",
    "            self.weight = pickle.load (open(f'{path_results}/model_weights.pk','rb'))\n",
    "            self.history = pickle.load (open(f'{path_results}/model_history.pk','rb'))\n",
    "            self.current_epoch = len(self.history['accuracy'])\n",
    "            if self.current_epoch > 0:\n",
    "                self.accuracy = self.history['accuracy'][-1]\n",
    "            else:\n",
    "                self.accuracy = self.offset\n",
    "        else:\n",
    "            print (f'model not found in {path_results}')\n",
    "        \n",
    "    def score (self):\n",
    "        # validation accuracy\n",
    "        validation_accuracy = self.accuracy + np.random.randn() * self.noise\n",
    "        \n",
    "        # test accuracy\n",
    "        if self.epochs < 10:\n",
    "            test_accuracy = self.accuracy + 0.1\n",
    "        else:\n",
    "            test_accuracy = self.accuracy - 0.1\n",
    "        test_accuracy = test_accuracy + np.random.randn() * self.noise\n",
    "        \n",
    "        # make accuracy be in interval [0,1] \n",
    "        validation_accuracy = max(min(validation_accuracy, 1.0), 0.0)\n",
    "        test_accuracy = max(min(test_accuracy, 1.0), 0.0)\n",
    "        \n",
    "        return validation_accuracy, test_accuracy\n",
    "    \n",
    "    # fake load_data which does nothing\n",
    "    def load_data (self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (experiment_manager.py, line 352)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/anaconda/envs/athena_gpu/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3319\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-56bacdad9f73>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from hpsearch.experiment_manager import ExperimentManager\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/mnt/athena/hpsearch/hpsearch/experiment_manager.py\"\u001b[0;36m, line \u001b[0;32m352\u001b[0m\n\u001b[0;31m    (other_parameters.get('repeat_experiment', False) or other_parameters.get('just_visualize', False))) or\u001b[0m\n\u001b[0m                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from hpsearch.experiment_manager import ExperimentManager\n",
    "import hpsearch\n",
    "import os\n",
    "from hpsearch.visualization import plot_utils \n",
    "\n",
    "class DummyExperimentManager (ExperimentManager):\n",
    "\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "\n",
    "    def run_experiment (self, parameters={}, path_results='./results'):\n",
    "        # extract hyper-parameters used by our model. All the parameters have default values if they are not passed.\n",
    "        offset = parameters.get('offset', 0.5)   # default value: 0.5\n",
    "        rate = parameters.get('rate', 0.01)   # default value: 0.01\n",
    "        epochs = parameters.get('epochs', 10) # default value: 10\n",
    "        noise = parameters.get('noise', 0.0)\n",
    "        \n",
    "        # other parameters that do not form part of our experiment definition\n",
    "        # changing the values of these other parameters, does not make the ID of the experiment change\n",
    "        verbose = parameters.get('verbose', True)\n",
    "        \n",
    "        # build model with given hyper-parameters\n",
    "        model = FakeModel (offset=offset, rate=rate, epochs=epochs, noise = noise, verbose=verbose)\n",
    "        \n",
    "        # load training, validation and test data (fake step)\n",
    "        model.load_data()\n",
    "        \n",
    "        # fit model with training data \n",
    "        model.fit ()\n",
    "        \n",
    "        # save model weights and evolution of accuracy metric across epochs\n",
    "        model.save_model_and_history(path_results)\n",
    "        \n",
    "        # evaluate model with validation and test data\n",
    "        validation_accuracy, test_accuracy = model.score()\n",
    "        \n",
    "        # store model\n",
    "        self.model = model\n",
    "        \n",
    "        # the function returns a dictionary with keys corresponding to the names of each metric. \n",
    "        # We return result on validation and test set in this example\n",
    "        dict_results = dict (validation_accuracy = validation_accuracy,\n",
    "                             test_accuracy = test_accuracy)\n",
    "        \n",
    "        return dict_results\n",
    "    \n",
    "    # implementing the following method is not necessary but recommended\n",
    "    def get_default_parameters (self, parameters):\n",
    "        \"\"\"Indicate the default value for each of the hyper-parameters used.\"\"\"\n",
    "        defaults = dict(offset=0.5,\n",
    "                        rate=0.01,\n",
    "                        epochs=10)\n",
    "        return defaults\n",
    "    \n",
    "    # implementing the following method is not necessary but recommended\n",
    "    def get_path_experiments (self, path_experiments = None, folder = None):\n",
    "        \"\"\"Gives the root path to the folder where results of experiments are stored.\"\"\"\n",
    "        path_experiments = f'{os.path.dirname(hpsearch.__file__)}/../results'\n",
    "        if folder != None:\n",
    "            path_experiments = f'{path_experiments}/{folder}'\n",
    "        return path_experiments\n",
    "    \n",
    "    # implementing the following method is not necessary but recommended\n",
    "    def get_default_operations (self):\n",
    "        default_operations = dict (root='',\n",
    "                                   metric='validation_accuracy',\n",
    "                                   op='max')\n",
    "        \n",
    "        return default_operations\n",
    "    \n",
    "    def experiment_visualization (self, experiments=None, run_number=0, root_path=None, root_folder=None, \n",
    "                                  name_file='model_history.pk', metric='test_accuracy', backend='matplotlib', \n",
    "                                  **kwargs):\n",
    "        if root_path is None:\n",
    "            root_path = self.get_path_experiments(folder=root_folder)\n",
    "        traces = []\n",
    "        for experiment_id in experiments:\n",
    "            path_results = self.get_path_results (experiment_id, run_number=run_number, root_path=root_path)\n",
    "            if os.path.exists('%s/%s' %(path_results, name_file)):\n",
    "                history = pickle.load(open('%s/%s' %(path_results, name_file),'rb'))\n",
    "                label = '{}'.format(experiment_id)\n",
    "                traces = plot_utils.add_trace ((1-np.array(history[metric]))*20, style='A.-', label=label, \n",
    "                                               backend=backend, traces=traces)\n",
    "        plot_utils.plot(title=metric, xlabel='epoch', ylabel=metric, traces=traces, backend=backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def run_multiple_experiments (nruns=1, noise=0.0, verbose=True, rate=0.03):\n",
    "    em = DummyExperimentManager ()\n",
    "    parameters_single_value = dict(rate=rate, noise=noise)   # parameters where we use a fixed value\n",
    "    parameters_multiple_values=dict(offset=[0.1, 0.3, 0.6], epochs=[5, 15, 30]) # parameters where we try multiple values\n",
    "    other_parameters = dict(verbose=verbose) # parameters that control other aspects that are not part of our experiment definition (a new experiment is not created if we assign different values for these parametsers)\n",
    "    em.grid_search (log_message='fixed rate, multiple epochs values',\n",
    "            parameters_single_value=parameters_single_value,\n",
    "            parameters_multiple_values=parameters_multiple_values,\n",
    "            other_parameters=other_parameters,\n",
    "            nruns=nruns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def remove_previous_experiments():\n",
    "    em = DummyExperimentManager ()\n",
    "    path_results = em.get_path_experiments()\n",
    "    if os.path.exists(path_results):\n",
    "        shutil.rmtree(path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse_output\n",
    "remove_previous_experiments()\n",
    "run_multiple_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "em = DummyExperimentManager ()\n",
    "path_results = em.get_path_experiments()\n",
    "df = pd.read_csv (f'{path_results}/experiments_data.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check that stored parameters are correct\n",
    "assert (df.epochs.values == np.array([ 5.,  5.,  5., 15., 15., 15., 30., 30., 30.])).all()\n",
    "assert (df.offset.values == np.array([0.1, 0.3, 0.6, 0.1, 0.3, 0.6, 0.1, 0.3, 0.6])).all()\n",
    "assert (df.rate.values == 0.03).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the accuracy values are correct\n",
    "epochs_before_overfitting = 20\n",
    "epochs_test = 10\n",
    "for experiment_id in df.index:\n",
    "    if df.loc[experiment_id, 'epochs'] < epochs_before_overfitting:\n",
    "        accuracy = df.loc[experiment_id, 'offset'] + df.loc[experiment_id, 'rate'] * df.loc[experiment_id, 'epochs']\n",
    "    else:\n",
    "        epochs_after_overfitting = df.loc[experiment_id, 'epochs']-epochs_before_overfitting\n",
    "        accuracy = df.loc[experiment_id, 'offset'] + df.loc[experiment_id, 'rate'] * (epochs_before_overfitting  - epochs_after_overfitting)\n",
    "    if df.loc[experiment_id, 'epochs'] < epochs_test:\n",
    "        test_accuracy = accuracy + 0.1\n",
    "    else:\n",
    "        test_accuracy = accuracy - 0.1\n",
    "    validation_accuracy = max(min(accuracy, 1.0), 0.0)\n",
    "    test_accuracy = max(min(test_accuracy, 1.0), 0.0)\n",
    "    \n",
    "    assert np.abs(df.loc[experiment_id, '0_validation_accuracy'] - validation_accuracy) <1.e-10, f\"experiment {experiment_id}: {df.loc[experiment_id, '0_validation_accuracy']} == {validation_accuracy}\" \n",
    "    assert np.abs(df.loc[experiment_id, '0_test_accuracy'] - test_accuracy) <1.e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check that model history is written correcly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_experiment = em.get_path_results (3, 0)\n",
    "model = FakeModel()\n",
    "model.load_model_and_history(path_experiment)\n",
    "assert np.max(np.abs(model.history['accuracy']-np.arange(0.13, 0.55, 0.03))) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.experiment_visualization ([3,4,5], backend='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script(recursive=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python (athena_gpu)",
   "language": "python",
   "name": "athena_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
