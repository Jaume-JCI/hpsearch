# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/tools/metric_visualization.ipynb (unless otherwise specified).

__all__ = ['include_best_and_last_experiment', 'set_metric', 'metric_visualization', 'parse_args',
           'parse_arguments_and_visualize', 'main']

# Cell
import warnings
warnings.filterwarnings('ignore')

import argparse
import sys
sys.path.append('.')
sys.path.append('src')
import pandas as pd
import pickle

from ..utils.experiment_utils import read_df, write_df
import hpsearch.visualization.experiment_visualization as ev
from ..config.hpconfig import (get_experiment_manager, get_path_experiments, get_em_args,
                                      add_em_args)
import hpsearch.config.hp_defaults as dflt

# Cell
def include_best_and_last_experiment (metrics, experiments=[-1, -2], run_number=0,
                                      op='max', manager_path=dflt.manager_path):
    path_experiments = get_path_experiments (manager_path=manager_path)
    remove_experiment = None
    for i in range(len(experiments)):
        if experiments[i] == -1:
            experiment_number = pickle.load(open(path_experiments/'current_experiment_number.pkl','rb'))
            experiments[i] = experiment_number

        if experiments[i] == -2:
            first_metric = metrics[0]
            if len(metrics)>1:
                print (f'we use the first metric {first_metric} in given list {metrics} '
                        'for obtaining the best experiment')
            df = read_df (path_experiments)
            score_column = (dflt.scores_col, first_metric, run_number)
            if score_column in df.columns:
                if op=='max':
                    experiments[i] = df[score_column].astype(float).idxmax()
                else:
                    experiments[i] = df[score_column].astype(float).idxmin()
            else:
                if remove_experiment is not None: raise ValueError ('more than one experiment is -2')
                remove_experiment = i
    if remove_experiment is not None: del experiments[remove_experiment]
    return experiments

# Cell
def set_metric (em, metric=None):
    assert metric is not None or em.key_score is not None
    if metric is None:
        metric = em.key_score
        metric = [metric]
    else:
        if not isinstance (metric, list): metric = [metric]
        em.key_score = metric[0]
    if em.key_score is None: em.key_score = metric[0]
    return metric

def metric_visualization (experiments=[-1,-2], run_number=0, parameters=None,
                          name_file='model_history.pk', visualization_options={},
                          manager_path=dflt.manager_path, backend=dflt.backend,
                          metric=None, **kwargs):
    em = get_experiment_manager (manager_path=manager_path, backend=backend, **kwargs)
    folder = em.folder
    metric = set_metric (em, metric)
    op = em.op
    backend = em.backend

    # metrics
    metrics = metric
    if not isinstance (experiments, list): experiments = [experiments]
    if isinstance (run_number, range): run_number = list(run_number)
    if not isinstance (run_number, list): run_number = [run_number]

    experiments = include_best_and_last_experiment (metrics, experiments=experiments,
                                                    run_number=run_number[0], op=op,
                                                    manager_path=manager_path)

    visualization_options = visualization_options.copy()
    visualization_options.update(kwargs)
    if 'visualization' in visualization_options.keys():
        visualization = visualization_options.pop('visualization')
    else:
        visualization = 'history'

    ev.visualize_experiments(visualization=visualization,
                             experiments=experiments, run_number=run_number,
                             metrics=metrics, metric=metrics[0], parameters=parameters,
                             name_file=name_file, **visualization_options, backend=backend)

# Cell
def parse_args(args):
    parser = argparse.ArgumentParser(description='show metrics about experiments')
    # Datasets
    parser.add_argument('-e', nargs='+', default=[-1, -2], type=int,
                        help="experiments")
    parser.add_argument('-m', '--metric', nargs='+', type=str, default=None, help="metrics")
    parser.add_argument('-l','--labels',nargs='+', default=None, type=str)
    parser.add_argument('--run', default=0, type=int)
    parser.add_argument('-f', '--file', default='model_history.pk', type=str)
    parser.add_argument('-v', '--visualization', default='{}', type=str)
    add_em_args (parser, but='metric')

    pars = parser.parse_args(args)

    pars.visualization = eval(pars.visualization)

    return pars

def parse_arguments_and_visualize (args):
    pars = parse_args(args)
    metric = pars.metric
    del pars.metric
    metric_visualization (pars.e, metric=metric, run_number=pars.run, parameters=pars.labels,
                          name_file=pars.file, visualization_options=pars.visualization,
                          **get_em_args (pars))

def main():
    parse_arguments_and_visualize (sys.argv[1:])